{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course overview","text":""},{"location":"#course-description","title":"Course description","text":"<p>Network science is a unifying framework to study complex systems, such as living organisms, societies, and many techno-social systems. Therefore, understanding networks and network (graph) data is fundamental to numerous domains. This graduate-level course introduces the fundamental concepts as well as key applications of network science for a broad range of students. Topics include statistical properties and models of real-world networks, network data &amp; algorithms, how information and diseases spread in our society, and machine learning with networks \u2014 e.g., community detection (clustering) and graph embedding.</p> <p>Because your learning should be the primary focus, please engage actively by voicing your confusions, challenges, and intriguing digressions! Rather than merely watching lectures, think critically, engage in debates, and immerse yourself. </p>"},{"location":"#course-objectives","title":"Course objectives","text":"<p>By the end of the course, students are expected to be able to identify, construct, model, and analyze networks by choosing and applying appropriate methods and algorithms, as well as understanding ethical issues surrounding network data. Students are also expected to be able to explain, both mathematically and conceptually, the key network concepts, algorithms, models, and statistical properties, as well as their implications.</p>"},{"location":"#basic-information","title":"Basic Information","text":"<ul> <li>Homepage: https://yy.github.io/netsci-course/</li> <li>GitHub: https://github.com/yy/netsci-course</li> <li>Instructor: Yong-Yeol (YY) Ahn </li> <li>Announcements: All announcements will be sent via Canvas and Slack. </li> <li>Syllabus: You can download the syllabus here. </li> <li>Textbook: We will primarily use Working with Network Data (WWND) by James Bagrow and yours truly (Cambridge University Press, in press). I also use some chapters from the following books (they are all great):<ul> <li>Network Science by Albert-L\u00e1szl\u00f3 Barab\u00e1si (Cambridge University Press, 2016). </li> <li>Networks: An Introduction by Mark Newman (Oxford University Press, 2018).</li> <li>Networks, Crowds, and Markets by David Easley and Jon Kleinberg (Cambridge University Press, 2010).</li> </ul> </li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Course Wiki</li> <li>Projects</li> <li>Resources</li> </ul>"},{"location":"#special-thanks","title":"Special Thanks","text":"<ul> <li>Francisco Alfaro helped the migration with mkdocs. </li> </ul>"},{"location":"__init__/","title":"init","text":""},{"location":"m00-primer/lab00-self_assessment/","title":"Self assessment quiz","text":""},{"location":"m00-primer/lab00-self_assessment/#self-assessment-quiz","title":"Self assessment quiz\u00b6","text":"<p>Network science uses a wide array of mathematical and computational tools, includign calculus, linear algebra, probability, statistics, algorithms, data structure, and so on. You can use this (ungraded) quiz to identify your strengths and weaknesses in these areas. This will help you identify the areas you need to work on to succeed in this course.</p> <p>Ch. 4 \"Primer\" from Working with Network Data (see Canvas) will be a useful reference to review and catch up on these topics but we will be happy to assist more. Please contact the instructor if you need help with any of these topics!</p>"},{"location":"m00-primer/lab00-self_assessment/#linear-algebra","title":"Linear algebra\u00b6","text":"<ol> <li><p>$\\vec{a} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}$ and $\\vec{b} = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.  Calculate $\\vec{a} \\cdot \\vec{b}$, and $\\vec{a} + \\vec{b}$.</p> </li> <li><p>$A = \\begin{bmatrix}3 &amp; 1 &amp; 0\\\\2 &amp; 1 &amp; 5\\end{bmatrix}$, calculate $A A^\\top$.</p> </li> <li><p>What are the eigenvectors and eigenvalues? Can you explain what they mean conceptually?</p> </li> </ol>"},{"location":"m00-primer/lab00-self_assessment/#probability-and-statistics","title":"Probability and Statistics\u00b6","text":"<ol> <li><p>Explain the difference between joint probability and conditional probability.</p> </li> <li><p>Assume that $\\text{Pr}(A, B) = \\text{Pr}(A) \\text{Pr}(B)$, what is the relationship between $A$ and $B$?</p> </li> <li><p>What is the definition of mean, median, mode, and standard deviation?</p> </li> <li><p>What is the Bayes theorem? Can you write it down and explain what each term means?</p> </li> <li><p>What is the expectation and variance of a Bernoulli distribution (with parameter $p$)?</p> </li> <li><p>What is the expectation and variance of a Poisson distribution (with parameter $\\lambda$)?</p> </li> <li><p>What are the differences between a binomial distribution and a Poisson distribution?</p> </li> <li><p>Can you draw an empirical CDF (cumulative distribution function) of the following data?</p> <pre><code>1, 2, 2, 2, 3, 7, 10, 10\n</code></pre> </li> <li><p>Assume that you are modeling the number of discrete (random) events that occur in a given time interval (e.g., how many buses pass a station in an hour). Which distribution will you use for your model? Assume that the data is given as the list of numbers above. What would be the likelihood function of your model?</p> </li> </ol>"},{"location":"m00-primer/lab00-self_assessment/#calculus","title":"Calculus\u00b6","text":"<ol> <li><p>What is the derivative of $f(x) = a x^3 - b x^2 + e^x$?</p> </li> <li><p>Evaluate the following integral: $\\int_{x}^{\\infty} C y^{-\\alpha} dy$.</p> </li> <li><p>$p(x) = C x^{-4}$ is a probability distribution function, which is defined where $1 \\le x$. What is the value of $C$?</p> </li> <li><p>Solve for $x$? $$ \\frac{dx}{dt} = \\beta x $$</p> </li> <li><p>What will happen in a system described by the following equations? $$ \\frac{dS}{dt} = -\\beta S I \\\\ \\frac{dI}{dt} = \\beta S I $$</p> </li> </ol>"},{"location":"m00-primer/lab00-self_assessment/#programming","title":"Programming\u00b6","text":"<ol> <li><p>What's the time complexity of the following code?</p> <pre><code>total_sum = 0\nfor i in range(0, n):\n    for j in range(0, n):\n        total_sum += i + j\n</code></pre> </li> <li><p>What is the result of the following Python code?</p> <pre><code>print(\"5\" + \"10\")\nprint(10*\"5\")\n</code></pre> </li> <li><p>What would be the result of the following Python code? Can you explain why this is happening?</p> <pre><code>alist = [1,2,3]\nanotherlist = alist\nanotherlist[0] = 5\nprint(alist)\n</code></pre> </li> <li><p>Write a function to calculate the mean, median, and standard deviation of a given list.</p> </li> <li><p>Can you implement a basic FIFO (first-in, first-out) queue class using Python's list?</p> </li> <li><p>You got salary data of the Acme Corporation. They are just numbers, not associated with names of the employees. Which Python data structure (among list, dictionary, and set) will you use to store this numbers? Justify your answer.</p> </li> <li><p>What are the differences of list, set, and dictionary? Explain when one should use one versus the other. Show some use case examples where using list is much more efficient than dictionary and vice versa.</p> </li> <li><p>In your script <code>myscript.py</code>, you want to use a function <code>donothing()</code> defined in a module called <code>usefulfunctions</code>. How can you import this module (assuming it's installed or in the same directory) and use this function?</p> </li> </ol>"},{"location":"m01-intro/class/","title":"Module 1: Why should we care about networks?","text":""},{"location":"m01-intro/class/#what-is-a-network","title":"What is a network?","text":""},{"location":"m01-intro/class/#readings","title":"Readings","text":"<ul> <li>Working with Network Data, Ch. 1-3</li> <li>Working with Network Data, Ch. 4: This chapter reviews basic mathematics and programming.</li> </ul>"},{"location":"m01-intro/class/#further-readings","title":"Further readings","text":"<ul> <li>Barab\u00e1si, Network Science, Ch. 1</li> </ul>"},{"location":"m01-intro/class/#meta-learning","title":"Meta-learning","text":""},{"location":"m01-intro/class/#why-quizzes-and-exams-retrievals-are-more-critical-than-reading-or-lecture-in-learning","title":"Why quizzes and exams (retrievals) are more critical than reading or lecture in learning?","text":"<ul> <li>Karpicke, Jeffrey D., and Henry L. Roediger. \"The critical importance of retrieval for learning.\" science 319.5865 (2008): 966-968. </li> </ul>"},{"location":"m01-intro/class/#on-the-importance-of-internal-motives","title":"On the importance of internal motives","text":"<ul> <li>RSA Animate - Drive: The surprising truth about what motivates us</li> <li>Wrzesniewski, Amy, et al. \"Multiple types of motives don't multiply the motivation of West Point cadets.\" Proceedings of the National Academy of Sciences 111.30 (2014): 10990-10995.</li> </ul>"},{"location":"m01-intro/lab01-env/","title":"Setting up Python environments","text":""},{"location":"m01-intro/lab01-env/#setting-up-python-environments","title":"Setting up Python environments\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m01-intro/lab01-env/#learning-objectives","title":"Learning objectives\u00b6","text":"<ul> <li>Learn why we need virtual environments.</li> <li>Learn how to create and use virtual environments using <code>venv</code>.</li> <li>Learn how to install packages using <code>pip</code>.</li> <li>Learn how to create an environment specification file using <code>pip freeze</code>.</li> <li>Learn the differences between major environment management tools <code>venv</code>, <code>conda</code>, and <code>poetry</code>.</li> <li>Learn the basics of Anaconda environments and conda.</li> <li>Learn how to create a virtual environment and install packages.</li> <li>Learn how to use the kernel with Jupyter notebooks/lab.</li> </ul>"},{"location":"m01-intro/lab01-env/#works-on-my-machine-problem-and-computing-environment","title":"\"Works-on-my-machine\" problem and computing environment\u00b6","text":"<p>Since the beginning of computing, this has been the most common problem:</p> <p>To run any piece of code on any computer, you need many supporting software \u2014 the computing environment. The computing environment includes the operating system, programming language, and libraries, and so on.</p> <p>To address this, we version software and ensures compatability across versions, tracks dependencies, and so on. But they are not perfect, and we may (maybe eternally, gasp) have to deal with the problem of \"it worked on my machine.\"</p> <p>Still, there are many things we can do to mitigate the \"works-on-my-machine\" problem. Here, I'd like to provide some general ideas and pointers to help you set up your Python environment.</p>"},{"location":"m01-intro/lab01-env/#reasons-to-learn-python-environment-management","title":"Reasons to learn Python environment management\u00b6","text":"<p>Setting up and maintaing Python environments can be painful! Python project has not been particularly good with dependency management systems. Python is easy to pick up and start coding because it's interpreted language. Also as a result of this approachability, there are so many users and usecases.</p> <p>This is a good thing, but it also contributes to the messiness of the Python ecosystem. For use cases where performance is vital (e.g., machine learning and scientific computing) \u2014 because Python is a slow, interpreted language \u2014 people had to write performance-critical code in C/C++/Fortran and wrap them in Python. This is great for performance, but it makes dependency management even more complicated.</p> <p></p> <p>(Python Environment, by Randall Munroe, https://xkcd.com/1987/)</p> <p>I must admit that learning how to set up and manage Python environments is not the most exciting thing. Also, you may be lucky enough to work with a dedicated DevOps team that takes care of this for you, or you may be in a situation where you can exclusively use cloud-based services like Google Colab.</p> <p>On the other hand, you may also be in a situation where you have to set up your own environment, to use cutting-edge packages or due to the constraints of your organization. In such cases, whether you can set up and manage your Python environment, and whether you can successfully install and use a single critical package, can make or break your project.</p> <p>In addition, because the environment management involves technical details about how your computer works, it can be a great learning opportunity. It can help you understand how your computer and Python work under the hood. Therefore, I encourage you to constantly learn how to set up and manage Python environments and packages!</p> <p>Having said that, struggling to learn how to manage Python environments on top of completing the weekly assignments and other courses may be too much for you, especially if you are new to programming. So, while I encourage everyone to learn the basics of Python environment management, it will not be a requirement. Please do feel free to use Google Colab or other cloud-based services to circumvent environment management if you get stuck!</p>"},{"location":"m01-intro/lab01-env/#a-general-principle-use-virtual-environments","title":"A general principle: use virtual environments!\u00b6","text":""},{"location":"m01-intro/lab01-env/#problem-with-having-a-single-global-environment","title":"Problem with having a single global environment\u00b6","text":"<p>Imagine a data scientist, Alice, who works on two very different projects. In one project, she is working on a machine learning model, which requires a cutting-edge package that is being actively developed. This package makes use of the most recent features (and thus most recent versions) of other foundational packages (e.g., the lastest version of <code>numpy</code>).</p> <p>In another project, she is debugging and maintaining a legacy codebase that breaks if she uses a recent version of <code>numpy</code>. What should she do in this situation?</p> <p>If Alice installs the latest version of <code>numpy</code> globally, she will not be able to test the legacy codebase, which requires an older version of <code>numpy</code>, and vice versa. She can potentially buy a new computer for each project, but that's just not practical. \ud83e\udd11</p>"},{"location":"m01-intro/lab01-env/#solution-virtual-environments","title":"Solution: virtual environments\u00b6","text":"<p>The solution is to use virtual environments! A virtual environment is a self-contained environment that contains its own version of software packages. Whether the package is a pure Python code or some binary code, there is nothing wrong with keeping multiple versions in the same computer, as long as they are isolated from each other and we can clearly specify which version to use for each project.</p> <p>This is exactly what all virtual environment tools do. Usually a virtual environment is essentially a folder somewhere in your computer (e.g., in your project directory or a dedicated directory for virtual environments) that contains a copy of Python and other packages. When you activate a virtual environment, it modifies your <code>PATH</code> environment variable so that you use whatever in your virtual environment instead of the global version. Then when you deactivate the virtual environment, it restores the <code>PATH</code> variable to its original state.</p> <p>In the most basic sense, that's it!</p>"},{"location":"m01-intro/lab01-env/#virtual-environments-are-not-just-for-python","title":"Virtual environments are not just for Python\u00b6","text":"<p>Virtual environments are not just for Python. For example, <code>Node.js</code> has <code>nvm</code> (Node Version Manager), <code>Ruby</code> has <code>rvm</code> (Ruby Version Manager) and <code>bundler</code>, and so on.</p> <p>If you go one step further, you can use a more general virtual environment tool like <code>Docker</code> to create a virtual environment that contains not only Python but also specific version of other software (e.g., a database) that your project requires.</p>"},{"location":"m01-intro/lab01-env/#basic-virtual-environment-management-with-venv","title":"Basic virtual environment management with <code>venv</code>\u00b6","text":""},{"location":"m01-intro/lab01-env/#what-is-venv","title":"What is <code>venv</code>?\u00b6","text":"<p><code>venv</code> is a built-in module in Python that allows us to create isolated virtual environments. Remember that you can think of each virtual environment as a directory that contains Python libraries, each with a specific version. This isolation means you can work on multiple Python projects with different dependencies on the same machine.</p>"},{"location":"m01-intro/lab01-env/#creating-a-virtual-environment","title":"Creating a Virtual Environment\u00b6","text":"<ul> <li>Open your terminal (Command Prompt on Windows, Terminal on macOS/Linux).</li> <li>Navigate to the directory where you want to create your virtual environment using the <code>cd</code> command.</li> <li>Run the following command:</li> </ul> <pre>python -m venv myenv\n</pre> <p>or</p> <pre>python3 -m venv myenv\n</pre> <p>What does it do? This command creates a new directory named <code>myenv</code> (or whatever you name it) in your current directory. This directory will contain the Python interpreter, a copy of the <code>pip</code> package manager, and other necessary files. It's a self-contained environment where you can install packages without affecting the global Python installation.</p> <p>But, this step does not activate your virtual environment. You need to activate it to use it.</p>"},{"location":"m01-intro/lab01-env/#activating-the-virtual-environment","title":"Activating the Virtual Environment\u00b6","text":"<ul> <li>Once the environment is created, you need to activate it.</li> <li>On Windows: Run</li> </ul> <pre>myenv\\Scripts\\activate\n</pre> <ul> <li>On macOS/Linux: Run</li> </ul> <pre>source myenv/bin/activate\n</pre> <p>What does this do? Activating the virtual environment adjusts your shell\u2019s environment variables so that when you run <code>python</code>, it uses the environment\u2019s Python interpreter and when you run <code>pip</code>, it manages the environment\u2019s packages. It changes your prompt to show the name of the activated environment to let you know that you're using a virtual environment. Always pay attention to which environment you're using!</p>"},{"location":"m01-intro/lab01-env/#installing-packages-in-the-virtual-environment","title":"Installing Packages in the Virtual Environment\u00b6","text":"<ul> <li>With the environment activated, you can install Python packages using <code>pip</code>.</li> <li>For example, to install <code>networkx</code>, run</li> </ul> <pre>pip install networkx\n</pre> <p>If you have activated the virtual environment, <code>networkx</code> should be installed in the virtual environment.</p> <p>Let's check this. First, we can run <code>pip list</code> to see what packages are installed in the virtual environment.</p> <pre>pip list\n</pre> <p>You should be able to see something like this:</p> <pre><code>Package    Version\n---------- -------\nnetworkx   X.X.X\npip        XX.X.X\nsetuptools XX.X.X\n</code></pre> <p>You can also see this by navigating into the virtual environment (remember, it's just a directory).</p> <pre>cd myenv\nls\n</pre> <p>You can do something along this line to see all the packages installed in the virtual environment:</p> <pre>ls myenv/lib/python3.11/site-packages\n</pre>"},{"location":"m01-intro/lab01-env/#letting-others-to-create-the-same-environment","title":"Letting others to create the same environment\u00b6","text":"<p>You can share the list of packages installed in your virtual environment with others by creating a <code>requirements.txt</code> file.</p> <pre>pip freeze &gt; requirements.txt\n</pre> <p>This will create a <code>requirements.txt</code> file that contains the list of packages installed in your virtual environment. Try it and see what's in the file.</p>"},{"location":"m01-intro/lab01-env/#deactivating-the-virtual-environment","title":"Deactivating the Virtual Environment\u00b6","text":"<p>When you\u2019re done working in the virtual environment, you can deactivate it by running</p> <pre>`deactivate`\n</pre> <p>This restores your shell\u2019s environment variables to their normal state, so that <code>python</code> and <code>pip</code> refer to the global Python installation again. So when you're done with a particular project, be sure to deactivate the virtual environment.</p>"},{"location":"m01-intro/lab01-env/#deleting-the-virtual-environment","title":"Deleting the Virtual Environment\u00b6","text":"<ul> <li>If you no longer need the virtual environment, you can simply delete the environment's folder. Everything installed in the virtual environment will be deleted.</li> <li>Use your file manager or the command line to delete the <code>myenv</code> directory.</li> </ul> <p>What happens? This is just a clean-up step. Since all the environment's files are contained within this directory, deleting it removes the environment completely.</p> <p>Now you've learned the most basic usage of <code>venv</code> \u2014 how to create, use, and manage a basic Python virtual environment. This is a fundamental skill in Python development, especially when working on multiple projects or when projects have differing dependencies. Remember, each virtual environment is independent, so feel free to experiment without worrying about affecting other projects or your system's Python setup!</p>"},{"location":"m01-intro/lab01-env/#anaconda-a-powerful-python-environment-management-tool-for-data-science","title":"Anaconda: a powerful Python environment management tool for data science\u00b6","text":"<p>Although <code>venv</code> may be all you need for many use cases (including this course), there are more powerful tools. One such tool, probably the most popular for data science, is Anaconda. It is optional to use Anaconda for this course, but I recommend you to play with it and see if it works for you. It also provides a nice graphical user interface (GUI) for managing environments, which may be helpful for you if you are not comfortable with the command line.</p>"},{"location":"m01-intro/lab01-env/#what-is-anaconda","title":"What is Anaconda?\u00b6","text":"<p>Anaconda is not just a virtual environment management tool, but a complete Python distribution that comes with many useful packages for data science. It is often prefered by data scientists because it comes with many packages pre-installed, and it is easy to install additional packages, even those that are nontrivial to install using <code>pip</code>. Another feature is that it comes as an isolated environment (a single folder), so it is easy to use and manage even if you are working in a shared computer that you do not have admin access to.</p>"},{"location":"m01-intro/lab01-env/#installing-anaconda","title":"Installing Anaconda\u00b6","text":"<ul> <li>Anaconda</li> </ul> <p>Simply follow the instructions on the website based on your operating system. The default installation comes with Python and many useful packages for data science. It also installs the <code>conda</code>, a command-line tool for managing environments. You can think of <code>conda</code> as a combined tool that does more or less what <code>venv</code> and <code>pip</code> do.</p>"},{"location":"m01-intro/lab01-env/#using-anaconda","title":"Using Anaconda\u00b6","text":"<p>There are two ways to use Anaconda. If you are not yet comfortable with the command line, you can use the Anaconda Navigator GUI. If you are comfortable with the command line, you can use the <code>conda</code> command. I will not go into details here, but you can find many tutorials online.</p>"},{"location":"m01-intro/lab01-env/#creating-a-virtual-environment-with-conda","title":"Creating a virtual environment with conda\u00b6","text":"<p>Creating a virtual environment is similar to <code>venv</code>.</p> <pre>conda create --name myenv\n</pre> <p>However, conda does not create the virtual environment (folder) in the current directory. Instead, it creates it in a specific directory that is dedicated to all conda environments. The location will depend on your environment and conda will tell you when you create an environment. It also does some \"magic\" in the background that we will not go into details here.</p> <p>A nice feature of conda, compared with <code>venv</code>, is that it allows you to specify which version of Python you want to use. For example, if you want to use Python 3.8, you can run:</p> <pre>conda create --name myenv python=3.8\n</pre> <p>So, it is much more straightforward to use a specific version of Python with conda than with <code>venv</code>.</p>"},{"location":"m01-intro/lab01-env/#activating-the-virtual-environment","title":"Activating the virtual environment\u00b6","text":"<p>Once you have created a conda environment, you can activate it using the <code>conda activate</code> command.</p> <pre>conda activate myenv\n</pre>"},{"location":"m01-intro/lab01-env/#installing-packages","title":"Installing packages\u00b6","text":"<p>You can install packages using <code>conda install</code> command. For example, to install <code>networkx</code>, you can run:</p> <pre>conda install networkx\n</pre> <p>This looks more or less the same as <code>pip install</code>. However, conda is more powerful than <code>pip</code> in that it can install packages that are not pure Python packages that cannot be installed with <code>pip</code>. Although it is becoming easier to install such packages with <code>pip</code>, there are still some packages that are tricky to install with <code>pip</code>. This is where anaconda/conda shines and the reason why so many data science projects use it.</p> <p>On the other hand, conda does not \"know\" every single package out there, particularly those that are not related to data science, that can be installed with <code>pip</code>. So, you'd often need to use both <code>conda</code> and <code>pip</code> to install all the packages you need, which is annoying and confusing! Making things even more complicated, there are conda channels, which are like package repositories, that contain packages that are not available in the default conda channel. You may not need to worry about this for this course and as long as you don't go deep into cutting-edge packages, but it is something to keep in mind.</p>"},{"location":"m01-intro/lab01-env/#deactivating-the-virtual-environment","title":"Deactivating the virtual environment\u00b6","text":"<p>You can deactivate the virtual environment using the <code>conda deactivate</code> command.</p> <pre>conda deactivate\n</pre>"},{"location":"m01-intro/lab01-env/#deleting-the-virtual-environment","title":"Deleting the virtual environment\u00b6","text":"<p>You can delete the virtual environment using the <code>conda remove</code> command.</p> <pre>conda remove --name myenv --all\n</pre>"},{"location":"m01-intro/lab01-env/#sharing-the-environment-specification","title":"Sharing the environment specification\u00b6","text":"<p>You can share the environment specification using the <code>conda env export</code> command.</p> <pre>conda env export &gt; environment.yml\n</pre> <p>And you can create an environment from the specification using the <code>conda env create</code> command.</p> <pre>conda env create -f environment.yml\n</pre> <p>The <code>environment.yml</code> file is similar to <code>requirements.txt</code> file, but it contains more information about the environment that conda uses (why??? xkcd: standards).</p>"},{"location":"m01-intro/lab01-env/#summary","title":"Summary\u00b6","text":"<ul> <li>Anaconda is a Python distribution that comes with many useful packages for data science.</li> <li>It also comes with <code>conda</code>, a powerful command-line tool for managing environments.</li> <li>You can use Anaconda Navigator GUI or <code>conda</code> command to manage environments.</li> <li>conda's primary power comes from its ability to install packages that are not pure Python packages and cannot be installed with <code>pip</code> (somewhat common in data science and scientific computing in general).</li> <li>Another superpower of conda is that it allows you to specify which version of Python you want to use.</li> <li>However, conda cannot install many non-scientific pacakges (which can be installed by <code>pip</code>). Thus you'd often need to use both <code>conda</code> and <code>pip</code> (it is ok to use both in the same environment).</li> <li>conda uses its own environment specification file (<code>environment.yml</code>), which is similar to <code>requirements.txt</code>.</li> </ul> <p>Refer the official documentation for more details: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html</p>"},{"location":"m01-intro/lab01-env/#poetry-a-modern-python-environment-management-tool","title":"Poetry: a modern Python environment management tool\u00b6","text":"<p>Poetry is a relatively new Python environment management tool (I quite like it!), which goes beyond the virtual environment management, handling dependencies and packaging as well.</p> <p>It is not as popular as Anaconda in data science, but it is gaining popularity. It is again a combined tool that does the job of <code>venv</code> and <code>pip</code>. It is good at managing complex dependencies and is particularly popular among Python developers.</p> <p>Instead of using the <code>requirements.txt</code> file, it uses <code>pyproject.toml</code> file to specify the environment, which is a more modern, cross-language specification format that does not only contain Python packages but also other information about the project.</p> <p>It also creates a \"lock file\" (<code>poetry.lock</code>) that contains the exact versions of the packages installed in the environment. This is similar to <code>requirements.txt</code> file, but it is more precise. For example, if you specify <code>numpy&gt;=1.19.0</code> in <code>pyproject.toml</code>, it will install the latest version of <code>numpy</code> that satisfies the requirement (e.g., <code>1.19.5</code>), and it will be recorded in <code>poetry.lock</code>. This allows you to more easily reproduce the exact environment in another computer.</p> <p>Because this is not the most common tool in data science, I will not go into details here. But if you are interested, you can find more information here: https://python-poetry.org/docs/basic-usage/</p>"},{"location":"m01-intro/lab01-env/#python-on-the-cloud","title":"Python on the cloud\u00b6","text":"<p>Cloud-based Python environments are becoming more and more popular with the rise of cloud computing. Among many advantages of cloud-based environments, there are two that are particularly relevant.</p> <p>First, once you have a cloud-based environment set up, you can access it from anywhere. You can work on your project from your laptop, desktop, or even your phone. Second, you don't have to worry about setting up environments and whatever you have on the cloud tend to be exactly replicable. Usually, these cloud environments are pre-configured with all the necessary software, so you can start working on your project right away. Whenever you fire up a Python notebook on the cloud, you have a clean (exactly same) environment that is ready to use, although changing this base environment can be difficult.</p> <p>Thanks to these strengths, cloud-based environments are becoming more and more popular. Many research papers and tutorials are now published as Jupyter notebooks on Google Colaboratory (Colab), so that they can be easily reproduced and run by anyone.</p>"},{"location":"m01-intro/lab01-env/#googles-colaboratory","title":"Google's colaboratory\u00b6","text":"<ul> <li>https://colab.research.google.com/notebook</li> </ul> <p>Google Colab is a free cloud-based Python environment that comes with many useful packages pre-installed. It is based on Jupyter notebook, so it is easy to use and share. It is also integrated with Google Drive, so you can easily share your notebooks with others and can use data stored in your Google Drive.</p> <p>This is what I recommend you to use for this course if you are not comfortable with setting up your own environment in your computer.</p>"},{"location":"m01-intro/lab01-env/#installing-packages-on-colab","title":"Installing packages on Colab\u00b6","text":"<p>Each colab notebook is like a virtual computer that is created whenever you create or open a notebook. It also allows you to install pacakges. You can install packages using <code>pip</code> as you would do on your own computer. However, because it does not provide you with a command line interface, you need to run the <code>pip</code> command inside your notebook.</p> <p>For example, you can run the following code in a Colab notebook to install <code>networkx</code> (it is already installed though).</p> <pre>!pip install networkx\n</pre> <p>Note that you need to put <code>!</code> in front of the command. Whenever we start a line with <code>!</code> in a Jupyter notebook, it runs the command in the terminal instead of asking it to run as Python code. So this command is equivalent to running <code>pip install networkx</code> in the terminal.</p> <p>Also note that you need to run this command every time you open a new notebook. This is because each notebook is like a new computer, and you need to install packages every time you open a new notebook.</p>"},{"location":"m01-intro/lab01-env/#jupyter-notebook","title":"Jupyter notebook\u00b6","text":"<p>Google Colab is essentially a Jupyter notebook that's running on virtual computers in Google Cloud. So, what is Jupyter notebook?</p>"},{"location":"m01-intro/lab01-env/#interactivity-can-be-incredibly-powerful","title":"Interactivity can be incredibly powerful\u00b6","text":"<p>Python is an interpreted language, so we can have a \"conversation\" with the Python interpreter. For example, we can first define some variables and then use them in the next line.</p> <pre>In [1]: x = 1\n\nIn [2]: y = 2\n\nIn [3]: x + y\n\nOut[3]: 3\n</pre> <p>People realized that this ability to converse with a programming language can be incredibly powerful for data science (and scientific computing in general). For instance, when you load a big tabular dataset, it is super useful to be able to explore the dataset interactively and process the data step by step.</p> <p>Imagine writing a script that performs a series of complicated data processing operations and analyses, where each step depends on the results of the previous steps. To develop this script without any interactivity, you need to go through a tedious loop of (1) writing the initial script, (2) running it, (3) checking the results, and (4) going back to step (1) to change the script. This is not only tedious but also error-prone. Moreover, as the size of the data increases, this process becomes more and more inefficient. You will need to wait for a long time for each iteration.</p>"},{"location":"m01-intro/lab01-env/#the-idea-of-computationalcomputable-document","title":"The idea of computational/computable document\u00b6","text":"<p>Probably the pioneer who realized this potential for the first time was Mathematica, a popular software for mathematical computing. It provides a powerful interface where you can interactively create a document that contains text, code, and visualizations. This document can not only present the results of an analysis, but it can also contain the code that can be executed to reproduce the results and even modified to explore alternatives. This was a revolutionary idea.</p> <p>However, it was not free and open-source, and it was not Python! This great idea began to spread to other languages, leading to the IPython and Jupyter project.</p>"},{"location":"m01-intro/lab01-env/#ipython-notebook-and-jupyter","title":"IPython notebook and Jupyter\u00b6","text":"<p>IPython (Interactive Python) project was a nice attempt to bring this idea to Python. It was a command-line tool (an IDLE replacement) that allowed you to interactively write and execute Python code. It also implemented the idea of computational document, via \"IPython notebook\", which was pretty much what Mathematica was doing, but without the fancy GUI and limited ability to create and interact with visualizations.</p> <p>In terms of its capacity, it was not a match for Mathematica, but it was free and open-source, and it was Python! It was also a great tool for data science, and it became very popular among data scientists. This eventually lead to the creation of Jupyter project.</p> <ul> <li>https://jupyter.org/</li> </ul> <p>Jupyter project was born because the same idea can be applied to many other languages besides Python, and because people realized that it is possible to extract the interface of IPython notebook and make it language-agnostic (and web-based!). Whatever interpreted language we use, we can have the exactly same interface that does not care about what language people are inputting. And then the language interpreter can interpret the code and return the results to the interface.</p> <p>The name Jupyter came from the combination of Julia, Python, and R, the three languages that the Jupyter project initially supported (now it supports many more).</p>"},{"location":"m01-intro/lab01-env/#what-is-jupyterlab","title":"What is JupyterLab?\u00b6","text":"<ul> <li>https://jupyterlab.readthedocs.io/en/stable/</li> </ul> <p>JupyterLab is a successor of the initial Jupyter notebook interface. It aims to be a more comprehensive development environment where you can put together multiple notebooks, terminal, text editor, and so on. It is under an active development and this is what you want to use in most cases.</p>"},{"location":"m01-intro/lab01-env/#installing-jupyterlab","title":"Installing JupyterLab\u00b6","text":"<ul> <li>https://jupyter.org/install</li> </ul> <p>You can install JupyterLab using <code>pip</code> or <code>conda</code>.</p> <pre>pip install jupyterlab\n</pre> <p>or</p> <pre>conda install -c conda-forge jupyterlab\n</pre>"},{"location":"m01-intro/lab01-env/#running-jupyterlab","title":"Running JupyterLab\u00b6","text":"<p>You can run JupyterLab using the <code>jupyter lab</code> command.</p> <pre>jupyter lab\n</pre> <p>This will open a new tab in your browser. You can create a new notebook by clicking the <code>+</code> button on the top left corner. You can also create a new notebook by clicking <code>File -&gt; New -&gt; Notebook</code> in the menu bar.</p>"},{"location":"m01-intro/lab01-env/#kernels-and-ipykernel","title":"\"Kernels\" and ipykernel\u00b6","text":"<p>One thing that may be quite confusing at first, especially with respect to the virtual environments, is the concept of \"kernel\". Let's say you have created a virtual environment and then ran <code>jupyter lab</code> in the terminal. If you're using Jupyter for the first time, I'd bet that your natural assumption is that Jupyter lab is using the virtual environment you have created and you can use all the packages that you have just installed. But that's not the case! \ud83d\ude2c</p> <p>As mentioned earlier, Jupyter is language-agnostic web interface. It does not care about what language you are using. It just sends the code you write to the language interpreter and returns the results. So, you need to tell Jupyter which language (and which version!) you want to use. This is what \"kernel\" means.</p> <p>Say, you have created a virtual environment named <code>myenv</code> and installed <code>networkx</code> in it. The Python interpreter in this virtual environment knows about <code>networkx</code>, but Jupyter does not know about this particular python interpreter (a \"kernel\") and <code>myenv</code> environment until we tell it.</p> <p>So, how can we let Jupyter know about our virtual environment and corresponding kernel (Python interpreter)? Here is the steps that we need to take:</p> <ol> <li>Install <code>ipykernel</code> in the virtual environment.</li> <li>Register the virtual environment as a kernel.</li> <li>Run Jupyter lab and select the kernel.</li> </ol>"},{"location":"m01-intro/lab01-env/#1-install-ipykernel-in-the-virtual-environment","title":"1. Install <code>ipykernel</code> in the virtual environment\u00b6","text":"<p>First, we need to install <code>ipykernel</code> in the virtual environment. This is a package that allows us to register the virtual environment as a kernel.</p> <pre>pip install ipykernel\n</pre>"},{"location":"m01-intro/lab01-env/#2-register-the-virtual-environment-as-a-kernel","title":"2. Register the virtual environment as a kernel\u00b6","text":"<p>Next, we need to register the virtual environment as a kernel. This is done by running the following command:</p> <pre>python -m ipykernel install --user --name=myenv\n</pre> <p>This will register the current virtual environment as a kernel named <code>myenv</code>. You can check this by running the following command:</p> <pre>jupyter kernelspec list\n</pre> <p>And then when we open Jupyter lab, we can select the kernel we want to use. Whenever we open a new or existing notebook, we can select or change the kernel by clicking <code>Kernel -&gt; Change kernel</code> in the menu bar. When you change the kernel, it brings all the packages installed in the corresponding virtual environment.</p> <p>Now you are ready to use Jupyter lab with your virtual environment! \ud83c\udf89</p>"},{"location":"m01-intro/lab01-env/#a-convenient-way-to-work-with-jupyter-notebooks-vscode-and-other-ides","title":"A convenient way to work with Jupyter notebooks: VSCode and other IDEs\u00b6","text":"<p>VSCode and other IDEs let you work with Jupyter notebooks more or less in the same way as JupyterLab (remember that Jupyter is just a language-agnostic interface for Python and other languages). It also removes a lot of tedious steps that you need to take with JupyterLab. For example, you don't need to install <code>ipykernel</code> and register the virtual environment as a kernel. VSCode handles that and let you simply choose the virtual environment you want to use.</p> <p>I strongly recommend to use VS Code for this course and for general data science in general. It is a great IDE with a powerful plugin ecosystem.</p> <p>You can find great tutorials on how to use VS Code for Jupyter notebooks. Here are some examples: https://www.youtube.com/results?search_query=vscode+jupyter+notebook</p>"},{"location":"m01-intro/lab01-env/#assignments","title":"Assignments\u00b6","text":"<p>Q1: Create a virtual environment using your preferred method (e.g., <code>venv</code>, <code>conda</code>, or <code>poetry</code>).  Install the following packages into your virtual environment. Create your environment file and submit it.</p> <p>List of pacakges to install (you can install more if you want):</p> <ul> <li><code>numpy</code></li> <li><code>pandas</code></li> <li><code>matplotlib</code></li> <li><code>networkx</code></li> <li><code>scipy</code></li> <li><code>jupyterlab</code></li> <li><code>ipykernel</code></li> <li><code>nbformat</code></li> </ul> <p>The environment file can be the following:</p> <ul> <li>If you use <code>venv</code>, submit <code>requirements.txt</code> file created by <code>pip freeze</code>.</li> <li>If you use <code>conda</code>, submit <code>environment.yml</code> file created by <code>conda env export &gt; environment.yml</code>.</li> <li>If you use <code>poetry</code>, submit <code>pyproject.toml</code> and <code>poetry.lock</code> files created by <code>poetry</code></li> </ul> <p>Q2: set up Jupyter lab and <code>ipykernel</code> in your virtual environment. Create a notebook, choose the right kernel, create a cell and import the packages that you have installed into the virtual environment (see below). Make sure to run this cell and check whether they can be imported successfully. Submit this notebook as a HTML file and a notebook file.</p> <p>Check this document for instructions on exporting a notebook as an HTML file.</p> <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport scipy as sp\n</pre>"},{"location":"m01-intro/lab01-jupyter/","title":"Setting up Python environments and Jupyter notebooks","text":"Run in Google Colab View on Github"},{"location":"m01-intro/lab01-jupyter/#setting-up-python-environments-and-jupyter-notebooks","title":"Setting up Python environments and Jupyter notebooks\u00b6","text":""},{"location":"m01-intro/lab01-jupyter/#python-setup","title":"Python setup\u00b6","text":""},{"location":"m01-intro/lab01-jupyter/#anaconda","title":"Anaconda\u00b6","text":"<p><code>Conda</code> is  a nice Python distribution for data analysis and visualization. It is called [Anaconda][conda] and you can freely download and use. It is usually the easiest solution to install and maintain necessary Python packages for data analysis, regardless of your platform. Here is the download link:</p> <ul> <li>Download Anaconda</li> </ul> <p>After installing it, you can keep it updated by executing <code>conda</code>.</p> <pre>conda update conda\nconda update anaconda\n</pre> <p>With <code>conda</code>, you can also install many Python packages. For instance:</p> <pre>conda install pandas\n</pre> <ol> <li><p>If you have not already, download and install Anaconda (Python3) on your laptop. If you are more comfortable with installing Python pacakges by yourself with <code>pip</code>, you can certainly do that too.</p> </li> <li><p>Run <code>jupyter notebook</code> from terminal. What do you see? Can you create a new notebook?</p> </li> <li><p>Run the \"Hello world!\" code below in your created notebook</p> </li> </ol> <pre>print('Hello world!')\n</pre> <ol> <li>Read the following tutorials and run any code that you do not understand</li> </ol> <ul> <li>https://docs.python.org/3.7/tutorial/introduction.html</li> <li>https://docs.python.org/3.7/tutorial/controlflow.html</li> </ul> <ol> <li>Import <code>networkx</code>.</li> </ol> <pre>import networkx as nx\n</pre> <p>It should already be installed with Anaconda, if it isn't then you will get an error message, after which you can install from the commandline with:</p> <pre><code>conda install networkx. \n</code></pre> <p>If running the import command produces no output then you should be good to continue.</p> <ol> <li>Rename your notebook as this <code>pysetup_lastname_firstname</code> and submit to Canvas in the Python Setup assignment section.</li> </ol>"},{"location":"m01-intro/lab01-jupyter/#installing-python-for-data-analysis-and-visualization","title":"Installing Python for data analysis and visualization\u00b6","text":""},{"location":"m01-intro/lab01-jupyter/#anaconda","title":"Anaconda\u00b6","text":""},{"location":"m01-intro/lab01-jupyter/#without-anaconda","title":"Without Anaconda\u00b6","text":"<p>If you use Mac or Linux and does not want to use Anaconda, you can install packages by using Python's <code>pip</code> package.  Install Python using either [homebrew][brew], pyenv, or the [official Python download][python-download]. Use <code>pip</code> (or <code>pip3</code>) to install necessary packages. You can run</p> <pre><code> pip3 install numpy scipy networkx jupyter jupyterlab ipython pandas matplotlib seaborn bokeh scikit-learn</code></pre> <p>to install most packages that you can use for data analysis and visualization.</p>"},{"location":"m01-intro/lab01-jupyter/#jupyterlab","title":"JupyterLab\u00b6","text":"<p>Once you have <code>Jupyter lab</code> (or notebook) (<code>Anaconda</code> creates a shortcut), you can simply run</p> <pre><code>jupyter lab </code></pre> <p>or</p> <pre><code>jupyter notebook </code></pre> <p>in the shell or use the launcher to launch ipython notebook. A browser window will appear and show the <code>IPython notebook</code> interface. From here, you can create your notebooks and load other notebooks.</p>"},{"location":"m01-intro/lab01-jupyter/#cloud-options","title":"Cloud options\u00b6","text":"<p>There are also a variety of cloud-based options where you can run Jupyter notebook on the cloud. Probably the best option will be Google Colaboratory. It lets you use a virtual machine and a Jupyter notebook. With colaboratory, you'll less likely to suffer from dependency issues and you'll be able to work whenever you have an access to the web.</p>"},{"location":"m01-intro/lab01-jupyter/#googles-colaboratory","title":"Google's colaboratory\u00b6","text":"<ul> <li>https://colab.research.google.com/notebook</li> </ul> <p>[conda]: [python-download]: https://www.python.org/downloads/ [brew]: http://brew.sh/ [continuum]: https://anaconda.org/anaconda/continuum-docs [wakari]: https://wakari.io</p>"},{"location":"m01-intro/lab01-python_review/","title":"Python review","text":"<p>Are you still new to Python? Or your python skill is a bit rusty? This notebook is for you!</p> <p>This notebook is a review of Python basics. It is not meant to be a comprehensive tutorial. It is just a quick refresher of the Python syntax and data structures. For a more comprehensive tutorial, please refer to the following resources:</p> <ul> <li>Official Python tutorial: it is a bit dry but very comprehensive. Highly recommended to go through it at least once!</li> <li>The Hitchhiker's Guide to Python: Learning Python: a great curated list of resources to learn Python.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Declaring an integer and a float variable\nx = 10 \ny = 3.0\nk = x + y + 100\n\n# Comments start with the hash symbol\n# y = 10\n\n# The value stored in the variable can be inspected by using print statement. \n# Type of a variable var can be checked by calling type(var) \nprint(\"The value of x is\", x, \"and it is of type\", type(x))\n\n# f-strings can be used to write more elegant print statement. \nprint(f\"The value of y is {y} and it is of type {type(y)}\")\nprint(f\"The value of k is {k} and it is of type {type(k)}\")\n\n# casting int to float\nprint(f\"x is of type {type(x)}\")\nx = float(x)\nprint(f\"x is of type {type(x)} after casting\")\n</pre> # Declaring an integer and a float variable x = 10  y = 3.0 k = x + y + 100  # Comments start with the hash symbol # y = 10  # The value stored in the variable can be inspected by using print statement.  # Type of a variable var can be checked by calling type(var)  print(\"The value of x is\", x, \"and it is of type\", type(x))  # f-strings can be used to write more elegant print statement.  print(f\"The value of y is {y} and it is of type {type(y)}\") print(f\"The value of k is {k} and it is of type {type(k)}\")  # casting int to float print(f\"x is of type {type(x)}\") x = float(x) print(f\"x is of type {type(x)} after casting\") In\u00a0[\u00a0]: Copied! <pre># Arithmetic operators\n\n# Addition\nz = x + y\nprint(f\"Adding x and y gives {z}\")\n\n# Subtraction\nz = x - y\nprint(f\"Subtracting y from x gives {z}\")\n\n# Multiplication\nz = x * y\nprint(f\"Multiplying x and y gives {z}\")\n\n# Division\nz = x / y \nprint(f\"x divided by y gives {z}\") \n\n# Floor Division\nz = x // y \nprint(f\"x divided by y gives {z} as quotient\") \n\n# Modulus Operator\nz = x % y \nprint(f\"x divided by y gives {z} as reminder\")\n\n# Exponentiation\nz = x ** y\nprint(f\"x raised to y gives {z}\") \n\n# self increment by 1\nx = x + 1\n# This is equivalent to x += 1\nprint(f\"x + 1 gives {x}\")\n</pre> # Arithmetic operators  # Addition z = x + y print(f\"Adding x and y gives {z}\")  # Subtraction z = x - y print(f\"Subtracting y from x gives {z}\")  # Multiplication z = x * y print(f\"Multiplying x and y gives {z}\")  # Division z = x / y  print(f\"x divided by y gives {z}\")   # Floor Division z = x // y  print(f\"x divided by y gives {z} as quotient\")   # Modulus Operator z = x % y  print(f\"x divided by y gives {z} as reminder\")  # Exponentiation z = x ** y print(f\"x raised to y gives {z}\")   # self increment by 1 x = x + 1 # This is equivalent to x += 1 print(f\"x + 1 gives {x}\") In\u00a0[\u00a0]: Copied! <pre># True and False are the key words that represent bool values in python\na = True\nb = False\n\nprint(f\"a is {a} and b is {b}\")\nprint(f\"Type of variable a and b is {type(a)}\")\n\n# None in python represents the absence of something; similar to null value\nc = None \nprint(f\"c is {c} and is of type {type(c)}\")\n\n# Any non-zero integer value is true and zero is false.\n# Also anything with a non-zero length is true and empty sequences are false.\n</pre> # True and False are the key words that represent bool values in python a = True b = False  print(f\"a is {a} and b is {b}\") print(f\"Type of variable a and b is {type(a)}\")  # None in python represents the absence of something; similar to null value c = None  print(f\"c is {c} and is of type {type(c)}\")  # Any non-zero integer value is true and zero is false. # Also anything with a non-zero length is true and empty sequences are false. In\u00a0[\u00a0]: Copied! <pre># logical operators \n\n# and, or and not operate on bool variables\n# OR operator: Gives True when either of the expressions evaluates to True\n# expr1 or expr2\nprint(f\"a or b is {a or b}\")\nprint(f\"a or a is {a or a}\")\nprint(f\"b or b is {b or b}\")\n\n# AND operator: Gives True when both the expressions evaluates to True\n# expr1 and expr2\nprint(f\"a and b is {a and b}\")\nprint(f\"a and a is {a and a}\")\nprint(f\"b and b is {b and b}\")\n\n# NOT operator: negates a bool\n# not expr1\nprint(f\"Not of a is {not a}\")\n</pre> # logical operators   # and, or and not operate on bool variables # OR operator: Gives True when either of the expressions evaluates to True # expr1 or expr2 print(f\"a or b is {a or b}\") print(f\"a or a is {a or a}\") print(f\"b or b is {b or b}\")  # AND operator: Gives True when both the expressions evaluates to True # expr1 and expr2 print(f\"a and b is {a and b}\") print(f\"a and a is {a and a}\") print(f\"b and b is {b and b}\")  # NOT operator: negates a bool # not expr1 print(f\"Not of a is {not a}\") In\u00a0[\u00a0]: Copied! <pre># comparison operators\n\nx = 10\ny = 3.0\nz = 5\n\n# greater that, less than, greater than equal to and lesser than equal to\nx &gt; y\nx &gt;= y\nx &lt; y\nx &lt;= y\n\n# equals and not equals\nx == y\nx != y\n\n# Chained Expressions \nx &gt; y &gt; z \n(x &gt; y) or (x &gt; z)\n</pre> # comparison operators  x = 10 y = 3.0 z = 5  # greater that, less than, greater than equal to and lesser than equal to x &gt; y x &gt;= y x &lt; y x &lt;= y  # equals and not equals x == y x != y  # Chained Expressions  x &gt; y &gt; z  (x &gt; y) or (x &gt; z) In\u00a0[\u00a0]: Copied! <pre># strings are represented using single or double quotes\nfirst_name = \"Adam\" \nlast_name = 'Eve'\n\n# \\ is used to escape characters\nmiddle_name = 'zero\\'s'\n\n# string concatenation\nfull_name = first_name +' ' + middle_name + ' ' + last_name\n\nprint(f\"Full name is {full_name}\")\nprint(f\"Full name is of type {type(full_name)}\")\n\n# strings can be indexed and sliced similar to lists and tuples. \n# List indexing is discussed in the list section\n</pre> # strings are represented using single or double quotes first_name = \"Adam\"  last_name = 'Eve'  # \\ is used to escape characters middle_name = 'zero\\'s'  # string concatenation full_name = first_name +' ' + middle_name + ' ' + last_name  print(f\"Full name is {full_name}\") print(f\"Full name is of type {type(full_name)}\")  # strings can be indexed and sliced similar to lists and tuples.  # List indexing is discussed in the list section In\u00a0[\u00a0]: Copied! <pre># casting str to int  \ntotal = int('1') + int('2')\nprint(f\"The value of total is {total} and it is of type {type(total)}\")\n\n# casting int to str\n     \ntotal = str(1) + str(2)\nprint(f\"The value of total is {total} and it is of type {type(total)}\")\n</pre> # casting str to int   total = int('1') + int('2') print(f\"The value of total is {total} and it is of type {type(total)}\")  # casting int to str       total = str(1) + str(2) print(f\"The value of total is {total} and it is of type {type(total)}\") In\u00a0[\u00a0]: Copied! <pre># Arrays are implemented as lists in python\n\n# creating empty list\nnames = []\nnames = list()\n\n# list of strings\nnames = ['Zach', 'Jay']\nprint(names)\n\n# list of intergers\nnums = [1, 2, 3, 4, 5]\nprint(nums)\n\n# list of different data types\nl = ['Zach', 1, True, None]\nprint(l)\n\n# list of lists\nll = [[1, 3], [2, 3], [3, 4]]\n\n# finding the length of list\nlength = len(l)\nprint(length)\n</pre> # Arrays are implemented as lists in python  # creating empty list names = [] names = list()  # list of strings names = ['Zach', 'Jay'] print(names)  # list of intergers nums = [1, 2, 3, 4, 5] print(nums)  # list of different data types l = ['Zach', 1, True, None] print(l)  # list of lists ll = [[1, 3], [2, 3], [3, 4]]  # finding the length of list length = len(l) print(length) In\u00a0[\u00a0]: Copied! <pre># Lists are mutable\n\nnames = names + ['Ravi']\nnames.append('Richard')\nnames.extend(['Abi', 'Kevin'])\nprint(names)\n</pre> # Lists are mutable  names = names + ['Ravi'] names.append('Richard') names.extend(['Abi', 'Kevin']) print(names) In\u00a0[\u00a0]: Copied! <pre># List indexing and slicing\n# an element or a subset of list can be accessed using element's index or slice of indices\n# same notation applies for strings but at char level\n\n# some_list[index]\n# some_list[start_index: end_index(not included)]\n\nnumbers = [0, 1, 2, 3, 4, 5, 6]\n\n# indices start from 0 in python\nprint(f'The first element in numbers is {numbers[0]}')\nprint(f'The third element in numbers is {numbers[2]}')\n\nprint(f'Elements from 1st to 5th index are {numbers[1:6]}')\nprint(f'Elements from start to 5th index are {numbers[:6]}')\nprint(f'Elements from 4th index to end are {numbers[4:]}')\n\nprint(f'Last Element is {numbers[-1]}')\nprint(f'Last four element are {numbers[-4:]}')\n\n# changing 1st element in the numbers list\nnumbers[0] = 100\nprint(numbers)\n\n# changing first 3 numbers\nnumbers[0: 3] = [100, 200, 300]\nprint(numbers)\n</pre> # List indexing and slicing # an element or a subset of list can be accessed using element's index or slice of indices # same notation applies for strings but at char level  # some_list[index] # some_list[start_index: end_index(not included)]  numbers = [0, 1, 2, 3, 4, 5, 6]  # indices start from 0 in python print(f'The first element in numbers is {numbers[0]}') print(f'The third element in numbers is {numbers[2]}')  print(f'Elements from 1st to 5th index are {numbers[1:6]}') print(f'Elements from start to 5th index are {numbers[:6]}') print(f'Elements from 4th index to end are {numbers[4:]}')  print(f'Last Element is {numbers[-1]}') print(f'Last four element are {numbers[-4:]}')  # changing 1st element in the numbers list numbers[0] = 100 print(numbers)  # changing first 3 numbers numbers[0: 3] = [100, 200, 300] print(numbers) In\u00a0[\u00a0]: Copied! <pre># Tuples are immutable lists. They are created using () instead of [].\n\nnames = tuple()\nnames = ('Zach', 'Jay') \nprint(names[0])\n\n# trying to alter the tuple gives an error\nnames[0] = 'Richard'\n\n# similar to tuples, strings are also immutable\n</pre> # Tuples are immutable lists. They are created using () instead of [].  names = tuple() names = ('Zach', 'Jay')  print(names[0])  # trying to alter the tuple gives an error names[0] = 'Richard'  # similar to tuples, strings are also immutable In\u00a0[\u00a0]: Copied! <pre># hash maps in python are called Dictionaries\n# dict{key: value}\n\n# Empty dictionary\nphonebook = dict()\n\n# contruction dict using sequences of key-value pairs\ndict([('sape', 4139), ('guido', 4127), ('jack', 4098)])\n\n# Dictionary with one item\nphonebook = {'jack': 4098}\n\n# Add another item\nphonebook['guido'] = 4127\n\nprint(phonebook)\nprint(phonebook['jack'])\nprint(phonebook.items())\nprint(phonebook.keys())\nprint(phonebook.values())\n\nprint('jack' in phonebook)\nprint('Kevin' in phonebook)\n\n# Delete an item\ndel phonebook['jack'] \nprint(phonebook)\n</pre> # hash maps in python are called Dictionaries # dict{key: value}  # Empty dictionary phonebook = dict()  # contruction dict using sequences of key-value pairs dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])  # Dictionary with one item phonebook = {'jack': 4098}  # Add another item phonebook['guido'] = 4127  print(phonebook) print(phonebook['jack']) print(phonebook.items()) print(phonebook.keys()) print(phonebook.values())  print('jack' in phonebook) print('Kevin' in phonebook)  # Delete an item del phonebook['jack']  print(phonebook) In\u00a0[\u00a0]: Copied! <pre># if expr1:\n#     code1\n# elif expr2:\n#     code2\n#   .\n#   .\n#   .\n#   .\n# else:\n#     code_n\n\n# code1 is executed if expr1 is evaluated to true. Else it moves to expr2 and checks for true \n# condition and moves to the next if not true. \n# Finally if all the excpression's are false, code_n is executed\n\nx = int(input(\"Please enter an integer: \"))\n\nif x &lt; 0:\n    x = 0\n    print('Negative changed to zero')\nelif x == 0:\n    print('Zero')\nelif x == 1:\n    print('Single')\nelse:\n    print('More')\n</pre> # if expr1: #     code1 # elif expr2: #     code2 #   . #   . #   . #   . # else: #     code_n  # code1 is executed if expr1 is evaluated to true. Else it moves to expr2 and checks for true  # condition and moves to the next if not true.  # Finally if all the excpression's are false, code_n is executed  x = int(input(\"Please enter an integer: \"))  if x &lt; 0:     x = 0     print('Negative changed to zero') elif x == 0:     print('Zero') elif x == 1:     print('Single') else:     print('More') In\u00a0[\u00a0]: Copied! <pre># for loop is used to iter over any iterable object\n\n# iterating over list\nfor name in ['Steve', 'Jill', 'Venus']:\n    print(name)\n</pre> # for loop is used to iter over any iterable object  # iterating over list for name in ['Steve', 'Jill', 'Venus']:     print(name) In\u00a0[\u00a0]: Copied! <pre># iterating over string\nfor char in \"Hellooooo\":\n    print(char)\n</pre> # iterating over string for char in \"Hellooooo\":     print(char) In\u00a0[\u00a0]: Copied! <pre># iterating over dict keys\nphone_nos = {\"Henry\": 6091237458,\n             \"James\": 1234556789, \n             \"Larry\": 5698327549, \n             \"Rocky\": 8593876589}\nfor name, no in phone_nos.items():\n    print(name, no)\n</pre> # iterating over dict keys phone_nos = {\"Henry\": 6091237458,              \"James\": 1234556789,               \"Larry\": 5698327549,               \"Rocky\": 8593876589} for name, no in phone_nos.items():     print(name, no) In\u00a0[\u00a0]: Copied! <pre># To iterate over a sequence of numbers we use range() function. \n# range(start=0, end, step=1)\nfor i in range(2, 20, 2):\n    print(i)\n</pre> # To iterate over a sequence of numbers we use range() function.  # range(start=0, end, step=1) for i in range(2, 20, 2):     print(i) In\u00a0[\u00a0]: Copied! <pre># using len of list/tuple in range\nnames = ['Steve', 'Rock', 'Harry']\nfor i in range(len(names)):\n    print(names[i])\n</pre> # using len of list/tuple in range names = ['Steve', 'Rock', 'Harry'] for i in range(len(names)):     print(names[i]) In\u00a0[\u00a0]: Copied! <pre># While loop executes as long as the condition remains true.\n# while cond1:\n#     pass\n\ni = 0\nwhile i &lt; 10:\n    print(i)\n    i += 1\n</pre> # While loop executes as long as the condition remains true. # while cond1: #     pass  i = 0 while i &lt; 10:     print(i)     i += 1 In\u00a0[\u00a0]: Copied! <pre># Forever loop: The below code runs for ever\n\n# while True:\n#     print('Forever...')\n</pre> # Forever loop: The below code runs for ever  # while True: #     print('Forever...') In\u00a0[\u00a0]: Copied! <pre># break statement breaks out of the the loop\nwhile True:\n    print('We\u2019re stuck in a loop...')\n    break # Break out of the while loop\nprint(\"not!\")\n</pre> # break statement breaks out of the the loop while True:     print('We\u2019re stuck in a loop...')     break # Break out of the while loop print(\"not!\") In\u00a0[\u00a0]: Copied! <pre># continue statement skips a loop\nfor i in range(5):\n    continue \n    print(i)\n</pre> # continue statement skips a loop for i in range(5):     continue      print(i) In\u00a0[\u00a0]: Copied! <pre># pass statement does nothing and is used as a placeholder\nfor i in range(10):\n    pass\n</pre> # pass statement does nothing and is used as a placeholder for i in range(10):     pass In\u00a0[\u00a0]: Copied! <pre># importing modules using import statement\nimport math\n\n# Import under an alias (avoid this pattern except with the most \n# common modules like pandas, numpy, etc.)\nimport math as m\n\n# Access components with pkg.fn\nm.pow(2, 3) \n\n# Import specific submodules/functions (not usually recommended \n# because it can be confusing to know where a function comes from)\nfrom math import pow\npow(2, 3)\n</pre> # importing modules using import statement import math  # Import under an alias (avoid this pattern except with the most  # common modules like pandas, numpy, etc.) import math as m  # Access components with pkg.fn m.pow(2, 3)   # Import specific submodules/functions (not usually recommended  # because it can be confusing to know where a function comes from) from math import pow pow(2, 3) In\u00a0[\u00a0]: Copied! <pre># Functions in python are defined using key word \"def\"\n\n# simple function to print greetings `greet_word` is an optional argument.\ndef greet(name, greet_word='Hello'):\n    print(f\"{greet_word} {name}. How are you doing?\")\n\n# here greet has 'Hello' as default argument for greet_word\nprint(greet('James'))\nprint(greet(\"Steven\", greet_word=\"Howdy\"))\n\n# Observe that the function by default returns None\n</pre> # Functions in python are defined using key word \"def\"  # simple function to print greetings `greet_word` is an optional argument. def greet(name, greet_word='Hello'):     print(f\"{greet_word} {name}. How are you doing?\")  # here greet has 'Hello' as default argument for greet_word print(greet('James')) print(greet(\"Steven\", greet_word=\"Howdy\"))  # Observe that the function by default returns None In\u00a0[\u00a0]: Copied! <pre># Function to print nth fibonacci number\ndef fib(n):\n    if n &lt;= 1:\n        return 1\n    else:\n        return fib(n-1)+fib(n-2)\nn = 10\nprint(f\"{n}th fibonacci number is {fib(n)}\")\n</pre> # Function to print nth fibonacci number def fib(n):     if n &lt;= 1:         return 1     else:         return fib(n-1)+fib(n-2) n = 10 print(f\"{n}th fibonacci number is {fib(n)}\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"m01-intro/lab01-python_review/#python-review","title":"Python review\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m01-intro/lab01-python_review/#built-in-object-types","title":"Built-in object types\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#int-and-float","title":"Int and Float\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#booleans-and-none","title":"Booleans and None\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#strings","title":"Strings\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#lists-and-tuples","title":"Lists and Tuples\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#dictionary","title":"Dictionary\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#flow-control-statements","title":"Flow control statements\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#if-elif-else","title":"if... elif... else...\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#loops","title":"Loops\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#for-loops","title":"For loops\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#while-loop","title":"While Loop\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#break-continue-and-pass-statements","title":"Break, Continue and Pass statements\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#importing-modules","title":"Importing Modules\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#defining-functions","title":"Defining Functions\u00b6","text":""},{"location":"m01-intro/lab01-python_review/#can-you-run-the-code-in-your-head","title":"Can you run the code in your head?\u00b6","text":"<p>An important part of learning programming is developing the ability to \"run\" the code in your head. The ability to debug your code largely depends on how well and quickly you can run the code in your head. This may not feel natural at first, but it is a skill that you will develop over time and you'll get better with practice!</p> <p>For each of the following question, first think about the result without running the code. Then test it by running the code. Reach out if you don't understand why!</p>"},{"location":"m01-intro/lab01-python_review/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>def func(a):\n    a = a + 2\n    a = a * 2\n    return a\n \nprint(func(2))\n</pre>"},{"location":"m01-intro/lab01-python_review/#true-false-why","title":"True? False? Why?\u00b6","text":"<pre>0.1 + 0.2 == 0.3\n</pre>"},{"location":"m01-intro/lab01-python_review/#3-what-is-list_1-and-list_2-and-why","title":"3. What is <code>list_1</code> and <code>list_2</code> and why?\u00b6","text":"<pre>list_1 = [1,2,3]\nlist_2 = list_1\nlist_1.append(4)\nlist_2 += [5]\nlist_2 = list_2 + [10]\n</pre>"},{"location":"m01-intro/lab01-python_review/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>l = [i**2 for i in range(10)]\nl[-4:2:-3]\n</pre>"},{"location":"m01-intro/lab01-python_review/#what-does-the-code-do-if-the-ordering-doesnt-matter-how-can-it-be-simplified","title":"What does the code do? If the ordering doesn't matter, how can it be simplified?\u00b6","text":"<pre>def func1(lst):\n    a = []\n    for i in lst:\n        if i not in a:\n            a.append(i)\n    return a\n</pre>"},{"location":"m01-intro/lab01-python_review/#what-would-be-the-output","title":"What would be the output?\u00b6","text":"<pre>val = [0, 10, 15, 20]\ndata = 15\ntry:\n    data = data/val[0]\nexcept ZeroDivisionError:\n    print(\"zero division error - 1\")\nexcept:\n    print(\"zero division error - 2\")\nfinally:\n    print(\"zero division error - 3\")\n\nval = [0, 10, 15, 20]\ndata = 15\ntry:\n    data = data/val[4]\nexcept ZeroDivisionError:\n    print(\"zero division error - 1\")\nexcept:\n    print(\"zero division error - 2\")\nfinally:\n    print(\"zero division error - 3\")\n</pre>"},{"location":"m01-intro/lab01-python_review/#what-does-the-code-do","title":"What does the code do?\u00b6","text":"<pre>def func(s):\n    d = {}\n    for c in s:\n        if c in d:\n            d[n] += 1\n        else:\n            d[n] = 1\n    return d\n</pre> <p>(Btw, the same operation can be done by simply running <code>Counter(s)</code> by using <code>Counter</code> data structure in the <code>collections</code> module.)</p>"},{"location":"m01-intro/lab01-python_review/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>def func(l):\n    l.append(10)\n    return l\n\na = [1,2,3]\nb = func(a)\na == b\n</pre>"},{"location":"m01-intro/lab01-python_review/#whats-happening-to-a-in-each-step-why","title":"What's happening to <code>a</code> in each step? Why?\u00b6","text":"<pre># step 1\na = [ [ ] ] * 5\n# step 2\na[0].append(10)\n# step 3\na[1].append(20)\n# step 4\na.append(30)\n</pre>"},{"location":"m01-intro/lab01-python_review/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>L = list('abcdefghijk')\nL[1] = L[4] = 'x'\nL[3] = L[-3]\nprint(L)\n</pre>"},{"location":"m01-intro/lab01-python_review/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>y = 8\nz = lambda x : x * y\nprint (z(6))\n</pre>"},{"location":"m01-intro/lab01-python_review/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>count = 1 \n\ndef func(count):\n    for i in (1, 2, 3): \n        count += 1\nfunc(count = 10)\ncount += 5\nprint(count)\n</pre>"},{"location":"m02-friendship_paradox/lab02/","title":"Friendship paradox assignment","text":"In\u00a0[1]: Copied! <pre>node_list = [1, 2, 3, 4, 5, 6]\nedge_list = [(1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (4, 5), (4, 6)]\n</pre> node_list = [1, 2, 3, 4, 5, 6] edge_list = [(1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (4, 5), (4, 6)] <p>Or we can use sets.</p> In\u00a0[2]: Copied! <pre>node_set = set(node_list)\nedge_set = set(edge_list)\n</pre> node_set = set(node_list) edge_set = set(edge_list) <p>We can easily check whether a node/edge is in the network or not. However, it is quite cumbersome to do other operations like finding all neighbors of a node or finding the degree of a node.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[3]: Copied! <pre>def neighbors_from_list_or_set(nodes, edges):\n    neighbors = {node: set() for node in nodes}\n    for edge in edges:\n        neighbors[edge[0]].add(edge[1])\n        neighbors[edge[1]].add(edge[0])\n    return neighbors\n</pre> def neighbors_from_list_or_set(nodes, edges):     neighbors = {node: set() for node in nodes}     for edge in edges:         neighbors[edge[0]].add(edge[1])         neighbors[edge[1]].add(edge[0])     return neighbors <p>The code is not so bad, but still we have to traverse all edges to figure out the neighbors of a single node. This is horribly inefficient, especially when we have a large network.</p> In\u00a0[4]: Copied! <pre>import numpy as np\n\nadj_matrix = np.array([[0, 1, 1],\n                       [1, 0, 1],\n                       [1, 1, 0]])\n\nprint(adj_matrix)\n</pre> import numpy as np  adj_matrix = np.array([[0, 1, 1],                        [1, 0, 1],                        [1, 1, 0]])  print(adj_matrix)  <pre>[[0 1 1]\n [1 0 1]\n [1 1 0]]\n</pre> <p>What could be the problem with this approach?</p> <p>One problem is that the adjacency matrix is not very space efficient. If we have $n$ nodes, we need to store $n^2$ values (a lot of them will be zero). I must say that there are some clever ways to store the adjacency matrix more efficiently (e.g., using sparse matrix) and it may well be the best way to store and compute network data for some cases. However, in general, it is not the easiest or most efficient way to store network data.</p> <p>Another operation that is not so easy to do with adjacency matrix is to find the neighbors of a node or compute the degree of a node because we need to traverse all columns or rows of the matrix. Can you write a function that calculates the degree of a node using the adjacency matrix?</p> <p>Q: Write a function that calculates the degree of a node using the adjacency matrix.</p> In\u00a0[5]: Copied! <pre>def calculate_degree(adj_matrix, node):\n    # YOUR SOLUTION HERE\n</pre> def calculate_degree(adj_matrix, node):     # YOUR SOLUTION HERE In\u00a0[6]: Copied! <pre>node2neighbors = {1: {2, 3}, 2: {1, 3, 4}, 3: {1, 2, 4}, 4: {2, 3, 5, 6}, 5: {4}, 6: {4}}\n</pre> node2neighbors = {1: {2, 3}, 2: {1, 3, 4}, 3: {1, 2, 4}, 4: {2, 3, 5, 6}, 5: {4}, 6: {4}} <p>Now the operations that we discussed above can be done quite easily and efficiently. Can you fill in the code below?</p> <p>Q: fill in the blanks below to perform basic network operations using adjacency set data structure.</p> In\u00a0[7]: Copied! <pre>def degree(node, node2neighbors):\n    # degree (number of neighbors) of a node in a network\n    # YOUR SOLUTION HERE\n\ndef node_set(node2neighbors):\n    # return the set of all nodes in the network\n    # YOUR SOLUTION HERE\n\ndef is_connected(node2neighbors, i, j):\n    # return True if i and j are connected, False otherwise\n    # YOUR SOLUTION HERE\n</pre> def degree(node, node2neighbors):     # degree (number of neighbors) of a node in a network     # YOUR SOLUTION HERE  def node_set(node2neighbors):     # return the set of all nodes in the network     # YOUR SOLUTION HERE  def is_connected(node2neighbors, i, j):     # return True if i and j are connected, False otherwise     # YOUR SOLUTION HERE <p><code>networkx</code> is one of the most famous Python libraries for network analysis. It provides all the basic functions for network analysis. It's written in Python and thus slower than some other libraries, but it's nice to use, inspect, and learn.</p> <p>Let's see how we can use <code>networkx</code> to create a network and perform basic network operations by thinking about the friendship paradox. We are going to work through some problems and examples in this notebook. The goal is empirically testing the friendship paradox with multiple types of networks.</p> <p>A few tasks will be left to you to complete and submit in your own notebook. Before we dive into creating our own paradox we will go over some basic commands used in the <code>networkx</code> library.</p> <p>First of all, you can ensure that all results are exactly reproducible by fixing the seed for the random number generator. This is a common technique for checking your computation that involves non-deterministic methods.</p> In\u00a0[8]: Copied! <pre># Please use this random seed for submission.\nimport random\nrandom.seed(42)     \n   \nimport numpy as np  # noqa: E402\nnp.random.seed(42)\n</pre> # Please use this random seed for submission. import random random.seed(42)          import numpy as np  # noqa: E402 np.random.seed(42) In\u00a0[9]: Copied! <pre>import networkx as nx\n</pre> import networkx as nx <p><code>import xxxxx as xx</code> is a common way to use widely-used Python libraries. By importing this way, we can use any of the library's classes and functions by prepending the name of the class or function with:</p> <pre><code>nx.\n</code></pre> <p>Networkx has extensive documention with many examples. Whenever unsure about the module, go to the official documentation page and search for the keywords that you are unsure about. In particular, the official tutorial is the best place to learn about the basic usage of <code>networkx</code> library.</p> <p>Let's start by making a simple undirected graph by hand:</p> In\u00a0[10]: Copied! <pre># Creates an instance of a networkx graph.\nmy_first_graph = nx.Graph() \n\n# Lets add some nodes to the graph\nmy_first_graph.add_node(1)\nmy_first_graph.add_node(2)\nmy_first_graph.add_node(3)\n\n# Now lets add some connections\nmy_first_graph.add_edge(1, 2)\nmy_first_graph.add_edge(3, 2)\n</pre> # Creates an instance of a networkx graph. my_first_graph = nx.Graph()   # Lets add some nodes to the graph my_first_graph.add_node(1) my_first_graph.add_node(2) my_first_graph.add_node(3)  # Now lets add some connections my_first_graph.add_edge(1, 2) my_first_graph.add_edge(3, 2) <p>We now have our first graph, which contains 3 nodes and 2 edges. We can look at it too by using <code>draw()</code> function. By the way, depending on the environment, you may have to import <code>matplotlib</code> first (see the <code>networkx</code> tutorial).</p> <p><code>Networkx</code> is underpowered for network visualization and it is rarely used for any serious network visualization. However, it has basic visualization capacity that is perfectly adequate for examining small networks.</p> In\u00a0[11]: Copied! <pre>nx.draw(my_first_graph, with_labels=True)\n</pre> nx.draw(my_first_graph, with_labels=True) <p><code>add_node()</code> and <code>add_edge()</code> are methods of the class <code>Graph</code>. Methods are member functions of classes in python and can be called from an object by using the <code>.</code> notation followed by the method name. We can find out some basic information about this graph using networkx functions and some default python functions:</p> In\u00a0[12]: Copied! <pre># len() is a python function that can be applied to any object to obtain their length. \n# networkx defines the number of nodes as the \"length\" of a graph object. \nlen(my_first_graph) \n</pre> # len() is a python function that can be applied to any object to obtain their length.  # networkx defines the number of nodes as the \"length\" of a graph object.  len(my_first_graph)  Out[12]: <pre>3</pre> <p>We can also look at the neighbors of a node.</p> In\u00a0[13]: Copied! <pre>my_first_graph[2]\n</pre> my_first_graph[2] Out[13]: <pre>AtlasView({1: {}, 3: {}})</pre> In\u00a0[14]: Copied! <pre>my_first_graph.neighbors(2)\n</pre> my_first_graph.neighbors(2) Out[14]: <pre>&lt;dict_keyiterator at 0x1210019e0&gt;</pre> <p>Ok. These are not exactly a list, set, or dictionary that you might have expected. <code>Networkx</code> uses \"views\" to let users to examine read-only set- or dictionary-like data structures. For instance, the <code>AtlasView</code> is a \"view\" of a dictionary of dictionaries. Each node may have some attributes (e.g., labels, size, etc.) and they hold these attributes as a dictionary. Therefore, a set of neighbors of a node is a dictionary, where the keys are the neighbor nodes and the values are the attribute dictionary of each node.</p> <p>Similarly, calling the <code>neighbors()</code> method returns a 'key iterator' of the same dictionary. An iterator yields each successive item. Read about iterators and generators if you are not familiar with them.</p> <p>In both cases, you can convert them into a list like the following,</p> In\u00a0[15]: Copied! <pre>print(list(my_first_graph[2]))\nprint(list(my_first_graph.neighbors(2)))\n</pre> print(list(my_first_graph[2])) print(list(my_first_graph.neighbors(2))) <pre>[1, 3]\n[1, 3]\n</pre> <p>Or iterate over the neighbors.</p> In\u00a0[16]: Copied! <pre>for node in my_first_graph[2]:\n    print(node)\n\nprint([node for node in my_first_graph.neighbors(2)])\n</pre> for node in my_first_graph[2]:     print(node)  print([node for node in my_first_graph.neighbors(2)]) <pre>1\n3\n[1, 3]\n</pre> <p>To examine the friendship paradox, we need to also be able to calculate the degree. Once we have the set/list of neighbors, of course we can simply count them. But <code>networkx</code> also provides a <code>degree()</code> method as well.</p> In\u00a0[17]: Copied! <pre>print(\"Node 2's degree: \", my_first_graph.degree(2))\n</pre> print(\"Node 2's degree: \", my_first_graph.degree(2)) <pre>Node 2's degree:  2\n</pre> <p>Make sure to review the documentation on both of these functions (degree and neighbors) so you are aware what additional arguments you can give. For instance, degree can take a sequence of nodes as an argument and return a sequence of corresponding degrees.</p> <p>We can also combine python control statements with <code>networkx</code> functions:</p> In\u00a0[18]: Copied! <pre># Lets get all the neighbors for each node in the graph\nfor node in my_first_graph.nodes():\n    print(\"Node\", node, \"'s neighbors:\", set(my_first_graph.neighbors(node)))\n</pre> # Lets get all the neighbors for each node in the graph for node in my_first_graph.nodes():     print(\"Node\", node, \"'s neighbors:\", set(my_first_graph.neighbors(node))) <pre>Node 1 's neighbors: {2}\nNode 2 's neighbors: {1, 3}\nNode 3 's neighbors: {2}\n</pre> <p>This <code>for</code> loop iterates over the list returned by the networkx graph method <code>nodes()</code>. This graph method returns a list of each node in the network. Similarly, we can loop over edges using <code>edges()</code>.</p> In\u00a0[19]: Copied! <pre># Quick example of iterating over edges\nfor edge in my_first_graph.edges():\n    print(edge)\n</pre> # Quick example of iterating over edges for edge in my_first_graph.edges():     print(edge) <pre>(1, 2)\n(2, 3)\n</pre> In\u00a0[20]: Copied! <pre>star_graph = nx.star_graph(n=20)\nnx.draw(star_graph)\n</pre> star_graph = nx.star_graph(n=20) nx.draw(star_graph) In\u00a0[21]: Copied! <pre># Lets calculate the average degree of the graph\ndegree_sum = 0.0\nfor node in star_graph.nodes():\n    # YOUR SOLUTION HERE\navg_degree = degree_sum / len(star_graph)\nprint(\"Average degree\", avg_degree)\n</pre> # Lets calculate the average degree of the graph degree_sum = 0.0 for node in star_graph.nodes():     # YOUR SOLUTION HERE avg_degree = degree_sum / len(star_graph) print(\"Average degree\", avg_degree) <pre>Average degree 1.9047619047619047\n</pre> In\u00a0[22]: Copied! <pre># Now lets do it in one line using numpy and list comprehension\n\n# Calculate the average degree of the nodes in the graph\navg_degree = np.mean([star_graph.degree(node) for node in star_graph.nodes()])\nprint(\"Average degree:\",avg_degree)\n</pre> # Now lets do it in one line using numpy and list comprehension  # Calculate the average degree of the nodes in the graph avg_degree = np.mean([star_graph.degree(node) for node in star_graph.nodes()]) print(\"Average degree:\",avg_degree) <pre>Average degree: 1.9047619047619047\n</pre> <p>To calculate the mean, we used numpy's mean function, which can come in handy for quickly calculating the mean of a list or sequence along an axis. So I passed the list comprehension directly to <code>np.mean</code> and it returned the mean of that list.</p> <p>Note: Numpy uses its own data types for carrying out calculations, so if you print a numpy float it will display differently than if you printed a python float, which is why we get the intentional truncation of the value when displayed.</p> <p>For basic graph information, networkx used to have an info() function. However, it has been removed in current versions. info removed</p> <p>Good news! You can easily get the same functionality of the old info() function by defining your own.</p> In\u00a0[23]: Copied! <pre>def info(graph):\n    graph_info = \"Name:\\t\\t\\t\" + graph.name\n    graph_info += \"\\nType:\\t\\t\\t\" + str(type(graph))\n    graph_info += \"\\nNumber of nodes:\\t\" + str(graph.number_of_nodes())\n    graph_info += \"\\nNumber of edges:\\t\" + str(graph.number_of_edges())\n    \n    return graph_info\n    \nstar_graph.name = \"Star Graph\"\nprint(info(star_graph))\n</pre> def info(graph):     graph_info = \"Name:\\t\\t\\t\" + graph.name     graph_info += \"\\nType:\\t\\t\\t\" + str(type(graph))     graph_info += \"\\nNumber of nodes:\\t\" + str(graph.number_of_nodes())     graph_info += \"\\nNumber of edges:\\t\" + str(graph.number_of_edges())          return graph_info      star_graph.name = \"Star Graph\" print(info(star_graph)) <pre>Name:\t\t\tStar Graph\nType:\t\t\t&lt;class 'networkx.classes.graph.Graph'&gt;\nNumber of nodes:\t21\nNumber of edges:\t20\n</pre> In\u00a0[24]: Copied! <pre>def avg_neighbor_degree(G, node):\n    # return the average degree of the neighbors of node\n    # `G` is a networkx grape and `node` is a node in `G`\n    # 1. get the neighbors of node\n    # 2. get the degrees of each neighbor\n    # 3. calculate and return the average degree of the neighbors\n\n    # YOUR SOLUTION HERE\n\n# Test your function here by going through each node in the graph and printing the average neighbor degree\n# YOUR SOLUTION HERE\n</pre> def avg_neighbor_degree(G, node):     # return the average degree of the neighbors of node     # `G` is a networkx grape and `node` is a node in `G`     # 1. get the neighbors of node     # 2. get the degrees of each neighbor     # 3. calculate and return the average degree of the neighbors      # YOUR SOLUTION HERE  # Test your function here by going through each node in the graph and printing the average neighbor degree # YOUR SOLUTION HERE  <pre>The average degree of 0's neighbors: 1.0\nThe average degree of 1's neighbors: 20.0\nThe average degree of 2's neighbors: 20.0\nThe average degree of 3's neighbors: 20.0\nThe average degree of 4's neighbors: 20.0\nThe average degree of 5's neighbors: 20.0\nThe average degree of 6's neighbors: 20.0\nThe average degree of 7's neighbors: 20.0\nThe average degree of 8's neighbors: 20.0\nThe average degree of 9's neighbors: 20.0\nThe average degree of 10's neighbors: 20.0\nThe average degree of 11's neighbors: 20.0\nThe average degree of 12's neighbors: 20.0\nThe average degree of 13's neighbors: 20.0\nThe average degree of 14's neighbors: 20.0\nThe average degree of 15's neighbors: 20.0\nThe average degree of 16's neighbors: 20.0\nThe average degree of 17's neighbors: 20.0\nThe average degree of 18's neighbors: 20.0\nThe average degree of 19's neighbors: 20.0\nThe average degree of 20's neighbors: 20.0\n</pre> In\u00a0[25]: Copied! <pre># calculate the fraction of nodes in the network whose degree is smaller than the average degree of their neighbors.\n \n# YOUR SOLUTION HERE\n</pre> # calculate the fraction of nodes in the network whose degree is smaller than the average degree of their neighbors.   # YOUR SOLUTION HERE <pre>Fraction of nodes with degree smaller than average degree of their neighbors: 0.95\n</pre> In\u00a0[26]: Copied! <pre># Calculate the average degree and the average _neighbor_ degree. \n\n# YOUR SOLUTION HERE\n</pre> # Calculate the average degree and the average _neighbor_ degree.   # YOUR SOLUTION HERE <pre>Average degree: 1.90\nAverage neighbor degree: 19.10\nThe difference: 17.19\n</pre> <p>Even though the star graph is a fairly trivial example it does display the friendship paradox. It is an extreme case where the average degree of the network is wildly different from the average degree of your neighbors. For all but one node (the center node 0) you have fewer friends than your friends.</p> <p>You can use the star graph to check if you functions work, since we know it holds in that case.</p> In\u00a0[27]: Copied! <pre># create a network without friendship paradox\n\n# YOUR SOLUTION HERE\n</pre> # create a network without friendship paradox  # YOUR SOLUTION HERE <pre>Name:\t\t\tA mystery graph\nType:\t\t\t&lt;class 'networkx.classes.graph.Graph'&gt;\nNumber of nodes:\t100\nNumber of edges:\t250\n\nFraction of nodes with degree smaller than average degree of their neighbors: 0.00\n\nAverage degree: 5.00\nAverage neighbor degree: 5.00\nThe difference: 0.00\n</pre> In\u00a0[28]: Copied! <pre># Draw a random graph with 20 nodes and a connection\n# probability of 0.3\nrnd_graph = nx.erdos_renyi_graph(n=20, p=0.3)\nnx.draw(rnd_graph)\n</pre> # Draw a random graph with 20 nodes and a connection # probability of 0.3 rnd_graph = nx.erdos_renyi_graph(n=20, p=0.3) nx.draw(rnd_graph) <p>Alternatively, scale-free networks are growing networks where nodes are preferentially attached to nodes with higher degree. This results in structures called 'hubs' which are nodes that have very high degree. Below is a visualization of one such network:</p> In\u00a0[29]: Copied! <pre># Draw a scale-free graph with 20 nodes\nsf_graph = nx.barabasi_albert_graph(n=30, m=4)\nnx.draw(sf_graph)\n</pre> # Draw a scale-free graph with 20 nodes sf_graph = nx.barabasi_albert_graph(n=30, m=4) nx.draw(sf_graph) <p>Unlike in the random graph where nodes tend to have comparable degrees that are situated pretty close to the average, the scale-free network has many low degree nodes and a few very high degree nodes which we can see in the figure above. Many networks tend to have this hub-like structure although they may not follow the exact degree distribution. We will be testing out the friendship paradox on both types of networks.</p> <p>You can use networkx's generator functions for making the scale-free graph and the random graph.</p> In\u00a0[30]: Copied! <pre># create a scale-free network with 200 nodes and m=3. \n\n# YOUR SOLUTION HERE\n</pre> # create a scale-free network with 200 nodes and m=3.   # YOUR SOLUTION HERE In\u00a0[31]: Copied! <pre># calculate the average degree of the network\n\n# YOUR SOLUTION HERE\n</pre> # calculate the average degree of the network  # YOUR SOLUTION HERE <pre>Average degree: 5.91\n</pre> In\u00a0[32]: Copied! <pre># calculate the fraction of nodes in the network whose degree is smaller than the average degree of their neighbors.\n\n# YOUR SOLUTION HERE\n</pre> # calculate the fraction of nodes in the network whose degree is smaller than the average degree of their neighbors.  # YOUR SOLUTION HERE <pre>Fraction of nodes with degree smaller than average degree of their neighbors: 0.88\n</pre> In\u00a0[33]: Copied! <pre>friendship_paradox_ver2(sf_graph)\n</pre> friendship_paradox_ver2(sf_graph) <pre>Average degree: 5.91\nAverage neighbor degree: 11.05\nThe difference: 5.14\n</pre> In\u00a0[34]: Copied! <pre># ER random graph with parameters ```n=200``` and ```p=0.2```\n\n# YOUR SOLUTION HERE\n</pre> # ER random graph with parameters ```n=200``` and ```p=0.2```  # YOUR SOLUTION HERE  In\u00a0[35]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <pre>Average degree: 40.67\n</pre> In\u00a0[36]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <pre>Fraction of nodes with degree smaller than average degree of their neighbors: 0.57\n</pre> <p>How about the difference between average degree and average neighbor degree?</p> In\u00a0[37]: Copied! <pre>friendship_paradox_ver2(er_graph)\n</pre> friendship_paradox_ver2(er_graph) <pre>Average degree: 40.67\nAverage neighbor degree: 41.52\nThe difference: 0.85\n</pre> <pre>\n# YOUR SOLUTION HERE\n</pre> <p>Rename your notebook as 'friendship_lastname_firstname'. Export it as an HTML file as well. Upload both to Canvas.</p> <p>See this document for instructions on exporting a notebook as an HTML file.</p> In\u00a0[38]: Copied! <pre># Above, we compared the node's degree with the _average degree of its neighbors_. But we can also ask: what's the probability that a _random neighbor of a randomly chosen node_ has a larger degree than the randomly chosen node? Can you test this? \nimport random \n\ndef random_neighbor_has_larger_degree(G):\n    # return the probability that a random neighbor of a random node has a larger degree than the random node\n    # 1. randomly choose a node\n    # 2. randomly choose a neighbor of the node\n    # 3. compare the degrees of the node and its neighbor\n    # 4. repeat steps 1-3 many times and calculate the fraction of times the neighbor has a larger degree than the node\n    # YOUR SOLUTION HERE\n\n\nprint(\"In SF network: \", random_neighbor_has_larger_degree(sf_graph))\nprint(\"In ER network: \", random_neighbor_has_larger_degree(er_graph))\n</pre> # Above, we compared the node's degree with the _average degree of its neighbors_. But we can also ask: what's the probability that a _random neighbor of a randomly chosen node_ has a larger degree than the randomly chosen node? Can you test this?  import random   def random_neighbor_has_larger_degree(G):     # return the probability that a random neighbor of a random node has a larger degree than the random node     # 1. randomly choose a node     # 2. randomly choose a neighbor of the node     # 3. compare the degrees of the node and its neighbor     # 4. repeat steps 1-3 many times and calculate the fraction of times the neighbor has a larger degree than the node     # YOUR SOLUTION HERE   print(\"In SF network: \", random_neighbor_has_larger_degree(sf_graph)) print(\"In ER network: \", random_neighbor_has_larger_degree(er_graph))   <pre>In SF network:  0.694\nIn ER network:  0.513\n</pre> <p>Also, we have only examined model networks. How about real networks? There are several repositories of real-world networks. For instance,</p> <ul> <li>https://icon.colorado.edu</li> <li>https://networks.skewed.de</li> </ul> <p>It may be fun to test how many networks you can find that have the friendship paradox!</p>"},{"location":"m02-friendship_paradox/lab02/#friendship-paradox-assignment","title":"Friendship paradox assignment\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m02-friendship_paradox/lab02/#lets-think-about-how-we-store-network-data","title":"Let's think about how we store network data\u00b6","text":"<p>By now you should have a working Juptyer Lab/Notebook and be using Python 3, either through Google Colaboratory or your local Python environment. If not, go back to the previous assignment to make sure that you have a working environment!</p> <p>When working with network data with code, it's important to think about the data structure\u2014how to represent the data in our computer. To do so, a useful exercise is to think about what we will need to do with the data. For instance, obviously, once we have our network data loaded, we should be able to answer questions like \"how many nodes are there?\" and \"how many edges are there?\". We would also want to be able to ask \"who are the neighbors of node 1?\" and \"what is the degree of node 1?\".</p>"},{"location":"m02-friendship_paradox/lab02/#node-and-edge-lists-or-sets","title":"Node and edge lists or sets\u00b6","text":"<p>The most immediate (but not a good) way to store network data is to use a list or a set. For instance, we can store the nodes and edges in a list:</p>"},{"location":"m02-friendship_paradox/lab02/#adjacency-matrix","title":"Adjacency matrix\u00b6","text":"<p>Another way to store network data is to use an adjacency matrix. An adjacency matrix is a matrix where each row and column represents a node, and the value of the matrix at row $i$ and column $j$ is 1 if there is an edge from node $i$ to node $j$, and 0 otherwise. (we can also store the continuous edge weight instead of 1 or 0.)</p> <p>$$ A_{ij} = \\begin{cases} 1 &amp; \\text{if there is an edge from node $i$ to node $j$} \\\\ 0 &amp; \\text{otherwise} \\end{cases} $$</p> <p>For instance, we can use <code>numpy</code> to create a small adjacency matrix.</p>"},{"location":"m02-friendship_paradox/lab02/#adjacency-list-or-set","title":"Adjacency list or set\u00b6","text":"<p>A pretty good solution is to use an adjacency list or set. The idea is to associate each node with a list or set of nodes that are connected to it. In Python, we can use a dictionary to store (node, neighbrs) as (key, value) pair. For instance,</p>"},{"location":"m02-friendship_paradox/lab02/#friendship-paradox-with-networkx","title":"Friendship paradox with <code>networkx</code>\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#step-1-using-networkx","title":"Step 1: Using networkx\u00b6","text":"<p>We are going to start by importing the networkx module:</p>"},{"location":"m02-friendship_paradox/lab02/#step-2-extreme-examples","title":"Step 2: Extreme examples\u00b6","text":"<p>Now let's apply some of these tools to the friendship paradox. We will start by using <code>networkx</code>'s graph generators to construct a graph for us, then we will calculate the average degree of the network. Once that is done we will calculate the average degree of the neighbors of a few random nodes in the network and compare those values with the degree of the nodes themselves.</p> <p>The one extreme example is the star network where the friendship paradox is extremely strong. Except the node at the center (the star), all other nodes have only one neighbor who are connected to everyone.</p>"},{"location":"m02-friendship_paradox/lab02/#q-average-degree-of-neighbors","title":"Q: Average degree of neighbors\u00b6","text":"<p>Now that you have seen some examples of networkx and numpy, we want you to complete the star graph example by writing a function below that calculates the average degree of a node's neighbors. Test this function to make sure that there is only one node with average neighbor degree of 1 and everyone else has 20.</p> <p>This involves find all of a node's neighbors and then taking the average of their degrees. The input arguments into the function should be a node and a networkx graph. The function should return a single value: the average neighbor degree for the given node. You should learn about how to define and use Python functions if you're not familiar with them yet. Use the space provided below:</p>"},{"location":"m02-friendship_paradox/lab02/#q-testing-the-friendship-paradox","title":"Q: Testing the friendship paradox\u00b6","text":"<p>We would like to check whether the friendship paradox holds for a network. The friendship paradox can be described in many different ways. Here, let's try two ways. First, determine what fraction of nodes in the graph have a larger average neighbor degree than their own. The larger the fraction, the stronger the effect of the friendship paradox. Second, we can calculate the \"average neighbor degree\" for each node and see if its average is larger than the average of the network.</p> <p>For each node we need to find its degree, then average neighbor degree (calculated with the previous function you made). If the average is greater, then it holds for that node. To get the fraction, we can count up all the nodes it holds for and divide by the total number of nodes. Write a function below that will do this for a graph:</p>"},{"location":"m02-friendship_paradox/lab02/#q-the-other-extreme","title":"Q: The other extreme?\u00b6","text":"<p>Can you come up with the other extreme? Find a network where there is no friendship paradox whatsoever. In other words, for every node, their degree is the same as the any of its neighbor's degree. Can you do the same thing above to test whether this is really the case?</p>"},{"location":"m02-friendship_paradox/lab02/#step-3-other-types-of-networks","title":"Step 3: Other types of networks\u00b6","text":"<p>Now let's apply your function to two other types of graphs: a scale-free graph and a random graph. Random graphs are generated from randomly connecting nodes together, with each node having the same probability of being connected to any other node. They don't have much structure to them. Below is a visualization of a random graph:</p>"},{"location":"m02-friendship_paradox/lab02/#q-scale-free-network","title":"Q: Scale-free network\u00b6","text":"<p>For the scale-free network, use the parameters <code>n=200</code> and <code>m=3</code>. You should answer the following questions:</p>"},{"location":"m02-friendship_paradox/lab02/#1-what-is-the-average-degree-of-the-graph","title":"1. What is the average degree of the graph?\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#2-what-fraction-of-nodes-in-the-graph-have-a-larger-average-neighbor-degree-than-their-degree","title":"2. What fraction of nodes in the graph have a larger average neighbor degree than their degree?\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#compare-average-degree-of-the-network-and-the-average-neighbor-degree-of-the-network","title":"Compare average degree of the network and the average 'neighbor' degree of the network.\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#3-did-the-friendship-paradox-occur-in-the-scale-free-graph-if-so-why-did-it-if-not-why-not-provide-your-response-in-the-markdown-cell-below","title":"3. Did the friendship paradox occur in the scale-free graph? If so, why did it? If not, why not? (provide your response in the markdown cell below)\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#your-solution-here","title":"YOUR SOLUTION HERE\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#q-random-graph","title":"Q: Random graph\u00b6","text":"<p>For the erdos-renyi graph use the parameters <code>n=200</code> and <code>p=0.2</code>.</p>"},{"location":"m02-friendship_paradox/lab02/#1-what-is-the-average-degree-of-the-graph","title":"1. What is the average degree of the graph?\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#2-what-fraction-of-nodes-in-the-graph-have-a-larger-average-neighbor-degree-than-their-degree","title":"2. What fraction of nodes in the graph have a larger average neighbor degree than their degree?\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#3-it-seems-like-we-still-see-a-small-difference-why-the-er-graph-is-a-random-graph-so-why-is-there-still-a-difference-provide-your-response-in-the-markdown-cell-below","title":"3. It seems like we still see a small difference. Why? The ER graph is a random graph, so why is there still a difference? (provide your response in the markdown cell below)\u00b6","text":""},{"location":"m02-friendship_paradox/lab02/#optional-exercise","title":"Optional exercise\u00b6","text":"<p>Above, we compared the node's degree with the average degree of its neighbors. But we can also ask: what's the probability that a random neighbor of a randomly chosen node has a larger degree than the randomly chosen node? Can you test this?</p>"},{"location":"m03-smallworld/lab03/","title":"Small world assignment","text":"<p>In this assignment, we will get ourselves more familiar with mathematical ways to understand networks, playing with walks, paths, and small-worldness.</p> <p>Let's start with importing the necessary libraries.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\n</pre> import numpy as np import matplotlib.pyplot as plt import networkx as nx  <pre>\n# YOUR SOLUTION HERE\n</pre> <pre>\n# YOUR SOLUTION HERE\n</pre> In\u00a0[2]: Copied! <pre># We can draw the adjacency matrix of a graph using the imshow function from matplotlib\nN = 100\np = 0.2\nG = nx.erdos_renyi_graph(N, p)\nA = nx.adjacency_matrix(G).todense()\nplt.imshow(A, cmap='Greys', interpolation='none')\n</pre> # We can draw the adjacency matrix of a graph using the imshow function from matplotlib N = 100 p = 0.2 G = nx.erdos_renyi_graph(N, p) A = nx.adjacency_matrix(G).todense() plt.imshow(A, cmap='Greys', interpolation='none') Out[2]: <pre>&lt;matplotlib.image.AxesImage at 0x10627c710&gt;</pre> <p>How about other types of graphs? What about a network with strong modular structure (if we reshuffle the rows and columns so that the structure is shown clearly)? How about DAG (directed acyclic graph)? How about bipartite graphs?</p> <p>It will be very useful for you to think about these questions and try to sketch the adjacency matrix of various types of networks. You can see some examples in WWND Ch. 25.</p> <pre>\n# YOUR SOLUTION HERE\n</pre> In\u00a0[3]: Copied! <pre>def simple_bfs(G, source_node) -&gt; dict:\n    # this function returns a dictionary with the distance from the source node \n    # to all other nodes. \n    # e.g., {0: 0, 1: 1, 2: 1, 3: 1} means that the distance from the source node \n    # to node 0 is 0, to node 1 is 1, and so on. \n    \n    distance_from_source = {}\n    nodes_to_explore = []       # the queue\n\n    # YOUR SOLUTION HERE\n\n    return distance_from_source\n\n# let's test the function\nG = nx.Graph()\nG.add_edges_from([(0, 1), (0,2), (1, 3), (3, 4), (1,4)])\nprint(simple_bfs(G, 0))\n\nassert simple_bfs(G, 0) == {0: 0, 1: 1, 2: 1, 3: 2, 4: 2}  # this is the expected answer\n</pre> def simple_bfs(G, source_node) -&gt; dict:     # this function returns a dictionary with the distance from the source node      # to all other nodes.      # e.g., {0: 0, 1: 1, 2: 1, 3: 1} means that the distance from the source node      # to node 0 is 0, to node 1 is 1, and so on.           distance_from_source = {}     nodes_to_explore = []       # the queue      # YOUR SOLUTION HERE      return distance_from_source  # let's test the function G = nx.Graph() G.add_edges_from([(0, 1), (0,2), (1, 3), (3, 4), (1,4)]) print(simple_bfs(G, 0))  assert simple_bfs(G, 0) == {0: 0, 1: 1, 2: 1, 3: 2, 4: 2}  # this is the expected answer  <pre>{0: 0, 1: 1, 2: 1, 3: 2, 4: 2}\n</pre> <p>This function lets you compute the shortest path from a given node to all other nodes. We can now use this function to compute all possible shortest paths between every node pair, and then compute the average shortest path length.</p> <p>If we have $N$ nodes in our network, we have roughly $N^2$ shortest paths to compute. It is slow and also consumes a lot of memory. There is not a lot of things we can do to drastically reduce the computing time, but we can drastically reduce the memory usage if we only care about the average shortest path length. How would you do that? Think about it before you move on.</p> Solution<p>We can compute the shortest path length from a given node to all other nodes, and then keep only the counts of the shortest path lengths. In other words, instead keeping the dictionary that we had above, we can simply summarize it as: there is one node (0) with distance 0, two nodes (1, 2) with distance 1, and so on.</p> <p>To do this, we can use the <code>collections.Counter</code> class. If you are not familiar with it, read about it here: https://docs.python.org/3/library/collections.html#collections.Counter What it can do can be done with a dictionary, but it is often much more convenient to use <code>Counter</code>.</p> <p>Q: Can you write a Python function that computes the shortest path length from a given node to all other nodes, and then returns the summary as a <code>Counter</code>?</p> In\u00a0[4]: Copied! <pre>from collections import Counter\ndef count_path_lengths(distances):\n    # YOUR SOLUTION HERE\n</pre> from collections import Counter def count_path_lengths(distances):     # YOUR SOLUTION HERE In\u00a0[5]: Copied! <pre># this should be satisfied. \nassert count_path_lengths({0: 0, 1: 1, 2: 1, 3: 2, 4: 2}) == Counter({0: 1, 1: 2, 2: 2})\n</pre> # this should be satisfied.  assert count_path_lengths({0: 0, 1: 1, 2: 1, 3: 2, 4: 2}) == Counter({0: 1, 1: 2, 2: 2}) <p>Q: Can you write a Python function that computes the average shortest path length of a network using the above function?</p> In\u00a0[6]: Copied! <pre># path_length_counter should be a Counter object with keys being path lengths \n# and values being the number of pairs with that path length.\npath_length_counter = Counter()\nG = nx.erdos_renyi_graph(1000, 0.01)\n\n# YOUR SOLUTION HERE\n    \nprint(path_length_counter)\n</pre> # path_length_counter should be a Counter object with keys being path lengths  # and values being the number of pairs with that path length. path_length_counter = Counter() G = nx.erdos_renyi_graph(1000, 0.01)  # YOUR SOLUTION HERE      print(path_length_counter) <pre>Counter({3: 51979, 4: 37416, 2: 9197, 1: 991, 5: 317, 0: 100})\n</pre> In\u00a0[7]: Copied! <pre>a_counter = Counter({0:10, 1:20, 2:10, 3:1})\na_counter.items()\n</pre> a_counter = Counter({0:10, 1:20, 2:10, 3:1}) a_counter.items() Out[7]: <pre>dict_items([(0, 10), (1, 20), (2, 10), (3, 1)])</pre> In\u00a0[8]: Copied! <pre>list(zip(*a_counter.items()))\n</pre> list(zip(*a_counter.items())) Out[8]: <pre>[(0, 1, 2, 3), (10, 20, 10, 1)]</pre> <p>Q: Can you draw the bar chart for the shortest path length distribution of the network that we calculated above?</p> In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n# YOUR SOLUTION HERE\n</pre> import matplotlib.pyplot as plt # YOUR SOLUTION HERE <p>Q: We can now also calculate the average path length of the whole network by averaging the path lengths.</p> In\u00a0[10]: Copied! <pre># calculate the average path length from the path_length_counter\n# YOUR SOLUTION HERE\nprint(average_path_length)\n</pre> # calculate the average path length from the path_length_counter # YOUR SOLUTION HERE print(average_path_length)  <pre>3.26571\n</pre> <p>Can you make it as a function?</p> In\u00a0[11]: Copied! <pre>def avg_path_length(G):\n    # YOUR SOLUTION HERE\n\navg_path_length(G)\n</pre> def avg_path_length(G):     # YOUR SOLUTION HERE  avg_path_length(G) Out[11]: <pre>3.276774</pre> <p>Ok! Now you can compute the average shortest path length of a network! \ud83c\udf89</p> <p>In practice, you'd not implement the BFS algorithm yourself, but use a library like NetworkX, igraph, graph-tools, etc. But, it is really good to understand what is happening under the hood!</p> In\u00a0[12]: Copied! <pre># Here's how you can do the same (average path length of a graph) with networkx\nnx.average_shortest_path_length(G)\n</pre> # Here's how you can do the same (average path length of a graph) with networkx nx.average_shortest_path_length(G) Out[12]: <pre>3.280054054054054</pre> <p>You may notice a small difference. I will leave it to you to identify why there is a difference (extra credit).</p> In\u00a0[13]: Copied! <pre>%%timeit\ntotal = 0\nfor i in range(1000000):\n    total += i\n  \n</pre> %%timeit total = 0 for i in range(1000000):     total += i    <pre>25.7 ms \u00b1 1.43 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[14]: Copied! <pre>%%time\ntotal = 0\nfor i in range(1000000):\n    total += i\n</pre> %%time total = 0 for i in range(1000000):     total += i <pre>CPU times: user 62.2 ms, sys: 2.17 ms, total: 64.4 ms\nWall time: 66.8 ms\n</pre> In\u00a0[\u00a0]: Copied! <pre># Use however many cells you need.\n\n# YOUR SOLUTION HERE\n</pre> # Use however many cells you need.  # YOUR SOLUTION HERE <p>Q: now make two plots that show two relationships.</p> <p>The first one is about the relationship between the number of nodes in your networks and their average path length. Are they correlated? You can test whether they have a roughly logarithmic relationship $ d \\sim \\log N$ or not.</p> <p>The second one is about the execution time and the number of nodes ($|V|$) &amp; the number of edges ($|E|$). Is it proportional to $|V|\\cdot|E|$?</p> In\u00a0[24]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE"},{"location":"m03-smallworld/lab03/#small-world-assignment","title":"Small world assignment\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m03-smallworld/lab03/#adjacency-matrix","title":"Adjacency matrix\u00b6","text":"<p>Adjacency matrix $\\mathbf{A}$ is an $N \\times N$ matrix, where each row $i$ corresponds to a node $i$ and each column $j$ corresponds to a node $j$. If the network is undirected and unweighted, the element $\\mathbf{A}_{ij}$ is 1 if there is an edge between node $i$ and node $j$, and 0 otherwise.</p> <p>In a directed graph, the element $\\mathbf{A}_{ij}$ can be used to represent either an edge from $i$ to $j$ or an edge from $j$ to $i$. We tend to use the latter ($j \\rightarrow i$) because this makes variuos matrix operations easier to understand (see WWND Ch. 25).</p>"},{"location":"m03-smallworld/lab03/#edges-and-degrees","title":"Edges and degrees\u00b6","text":"<p>Once you have an adjacency matrix, you can use it to compute various network properties. For example, the degree of a node $i$ is the sum of the elements in the $i$-th row (or column) of the adjacency matrix, i.e., $\\sum_j \\mathbf{A}_{ij}$.</p> <p>Q: can you write how we can calculate the number of edges in a network using the adjacency matrix? Assume that the network is undirected and unweighted.</p> <p>Explain as clearly as possible. If you're familiar with LaTeX, you can use LaTeX to write mathematical expressions.</p>"},{"location":"m03-smallworld/lab03/#how-to-they-look-like","title":"How to they look like?\u00b6","text":"<p>A useful exercise to build intuition into the adjacency matrix is to imagine how the adjacency matrix of a network would look like if we \"draw\" it. What I mean by that is to draw a square grid of size $N \\times N$, and then fill in the cells according to the adjacency matrix (e.g., black if 1 and white if 0).</p> <p>For instance, consider a random network where every edge is randomly placed with a probability $p$. What would the adjacency matrix look like? Think about it for a moment before you move on. Use the cell below to write down your thoughts (not graded).</p>"},{"location":"m03-smallworld/lab03/#walks-and-paths","title":"Walks and paths\u00b6","text":"<p>Adjacency matrix also allows us to mathematically think about walks and paths. The simplest case is a walk of length 1.</p> <p>Think about this question: How many walks of length 1 are there from node $i$ to node $j$?</p> Solution<p>It's just $\\mathbf{A}_{ji}$</p> <p>How about walks of length 2? Think about it first before revealing the solution.</p> Solution<p>You need to go to a node (say $k$) that is connected to $i$ first. And then you need to be able to go from $k$ to $j$. Therefore, it's $\\sum_k \\mathbf{A}_{ik} \\mathbf{A}_{kj}$</p> <p>Q: Can you generalize this to walks of length $l$ from $i$ to $j$?</p>"},{"location":"m03-smallworld/lab03/#computing-shortest-paths","title":"Computing shortest paths\u00b6","text":"<p>It is interesting to see the link between the adjacency matrix and the number of walks with a certain length. For the shortest paths, it is much easier to think about the problem computationally and algorithmically, especially when the network involves weights and directions. Let's first think about the BFS (breadth-first search) algorithm.</p>"},{"location":"m03-smallworld/lab03/#bfs-algorithm","title":"BFS algorithm\u00b6","text":"<p>When the network is unweighted, we can compute the shortest path and distance between any pair of nodes using the BFS algorithm. The algorithm is simple:</p> <ol> <li>Start from the source node $s$ and set the distance to itself as 0.</li> <li>Set the distance to all other nodes as $\\infty$. Or you can simply keep a set/dictionary of distances and check whether you have visited the node before.</li> <li>Add the source node to the queue.</li> <li>While the queue is not empty, do the following:<ol> <li>Pop the first node from the queue.</li> <li>For each neighbor of the popped node, if the distance is $\\infty$, set the distance to the distance of the popped node plus 1, and add the neighbor to the queue.</li> </ol> </li> </ol> <p>If it is unclear why this works, I encourage you to try to run the algorithm on a piece of paper with a simple network and watch how the distance is updated. Also there are many good videos on YouTube that explain the BFS algorithm that you can watch as well!</p> <p>Q: Can you write a Python function that computes, for every node, the shortest distance to a given node using the BFS algorithm?</p>"},{"location":"m03-smallworld/lab03/#visualizing-the-results","title":"Visualizing the results\u00b6","text":"<p>Now that you have a list of the shortest paths for the graph, we can examine it by drawing a bar chart of shortest path length distribution. We can use the following snippets to draw the bar chart.</p>"},{"location":"m03-smallworld/lab03/#how-does-it-scale","title":"How does it scale?\u00b6","text":"<p>Now go to https://icon.colorado.edu/#!/ and download multiple (at least three) networks that span a range of scale. For instance, pick a couple of networks with ~1000 nodes and then ~10000 nodes, and so on. Be careful with large networks! Calculating shortest paths is an expensive computation and it may take too much time (or not even finish)! Stick with not-so-large and not-too-small networks, but do experiment how far you can push.</p> <p>Q: Using your code above, calculate the average path length of each network and also measure how long it takes.</p> <p>You can use either <code>%%time</code> or <code>%%timeit</code> Jupyter magic commands or use a script. <code>%%timeit</code> runs the code multiple times to get a better estimate. So it may not be suitable for large networks.</p>"},{"location":"m04-weakties/lab04/","title":"Network visualization assignment","text":"<p>In this assignment, you will play with network visualization tools: Gephi and Cytoscape. Both software can not only visualize networks but also analyze them to some extent. You can use either one of them, but feel free to play with both. You may want to try  Gephi first, and then Cytoscape if you have trouble installing Gephi.</p> <p> </p>"},{"location":"m04-weakties/lab04/#network-visualization-assignment","title":"Network visualization assignment\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m04-weakties/lab04/#gephi","title":"Gephi\u00b6","text":"<p>Gephi became popular with its fast layout algorithms and powerful functionalities. However, it is currently not well maintained. As a result, the installation can be problematic, particularly in recent systems. There are some troubleshooting tips in the course wiki.</p> <p>You can download and install Gephi: https://gephi.org/. The following is a tutorial made by a previous AI, Nathaniel.</p> <p>Gephi tutorial</p> <p>Depending upon what version of Gephi you use the location of some buttons and tabs may vary, but the general functionality is roughly the same. If you want to know more about Gephi or what file formats it can read/write you can visit their documentation page.</p>"},{"location":"m04-weakties/lab04/#cytoscape","title":"Cytoscape\u00b6","text":"<p>As you can see from the name, Cytoscape is more geared towards biological networks because it was developed for biological network analysis. Still, it can visualize all kinds of networks.</p> <p>Additionally, it does not have many built-in network analysis functionalities. Instead it uses plug-in system and many plug-ins can be installed and used.</p> <p>Cytoscape is sustained by a larger community and federal funding, so it is more stable than Gephi in many ways. You can download it at http://www.cytoscape.org/ and check out the tutorials at: https://github.com/cytoscape/cytoscape-tutorials/wiki.</p>"},{"location":"m04-weakties/lab04/#assignment","title":"Assignment\u00b6","text":"<p>We'll use the Les Miserables graph, which can be downloaded from: http://www-personal.umich.edu/~mejn/netdata/. This site has other graphs which you can download. They are already in a format that Gephi or Cytoscape will recognize (<code>.net</code>, <code>.gexf</code>, <code>.gml</code>, etc).</p> <p>Once you have the Les Miserables graph do the following:</p> <ol> <li>Load the graph into Gephi/Cytoscape as an undirected graph.</li> <li>Calculate and record the number of nodes, number of edges, average degree, average clustering coefficient, and average path length of the graph.</li> <li>Save a plot of the degree distribution of the nodes. Note: if you use Cytoscape, you may need to do this step in networkx.</li> <li>Then, select a graph layout that you think best visualizes the\u00a0graph and save it.</li> <li>Create a new Jupyter notebook and use the random graph generator tool introduced in the last assignment to create a random graph with the same number of nodes and about the same average degree and number of edges as the real graph. Please set the random seed as <code>seed=42</code>.</li> <li>Save this random graph to a file format that Gephi/Cytoscape can read (see this page for Networkx read/write functions) and perform steps 1-4 with this new graph. You can save it as pajek (.net), gml, or gexf.</li> </ol> <p>You should then answer the following questions:</p> <ol> <li>How do the degree distributions of the real graph and random graph compare? Explain any differences or similarities you see and consider why they might exist.</li> <li>Is the real graph more clustered or less than the random one? What might this tell you about the organizing principles of the real graph?</li> <li>Does the real graph exhibit the small-world property?</li> </ol> <p>When complete, compile your results, figures, and analysis into a <code>.pdf</code> file and upload that, along with the Jupyter notebook you used to generate the random graph exported to a <code>.html</code> file.</p>"},{"location":"m05-scalefree/lab05/","title":"BA model and scale-free networks","text":"In\u00a0[1]: Copied! <pre>import random \n\nnode_list = [0,1,2,3,4,5]\nprint(random.sample(node_list, 1))\n</pre> import random   node_list = [0,1,2,3,4,5] print(random.sample(node_list, 1)) <pre>[1]\n</pre> <p>Can we make a list where node 0 is 7 times more likely to be chosen than node 1?</p> <p>A simple way to do this is to simply repeat the node 0 seven times in the list. Then, when we sample a node from the list, we are 7 times more likely to choose node 0 than node 1.</p> In\u00a0[2]: Copied! <pre>node_list = [0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5]\n[random.sample(node_list, 1) for i in range(10)]\n</pre> node_list = [0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5] [random.sample(node_list, 1) for i in range(10)] Out[2]: <pre>[[0], [0], [0], [0], [0], [4], [0], [0], [4], [2]]</pre> <p>In other words, if we simply repeat each node $i$ in the list $k_i$ (degree of node $i$), then the probability of choosing node $i$ is proportional to $k_i$. As you can imagine, this is not the most efficient way to do this, but it's a start.</p> <p>A more space-efficient way is using <code>numpy</code>'s sampling method. If you run the following cell, the documentation for the <code>np.random.choice</code> function will appear.</p> In\u00a0[3]: Copied! <pre>import numpy as np\n\nnp.random.choice?\n</pre> import numpy as np  np.random.choice? <pre>Docstring:\nchoice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n\n.. note::\n    New code should use the `~numpy.random.Generator.choice`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\n\nParameters\n----------\na : 1-D array-like or int\n    If an ndarray, a random sample is generated from its elements.\n    If an int, the random sample is generated as if it were ``np.arange(a)``\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\nreplace : boolean, optional\n    Whether the sample is with or without replacement. Default is True,\n    meaning that a value of ``a`` can be selected multiple times.\np : 1-D array-like, optional\n    The probabilities associated with each entry in a.\n    If not given, the sample assumes a uniform distribution over all\n    entries in ``a``.\n\nReturns\n-------\nsamples : single item or ndarray\n    The generated random samples\n\nRaises\n------\nValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\nSee Also\n--------\nrandint, shuffle, permutation\nrandom.Generator.choice: which should be used in new code\n\nNotes\n-----\nSetting user-specified probabilities through ``p`` uses a more general but less\nefficient sampler than the default. The general sampler produces a different sample\nthan the optimized sampler even if each element of ``p`` is 1 / len(a).\n\nSampling random rows from a 2-D array is not possible with this function,\nbut is possible with `Generator.choice` through its ``axis`` keyword.\n\nExamples\n--------\nGenerate a uniform random sample from np.arange(5) of size 3:\n\n&gt;&gt;&gt; np.random.choice(5, 3)\narray([0, 3, 4]) # random\n&gt;&gt;&gt; #This is equivalent to np.random.randint(0,5,3)\n\nGenerate a non-uniform random sample from np.arange(5) of size 3:\n\n&gt;&gt;&gt; np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\narray([3, 3, 0]) # random\n\nGenerate a uniform random sample from np.arange(5) of size 3 without\nreplacement:\n\n&gt;&gt;&gt; np.random.choice(5, 3, replace=False)\narray([3,1,0]) # random\n&gt;&gt;&gt; #This is equivalent to np.random.permutation(np.arange(5))[:3]\n\nGenerate a non-uniform random sample from np.arange(5) of size\n3 without replacement:\n\n&gt;&gt;&gt; np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\narray([2, 3, 0]) # random\n\nAny of the above can be repeated with an arbitrary array-like\ninstead of just integers. For instance:\n\n&gt;&gt;&gt; aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n&gt;&gt;&gt; np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\narray(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n      dtype='&lt;U11')\nType:      builtin_function_or_method</pre> <p>It accepts <code>p</code> parameter and you can specifies the probability of each item in the list! So instead of creating</p> In\u00a0[4]: Copied! <pre>alist = [1,1,1,1,1,1,2,2,2,3,3]\n</pre> alist = [1,1,1,1,1,1,2,2,2,3,3] <p>You can do the following.</p> In\u00a0[5]: Copied! <pre>nodes = [1,2,3]\ndegrees = [6,3,2]\nsum_degrees = sum(degrees)\nnode_probs = [degree*1.0/sum_degrees for node, degree in zip(nodes,degrees)] \nnode_probs\n</pre> nodes = [1,2,3] degrees = [6,3,2] sum_degrees = sum(degrees) node_probs = [degree*1.0/sum_degrees for node, degree in zip(nodes,degrees)]  node_probs Out[5]: <pre>[0.5454545454545454, 0.2727272727272727, 0.18181818181818182]</pre> <p>We can do more efficiently with numpy.</p> In\u00a0[6]: Copied! <pre># using numpy to calculate the probabilities of each node more efficiently\nnode_probs = np.array(degrees) / np.sum(degrees)\nnode_probs\n</pre> # using numpy to calculate the probabilities of each node more efficiently node_probs = np.array(degrees) / np.sum(degrees) node_probs Out[6]: <pre>array([0.54545455, 0.27272727, 0.18181818])</pre> <p>Now, we can sample from the list with corresponding probability.</p> In\u00a0[7]: Copied! <pre>[np.random.choice(nodes, p=node_probs) for i in range(10)]\n</pre> [np.random.choice(nodes, p=node_probs) for i in range(10)] Out[7]: <pre>[3, 1, 3, 1, 1, 2, 2, 3, 1, 1]</pre> <p>We can also sample multiple nodes at once (without replacement) from the list.</p> In\u00a0[8]: Copied! <pre>np.random.choice(nodes, 2, replace=False, p=node_probs)\n</pre> np.random.choice(nodes, 2, replace=False, p=node_probs) Out[8]: <pre>array([2, 1])</pre> <p>Ok, now the sampling can be done. How about the initial graph with <code>m0</code> nodes? <code>networkx</code> has a convenient function for that: https://networkx.org/documentation/stable/reference/generated/networkx.generators.classic.complete_graph.html</p> <p>Q: Can you create a complete graph with 5 nodes and then check the edges to make sure you have the right graph?</p> In\u00a0[9]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[9]: <pre>EdgeView([(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)])</pre> <p>Now you're ready to implement the BA algorithm! Create a fully-connected network with <code>m0</code> nodes. Then in each step, sample <code>m</code> nodes from the existing node pool. Add a new node and connect it to the chosen <code>m</code> nodes.</p> <p>Q: Can you implement the BA algorithm?</p> In\u00a0[10]: Copied! <pre>def barabasi_albert_graph(n, m0=5, m=2):\n    \"\"\"Create a BA network with n nodes, where each new node connects to \n    m existing nodes according to the preferential attachment rule. The initial\n    network is a clique (fully-connected network) with m0 nodes. \n    \"\"\"\n    # Initial network of m_o nodes (a complete graph)\n    # YOUR SOLUTION HERE\n\n    # Prepare a degree list and node probability list. Use numpy arrays for more efficient calculations.\n    # YOUR SOLUTION HERE\n\n    # Until network has n nodes, \n    # 1. *preferentially* sample m nodes from the network,\n    # 2. create a new node, \n    # 3. and connect the new node to the m selected nodes. \n\n    # YOUR SOLUTION HERE\n\n    return G        \n</pre> def barabasi_albert_graph(n, m0=5, m=2):     \"\"\"Create a BA network with n nodes, where each new node connects to      m existing nodes according to the preferential attachment rule. The initial     network is a clique (fully-connected network) with m0 nodes.      \"\"\"     # Initial network of m_o nodes (a complete graph)     # YOUR SOLUTION HERE      # Prepare a degree list and node probability list. Use numpy arrays for more efficient calculations.     # YOUR SOLUTION HERE      # Until network has n nodes,      # 1. *preferentially* sample m nodes from the network,     # 2. create a new node,      # 3. and connect the new node to the m selected nodes.       # YOUR SOLUTION HERE      return G         In\u00a0[11]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[11]: <pre>4.019531554072839</pre> <p>Q: Calculate (and print) the average clustering coefficient of the graph.</p> In\u00a0[12]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[12]: <pre>0.02681342349673337</pre> <p>The cumulative distribution function (CDF) and complementary cumulative distribution function (CCDF) are among the most direct ways to identify a power-law-like distribution. Plot the CCDF of the graph's degree distribution.</p> <p>First, CDF for a discrete distribution is defined as the following:</p> <p>$$ F_X(x) = P(X \\le x) = \\sum_{x' \\le x} P(x') $$</p> <p>CCDF is it's reverse and is defined as following:</p> <p>$$\\bar F_X(x) = P(X &gt; x) = \\sum_{x' &gt; x} P(x') = 1 - F_X(x).$$</p> In\u00a0[13]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef CCDF(degrees):\n    # YOUR SOLUTION HERE\n    pass\n\n\n# YOUR SOLUTION HERE\n</pre> import matplotlib.pyplot as plt  def CCDF(degrees):     # YOUR SOLUTION HERE     pass   # YOUR SOLUTION HERE  In\u00a0[14]: Copied! <pre>G_BA = barabasi_albert_graph(1000, m0=7, m=7)\nprint(\"number of nodes:\", G_BA.number_of_nodes())\nprint(\"number of edges:\", G_BA.number_of_edges())\n</pre> G_BA = barabasi_albert_graph(1000, m0=7, m=7) print(\"number of nodes:\", G_BA.number_of_nodes()) print(\"number of edges:\", G_BA.number_of_edges()) <pre>number of nodes: 1000\nnumber of edges: 6972\n</pre> In\u00a0[15]: Copied! <pre># you can use nx.gnm_random_graph(n, m) to create a random graph with n nodes and m edges.\n\n# YOUR SOLUTION HERE\n</pre> # you can use nx.gnm_random_graph(n, m) to create a random graph with n nodes and m edges.  # YOUR SOLUTION HERE <pre>number of nodes: 1000\nnumber of edges: 6972\n</pre> <p>Q: calculate the average path length in both graphs.</p> In\u00a0[16]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  <pre>Average shortest path length of G_BA: 2.731253253253253\nAverage shortest path length of G_random: 2.8844964964964963\n</pre> <p>Now plot the CCDF (for BA and ER) of the degree distribution of the random graph:</p> In\u00a0[17]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[18]: Copied! <pre>def barabasi_albert_graph_without_knowing_degrees(n, m0=5, m=2):\n    \"\"\"Create a BA network with n nodes, where each new node connects to \n    m existing nodes according to the preferential attachment rule. The initial\n    network is a clique (fully-connected network) with m0 nodes. \n\n    This function does not use the degree list and node probability list.\n    \"\"\"\n    # Create the initial network with m_o nodes (a complete graph)\n    # YOUR SOLUTION HERE\n\n    # now we add new nodes and grow the network by preferential attachment. \n    while(len(G.nodes()) &lt; n):\n        # how would you do the preferential attachment without knowing the degree of the nodes?\n        new_node = len(G.nodes())\n        # YOUR SOLUTION HERE\n        \n    return G            \n</pre> def barabasi_albert_graph_without_knowing_degrees(n, m0=5, m=2):     \"\"\"Create a BA network with n nodes, where each new node connects to      m existing nodes according to the preferential attachment rule. The initial     network is a clique (fully-connected network) with m0 nodes.       This function does not use the degree list and node probability list.     \"\"\"     # Create the initial network with m_o nodes (a complete graph)     # YOUR SOLUTION HERE      # now we add new nodes and grow the network by preferential attachment.      while(len(G.nodes()) &lt; n):         # how would you do the preferential attachment without knowing the degree of the nodes?         new_node = len(G.nodes())         # YOUR SOLUTION HERE              return G             <p>Q: build a network using this function and plot its CCDF to see if it has a power-law degree distribution.</p> In\u00a0[19]: Copied! <pre>G = barabasi_albert_graph_without_knowing_degrees(5000, m0=7, m=4)\n\n# YOUR SOLUTION HERE\n</pre> G = barabasi_albert_graph_without_knowing_degrees(5000, m0=7, m=4)  # YOUR SOLUTION HERE"},{"location":"m05-scalefree/lab05/#ba-model-and-scale-free-networks","title":"BA model and scale-free networks\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m05-scalefree/lab05/#implement-ba-algorithm","title":"Implement BA algorithm\u00b6","text":"<p>For this assignment you will be implementing the BA algorithm from the reading (see Barabasi Ch 5.3). Create a function that takes <code>n</code>, the number of nodes for the graph, and <code>m0</code> the initial number of nodes, as arguments and returns a networkx graph with a power-law degree distribution.</p> <p>The first step is figuring out how to do \"preferential attachment\" based on the degree of existing nodes. In other words, a node with degree 10 should be 10 times more likely to get a new edge than a node with degree 1 and 5 times more likely than a node with degree 2. How can we do this?</p> <p>If we just sample from a list containing all nodes, the probability of choosing a node is same for all nodes.</p>"},{"location":"m05-scalefree/lab05/#ba-graph-analysis","title":"BA graph analysis\u00b6","text":"<p>Q: Test your algorithm by creating a graph with $n = 1200$, $m_0 = 7$, and $m=2$. Calculate (and print) the average shortest path length of the graph:</p>"},{"location":"m05-scalefree/lab05/#ba-and-er-comparison","title":"BA and ER comparison\u00b6","text":"<p>Now let's compare the scale-free and random graphs. Create a random graph with the same number of nodes and about the same number of edges, then calculate the average shortest path length of that graph:</p>"},{"location":"m05-scalefree/lab05/#preferential-attachment-without-using-the-degree","title":"Preferential attachment without using the degree\u00b6","text":"<p>As you know from the discussion and videos, it is possible to achieve the preferential attachment without calculating the degree by using the friendship paradox. Implement this version and see whether you can get a power-law degree distribution.</p>"},{"location":"m06-centrality/lab06/","title":"Centralities: who's the most important?","text":"<pre>\n# YOUR SOLUTION HERE\n</pre> In\u00a0[1]: Copied! <pre>import networkx as nx\ndolphin_social_network = nx.read_gml('dolphins.gml')\n\n# number of nodes, edges, and average degree\nnum_nodes = len(dolphin_social_network.nodes())\nnum_edges = len(dolphin_social_network.edges())\navg_degree = num_edges / num_nodes\nprint('Number of nodes:', num_nodes)\nprint('Number of edges:', num_edges)\nprint('Average degree:', avg_degree)\n</pre> import networkx as nx dolphin_social_network = nx.read_gml('dolphins.gml')  # number of nodes, edges, and average degree num_nodes = len(dolphin_social_network.nodes()) num_edges = len(dolphin_social_network.edges()) avg_degree = num_edges / num_nodes print('Number of nodes:', num_nodes) print('Number of edges:', num_edges) print('Average degree:', avg_degree) <pre>Number of nodes: 62\nNumber of edges: 159\nAverage degree: 2.564516129032258\n</pre> In\u00a0[2]: Copied! <pre>nx.draw(dolphin_social_network)\n</pre> nx.draw(dolphin_social_network) In\u00a0[3]: Copied! <pre>import networkx as nx\n\nmy_graph = nx.erdos_renyi_graph(500, 0.3)\n\n# Get the eigenvector centralities for all the nodes\ncentralities = nx.eigenvector_centrality(my_graph)\n\n# Set the attributes of the nodes to include the centralities\n# The arguments are: &lt;graph&gt; &lt;attribute key&gt; &lt;values&gt;\n# Where &lt;values&gt; is a dictionary with keys=nodes\nnx.set_node_attributes(my_graph, centralities, \"eigenvector\")\n\n# Now we can refer to the node's attributes in the graph\nprint(my_graph.nodes[3][\"eigenvector\"])\n</pre> import networkx as nx  my_graph = nx.erdos_renyi_graph(500, 0.3)  # Get the eigenvector centralities for all the nodes centralities = nx.eigenvector_centrality(my_graph)  # Set the attributes of the nodes to include the centralities # The arguments are:   # Where  is a dictionary with keys=nodes nx.set_node_attributes(my_graph, centralities, \"eigenvector\")  # Now we can refer to the node's attributes in the graph print(my_graph.nodes[3][\"eigenvector\"]) <pre>0.04481164703666933\n</pre> <p>We want to do this so that we can export our graph as a <code>gexf</code> file using networkx's write_gexf function. Gexf is able to contain a lot more information than other graph datatypes like pajek. It can contain information about the node attributes or edge attributes that belong to the graph and then these attributes will be recognized by Gephi for plotting.</p> <p>Alternatively, if you use Cytoscape, you can export the centralities as a node property file, which is just a CSV contains the node IDs as the first column, and centralities as the other columns. Cytoscape can read CSV files through \"important data tables\" functionality: https://manual.cytoscape.org/en/stable/Node_and_Edge_Column_Data.html</p> <p>Once the graph is saved and you open it in Gephi or cytoscape, you can use the node (or edge) attributes to control node (or edge) size and color.</p> <p>You can then arrange your nodes accordingly and then save separate visualizations that only change the node color/size according to your saved attributes. You will be using this ability for the following questions.</p> <p>What to submit: For this assignment, submit a PDF that contains your responses to the questions in this notebook, including visualizations for the following questions. You do not need to submit a copy of this notebook if all of your solutions are in the PDF. Keep the node location the same for your graph visualizations.</p> In\u00a0[\u00a0]: Copied! <pre>nx.write_gexf(dolphin_social_network, \"dolphin_centrality.gexf\")\n</pre> nx.write_gexf(dolphin_social_network, \"dolphin_centrality.gexf\")"},{"location":"m06-centrality/lab06/#centralities-whos-the-most-important","title":"Centralities: who's the most important?\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)      <p>For this assignment we will be exploring several centralities to get an intuitive sense of what the various centrality metrics tell us about the nodes in the graph.</p>"},{"location":"m06-centrality/lab06/#degree-centrality-and-eigenvector-centrality","title":"Degree centrality and Eigenvector centrality\u00b6","text":"<p>Before messing with data, let's think about the two foundational centralities: degree centrality and eigenvector centrality.</p> <p>Degree centrality is simply defined as the number of edges connected to a node. If you imagine a social network where each node (person) can influence others, then the degree centrality captures the idea of how directly influential a person would be. This may be a good first approximation. If someone is connected to so many people (e.g., those with many followers on a social media platform), then it is likely that the person is influential.</p> <p>Still, this assumes that all connections (followers) are more or less equal. But in reality, some followers may be more influential than others. For instance, imagine a guru followed by hugely influential politicians and celebrities. Even if the guru may not be directly connected to many people, the guru's influence can be huge.</p> <p>This is where the eigenvector centrality comes in. The idea is an extension of the degree centrality. Instead of defining the centrality of a node as its degree, eigenvector centrality defines the centrality of a node as the sum of eigenvector centrality of the neighbors. In other words, an important node is not just a node with many neighbors, but the one with many important neighbors. We can think about repeating the following process:</p> <p>$$ x^{t+1}_i = \\frac{1}{\\lambda} \\sum_{j=1}^{n} A_{ij} x^{t}_j $$</p> <p>where $A$ is the adjacency matrix of the graph, $x$ is the eigenvector centrality vector, and $\\lambda$ is a normalization constant. It turns out, as you have learned, mathematically this idea can be translated into finding the eigenvector of the adjacency matrix that corresponds to the largest (and positive) eigenvalue.</p> <p>$$ \\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x} $$</p> <p>Here is a question: consider an undirected $k$-regular graph with only one connected component. That means that everyone can be reached from everyone else and every node's degree is $k$. What would be the eigenvector centrality vector of this graph?</p>"},{"location":"m06-centrality/lab06/#find-the-most-important-dolphin","title":"Find the most important dolphin!\u00b6","text":"<p>We will be using the Dolphin social network. (You may need to copy the link address into a new tab/window to trigger the download.) Download the graph and load it as a networkx graph.</p>"},{"location":"m06-centrality/lab06/#centrality-in-networkx","title":"Centrality in Networkx\u00b6","text":"<p>Networkx has several functions available for calculating the centralities of the nodes in the graph. There are functions for eigenvector, katz, closeness, betweenness, degree, etc. For a full list you can visit the documentation page.</p> <p>These functions take a graph as an argument and return a dictionary with nodes as keys and the centrality as values. This is convenient for us because we can set these as attributes for the nodes in the graph using the <code>set_node_attributes</code> function. For example:</p>"},{"location":"m06-centrality/lab06/#picking-the-right-dolphins","title":"Picking the right Dolphins\u00b6","text":"<p>Answer the following questions:</p>"},{"location":"m06-centrality/lab06/#1-popularity-contest","title":"(1) Popularity contest\u00b6","text":"<p>We want to know who the top dolphins are in the network, the real centers of attraction. Using what you learned about centrality from the readings and videos, choose an appropriate centrality measure that will tell us who those dolphins are. Justify your decision and list who the important dolphins are.</p>"},{"location":"m06-centrality/lab06/#2-relay","title":"(2) Relay\u00b6","text":"<p>Dolphins like passing information around efficiently along the shortest-paths. Among their neighbors who are the most important message relayers in the network? Justify your centrality choice for finding these dolphins.</p>"},{"location":"m06-centrality/lab06/#3-gossip","title":"(3) Gossip\u00b6","text":"<p>There is a lot smack going around the pod and everyone wants to know if Flipper will be inviting them to the party next week. But gossip takes time travel. Which dolphins are in the best position for getting all the best gossip from around the pod? Justify your centrality choice for finding these dolphins.</p>"},{"location":"m06-centrality/lab06/#your-solution-here","title":"YOUR SOLUTION HERE\u00b6","text":""},{"location":"m07-communities/class/","title":"Class","text":"<p>Probably the most basic way to quantify network community structure is in terms of categorical homophily. </p> <p>This paper overviews multiple perspectives for community detection.</p> <ul> <li>M. T. Schaub et al., The many facets of community detection in complex networks, Applied Network Science 2:4 (2017) https://doi.org/10.1007/s41109-017-0023-6</li> </ul>"},{"location":"m07-communities/lab07-2/","title":"Overlapping communities and statistical inference of communities","text":"<p>Another way to think about this situation is to use a bipartite network of people and communities. There are many people and many communities and each person can belong to multiple communities. Then, the social network we see is a projection of this bipartite network.</p> <p>Let's look at a simple example. Here, there are two communities. Most people belong to only one of them, but person \"D\" belongs to both.</p> In\u00a0[1]: Copied! <pre>import networkx as nx\n\npeople = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\ncommunities = [\"C1\", \"C2\"]\nmembership = {\n    \"A\": [\"C1\"],\n    \"B\": [\"C1\"],\n    \"C\": [\"C1\"],\n    \"D\": [\"C1\", \"C2\"],\n    \"E\": [\"C2\"],\n    \"F\": [\"C2\"],\n    \"G\": [\"C2\"],\n    \"H\": [\"C2\"],\n}\n\nB = nx.Graph()\nB.add_nodes_from(people, bipartite=0)\nB.add_nodes_from(communities, bipartite=1)\nB.add_edges_from(\n    [(person, community) for person in people for community in membership[person]]\n)\n\nnx.draw(\n    B,\n    with_labels=True,\n    node_color=[\"orange\" if B.nodes[n][\"bipartite\"] == 0 else \"blue\" for n in B.nodes],\n)\n</pre> import networkx as nx  people = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"] communities = [\"C1\", \"C2\"] membership = {     \"A\": [\"C1\"],     \"B\": [\"C1\"],     \"C\": [\"C1\"],     \"D\": [\"C1\", \"C2\"],     \"E\": [\"C2\"],     \"F\": [\"C2\"],     \"G\": [\"C2\"],     \"H\": [\"C2\"], }  B = nx.Graph() B.add_nodes_from(people, bipartite=0) B.add_nodes_from(communities, bipartite=1) B.add_edges_from(     [(person, community) for person in people for community in membership[person]] )  nx.draw(     B,     with_labels=True,     node_color=[\"orange\" if B.nodes[n][\"bipartite\"] == 0 else \"blue\" for n in B.nodes], ) <p>If you project this bipartite network to the network of people, then you will see a network with two overlapping communities. They overlap at node \"D\".</p> <p>Q: perform a bipartite projection of B to get the network of people P. And then draw the network</p> In\u00a0[2]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[4]: Copied! <pre>import linkcom\n\ne2c, S, D, Dlist = linkcom.cluster(P)\n</pre> import linkcom  e2c, S, D, Dlist = linkcom.cluster(P) <pre>clustering...\ncomputing similarities...\n# D_max = 1.000000\n# S_max = 0.125000\n</pre> <p>The <code>e2c</code> stores the edge-to-community mapping. It is a dictionary where the keys are the edges and the values are the community IDs.</p> In\u00a0[5]: Copied! <pre>e2c\n</pre> e2c Out[5]: <pre>{('F', 'G'): 14,\n ('A', 'D'): 6,\n ('A', 'C'): 6,\n ('C', 'D'): 6,\n ('E', 'G'): 14,\n ('D', 'F'): 14,\n ('A', 'B'): 6,\n ('D', 'H'): 14,\n ('B', 'D'): 6,\n ('B', 'C'): 6,\n ('D', 'G'): 14,\n ('G', 'H'): 14,\n ('D', 'E'): 14,\n ('F', 'H'): 14,\n ('E', 'F'): 14,\n ('E', 'H'): 14}</pre> <p>You can see that there are two communities 3 and 15. Now we can set edge attributes based on the community IDs.</p> In\u00a0[6]: Copied! <pre>nx.set_edge_attributes(P, e2c, \"linkcom\")\n</pre> nx.set_edge_attributes(P, e2c, \"linkcom\") <p>Q: Can you draw the network again with the edge colors based on the community IDs?</p> In\u00a0[7]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>Can you see now why link community approach can detect highly overlapping communities?</p> <p>Q: Now, download a small network where you expect to see pervasively overlapping community strcuture and try the link community detection method. Visualize the network by coloring the edges using the found community ID.</p> In\u00a0[7]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[8]: Copied! <pre>import random\n\nG = nx.Graph()\nG.add_nodes_from(range(20))\n\n# add community id as node attribute\nfor i in range(10):\n    G.nodes[i][\"community\"] = 0\nfor i in range(10, 20):\n    G.nodes[i][\"community\"] = 1\n\n# Add edges between nodes with probability 0.7 if they are in the same community, and 0.05 otherwise\n# YOUR SOLUTION HERE\n\nnx.draw(G, with_labels=True)\n</pre> import random  G = nx.Graph() G.add_nodes_from(range(20))  # add community id as node attribute for i in range(10):     G.nodes[i][\"community\"] = 0 for i in range(10, 20):     G.nodes[i][\"community\"] = 1  # Add edges between nodes with probability 0.7 if they are in the same community, and 0.05 otherwise # YOUR SOLUTION HERE  nx.draw(G, with_labels=True) <p>Congratulations! Now you implemented a simple stochastic block model. This model generated a network given a set of parameters. Assuming that we fix the total number of nodes, what are the parameters that we set?</p> <ol> <li>The number of communities $K$</li> <li>The community assignments of the nodes $z_i$ ($ 1 \\times N$ vector)</li> <li>The edge probabilities $p_{ij}$ ($K \\times K$ matrix)</li> </ol> In\u00a0[9]: Copied! <pre>coin_flip_data = [\"H\", \"T\", \"H\", \"H\", \"T\", \"T\", \"H\", \"T\", \"H\", \"H\"]\ntheta = 0.1  # parameter\n\n# likelihood\nlikelihood = 1.0\nfor x in coin_flip_data:\n    if x == \"H\":\n        likelihood *= theta\n    else:\n        likelihood *= 1 - theta\n\nlikelihood\n</pre> coin_flip_data = [\"H\", \"T\", \"H\", \"H\", \"T\", \"T\", \"H\", \"T\", \"H\", \"H\"] theta = 0.1  # parameter  # likelihood likelihood = 1.0 for x in coin_flip_data:     if x == \"H\":         likelihood *= theta     else:         likelihood *= 1 - theta  likelihood Out[9]: <pre>6.561000000000003e-07</pre> <p>We can make it as a function.</p> In\u00a0[10]: Copied! <pre>import numpy as np\n\n\ndef coin_flip_likelihood(theta, data):\n    return np.prod([theta if x == \"H\" else 1 - theta for x in data])\n</pre> import numpy as np   def coin_flip_likelihood(theta, data):     return np.prod([theta if x == \"H\" else 1 - theta for x in data]) <p>We can evaluate the likelihood for several values of $\\theta$.</p> In\u00a0[11]: Copied! <pre>for theta in np.linspace(0, 1, 10):\n    print(theta, coin_flip_likelihood(theta, coin_flip_data))\n</pre> for theta in np.linspace(0, 1, 10):     print(theta, coin_flip_likelihood(theta, coin_flip_data)) <pre>0.0 0.0\n0.1111111111111111 1.1747213274285833e-06\n0.2222222222222222 4.4070404799312956e-05\n0.3333333333333333 0.0002709614049348847\n0.4444444444444444 0.0007342008296428648\n0.5555555555555556 0.0011471887963169767\n0.6666666666666666 0.0010838456197395387\n0.7777777777777777 0.0005398624587915844\n0.8888888888888888 7.518216495542947e-05\n1.0 0.0\n</pre> <p>Or, plot a likelihood curve.</p> In\u00a0[12]: Copied! <pre># plot a likelihood curve\nimport matplotlib.pyplot as plt\n\nthetas = np.linspace(0, 1, 100)\n\n# YOUR SOLUTION HERE\n\nplt.xlabel(\"$\\\\theta$\")\nplt.ylabel(\"Likelihood\")\nplt.title(\"Likelihood of theta given coin flip data\")\nplt.show()\n</pre> # plot a likelihood curve import matplotlib.pyplot as plt  thetas = np.linspace(0, 1, 100)  # YOUR SOLUTION HERE  plt.xlabel(\"$\\\\theta$\") plt.ylabel(\"Likelihood\") plt.title(\"Likelihood of theta given coin flip data\") plt.show() <p>Can you find the best parameter now? How?</p> <p>Yes, when the likelihood is maximized, we get the best parameter according to this reasoning. This is called the maximum likelihood estimation (MLE) and it is one of the most common statistical inference method.</p> In\u00a0[13]: Copied! <pre>nx.draw(G, with_labels=True)\n</pre> nx.draw(G, with_labels=True) <p>To simplify things, let's assume that we fix the number of communities $K=2$ and the community assignments $z_i$ are known. Let's also assume that $p_{00} = p_{11}$ and $p_{01} = p_{10}$. Then, the only parameters that we need to infer are the edge probabilities $p_{ij}$, namely the two numbers $p_{00}$ and $p_{01}$.</p> <p>Note that given the data (the observed network) and parameters, we can calculate the likelihood. This is the probability of observing the network given the parameters. For instance, given a node pair $(i, j)$, the probability of observing an edge between them is $p_{z_i z_j}$ and the probability of not observing an edge between them is $1 - p_{z_i z_j}$.</p> <p>Then, the probability of observing the entire network is the product of these probabilities for all node pairs.</p> In\u00a0[14]: Copied! <pre>from itertools import combinations\n\np00 = 0.7\np01 = 0.05\n\n# likelihood\nlikelihood = 1.0\nfor i, j in combinations(G.nodes, 2):\n    # YOUR SOLUTION HERE\n\nlikelihood\n</pre> from itertools import combinations  p00 = 0.7 p01 = 0.05  # likelihood likelihood = 1.0 for i, j in combinations(G.nodes, 2):     # YOUR SOLUTION HERE  likelihood Out[14]: <pre>3.676841317675112e-40</pre> <p>Btw, do you note that the likelihood is extremely small? As we multiply many small numbers (probabilities), the result becomes extremely small. This can lead to all kinds of numerics problems and we often use the log-likelihood instead. Using log-likelihood makes the multiplication into addition and the numbers are much more manageable.</p> <p>Let's write a super simple function to calculate the log-likelihood of the SBM.</p> In\u00a0[15]: Copied! <pre>def sbm_likelihood(G, p00, p01):\n    log_likelihood = 0.0\n    for i, j in combinations(G.nodes, 2):\n        # YOUR SOLUTION HERE\n    return log_likelihood\n\n\nsbm_likelihood(G, 0.7, 0.05)\n</pre> def sbm_likelihood(G, p00, p01):     log_likelihood = 0.0     for i, j in combinations(G.nodes, 2):         # YOUR SOLUTION HERE     return log_likelihood   sbm_likelihood(G, 0.7, 0.05) Out[15]: <pre>-90.80134967375167</pre> <p>Now we can vary a parameter to see how the log-likelihood changes.</p> In\u00a0[16]: Copied! <pre>p00 = np.linspace(0.001, 0.9, 100)\np01 = 0.05\nlikelihoods = [sbm_likelihood(G, p00, p01) for p00 in p00]\nplt.plot(p00, likelihoods)\nplt.xlabel(\"$p_{00}$\")\nplt.ylabel(\"Log likelihood\")\nplt.title(\"Log likelihood of $p_{00}$ given graph G\")\n# draw a vertical line at 0.7\nplt.axvline(0.7, color=\"red\", linestyle=\"--\")\nplt.show()\n</pre> p00 = np.linspace(0.001, 0.9, 100) p01 = 0.05 likelihoods = [sbm_likelihood(G, p00, p01) for p00 in p00] plt.plot(p00, likelihoods) plt.xlabel(\"$p_{00}$\") plt.ylabel(\"Log likelihood\") plt.title(\"Log likelihood of $p_{00}$ given graph G\") # draw a vertical line at 0.7 plt.axvline(0.7, color=\"red\", linestyle=\"--\") plt.show() <p>Q: try with p01.</p> In\u00a0[17]: Copied! <pre>p00 = 0.7\np01_list = np.linspace(0.001, 0.9, 100)\n\n# YOUR SOLUTION HERE\n</pre> p00 = 0.7 p01_list = np.linspace(0.001, 0.9, 100)  # YOUR SOLUTION HERE <p>It's cool to see that the log-likelihood is maximized roughly around the true value of the parameter!</p> In\u00a0[18]: Copied! <pre># We can vary both. \np00_list = np.linspace(0.001, 0.9, 100)\np01_list = np.linspace(0.001, 0.9, 100)\nX, Y = np.meshgrid(p00_list, p01_list)\nlikelihoods = [[sbm_likelihood(G, p00, p01) for p00 in p00_list] for p01 in p01_list]\n\n# colormap\nplt.pcolor(X, Y, likelihoods)\nplt.colorbar(label=\"Log likelihood\")\nplt.xlabel(\"$p_{00}$\")\nplt.ylabel(\"$p_{01}$\")\nplt.title(\"Log likelihood of $p_{00}$ and $p_{01}$ given graph G\")\nplt.show()\n</pre> # We can vary both.  p00_list = np.linspace(0.001, 0.9, 100) p01_list = np.linspace(0.001, 0.9, 100) X, Y = np.meshgrid(p00_list, p01_list) likelihoods = [[sbm_likelihood(G, p00, p01) for p00 in p00_list] for p01 in p01_list]  # colormap plt.pcolor(X, Y, likelihoods) plt.colorbar(label=\"Log likelihood\") plt.xlabel(\"$p_{00}$\") plt.ylabel(\"$p_{01}$\") plt.title(\"Log likelihood of $p_{00}$ and $p_{01}$ given graph G\") plt.show() <p>As you can see, the true parameter value is close to the maximum likelihood estimate, as it should be!</p> <p>This is the basic idea of the inference from a generative model. If your model is good, then the best parameters may be those that explain the data best. Although there are much more complexity beyond this simple maximum likelihood estimation, this is the basic idea that you want to really understand. Being able to generate a data from a set of parameters allows us to evaluate how good the parameters are given the data, which in turn allows us to identiy good parameter values given data.</p> <p>Q: Can you identify the best parameters ($p_{00}$ and $p_{01}$) for the network that we generated, by finding the parameters that maximize the log likelihood?</p> In\u00a0[19]: Copied! <pre>p00_list = np.linspace(0.001, 0.9, 100)\np01_list = np.linspace(0.001, 0.9, 100)\nX, Y = np.meshgrid(p00_list, p01_list)\nlikelihoods = [[sbm_likelihood(G, p00, p01) for p00 in p00_list] for p01 in p01_list]\nlikelihoods = np.array(likelihoods)\n\n# YOUR SOLUTION HERE\n</pre> p00_list = np.linspace(0.001, 0.9, 100) p01_list = np.linspace(0.001, 0.9, 100) X, Y = np.meshgrid(p00_list, p01_list) likelihoods = [[sbm_likelihood(G, p00, p01) for p00 in p00_list] for p01 in p01_list] likelihoods = np.array(likelihoods)  # YOUR SOLUTION HERE <pre>p00_max: 0.718\np01_max: 0.110\nmax_log_likelihood: -87.83062259736532\n</pre> <p>Let's generate a network with these paraemters and compare the true network and the inferred network side by side. Note that we can generate as many networks as we want from the same MLE parameters. That means we can do all kinds of measurements on the generated networks and estimate the uncertainty of the measurements! This is one of the cool powers of the statistical inference approach!</p> In\u00a0[20]: Copied! <pre>G_MLE = nx.Graph()\nG_MLE.add_nodes_from(range(20))\n\n# add community id as node attribute\nfor i in range(10):\n    G_MLE.nodes[i][\"community\"] = 0\nfor i in range(10, 20):\n    G_MLE.nodes[i][\"community\"] = 1\n\n# Create G_MLE and then draw both G and G_MLE\n\n# YOUR SOLUTION HERE\n</pre> G_MLE = nx.Graph() G_MLE.add_nodes_from(range(20))  # add community id as node attribute for i in range(10):     G_MLE.nodes[i][\"community\"] = 0 for i in range(10, 20):     G_MLE.nodes[i][\"community\"] = 1  # Create G_MLE and then draw both G and G_MLE  # YOUR SOLUTION HERE"},{"location":"m07-communities/lab07-2/#overlapping-communities-and-statistical-inference-of-communities","title":"Overlapping communities and statistical inference of communities\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m07-communities/lab07-2/#whats-wrong-with-modularity-maximization-or-similar-methods","title":"What's wrong with modularity-maximization or similar methods?\u00b6","text":"<p>They are not necessarilly wrong! Those methods we tested last time may work perfectly fine for many networks and the community structure that you are interested in. However, there is an important implicit assumption shared by most community detection methods. That is, the communities are disjoint or non-overlapping, which  means that each node belongs to exactly one community.</p> <p>Imagine the social network around you. Do you belong to exactly one community? Probably not, because you have friends, family, and colleagues in many different contexts (a figure from Palla et al., 2005).</p> <p></p> <p>Moreover, this is not only true for yourself, but also for all other nodes in the network. If this is how the network is structured, then community detection methods that assume disjoint communities will not be able to capture the true community structure of the network.</p> <p></p>"},{"location":"m07-communities/lab07-2/#so-how-can-we-identify-strongly-overlapping-communities","title":"So, how can we identify strongly overlapping communities?\u00b6","text":"<p>If you apply Louvain method or any other method that assumes disjoint communities to this network, they will have to put node \"D\" into either one of the communities, which is not ideal. A common approach to overlapping community detection is to allow \"fuzzy\" boundaries between communities. There are many ways to do this, and in this simple example, this type of fuzzy overlapping community detection method will work perfectly fine.</p> <p>But, while keeping the example very simple, let's think about more complex cases where where the overlap is pervasive and each node can belong to multiple communities. Then the \"fuzzy\" community detection approach may not work well, again due to the limiting assumption that the communities are almost disjoint.</p> <p>Going beyond this \"fuzzy\" assumption, we can think about \"pervasive overlapping communities\" where each and every node can belong to multiple communities. This is the most challenging case, and it can also be the most realistic case for many real-world networks. There are, again, many ways to do this, but let's try a conceptually simple and illustrative method based on the concept of \"link communities\" (Paper), where we think about the communities of links (edges) rather than nodes. It also happened to be my method as well. \ud83d\ude48</p> <p>The key idea is that 'disjoint link communities' is a better approximation than 'disjoint node communities'. Even when every node belongs to multiple communities, the links between nodes can be partitioned into disjoint communities very well. Instead of trying to put nodes into communities, we put links into communities.</p> <p>The method first constructs a weighted \"line graph\" where each edge in the original network becomes a node and two nodes are connected if the corresponding edges share a node. The weights of the edges in the line graph are calculated based on the shared neighbors of the corresponding edges. Then, it applies a hierarchical clustering to the nodes of this line graph.</p> <p>Let's try. First, activate your virtual environment and install the package to your virtual environment by running the following command.</p> <pre>pip install git+https://github.com/Nathaniel-Rodriguez/linkcom.git\n</pre> <p>If you use <code>uv</code>, you can install it by running the following command.</p> <pre>uv pip install \"linkcom @ git+https://github.com/Nathaniel-Rodriguez/linkcom.git\"\n</pre> <p>Then we can import and run the method as following.</p>"},{"location":"m07-communities/lab07-2/#statistical-inference-approach-to-community-detection","title":"Statistical inference approach to community detection\u00b6","text":"<p>Pretty much all methods that we have discussed in the class so far are based on some heuristics and/or optimization algorithms. Although they may work perfectly fine for many cases, there is another powerful approach that we want to know about. That is the statistical inference approach to community detection. In this approach, we can be more explicit and flexible about the exact nature of the community structure that we are looking for. We can also quantify the uncertainty of the community structure that we infer and compare different community structures in a principled way.</p>"},{"location":"m07-communities/lab07-2/#stochastic-block-models","title":"Stochastic block models\u00b6","text":"<p>The key idea is that we can think of the community structure as a generative model of the network. That is, we assume that the network is generated by some underlying community structure and we want to infer this community structure from the network. If we have a probabilistic model of the network, we can use the observed network to infer the parameters of the model (Bayesian inference).</p> <p>The most fundamental model is the stochastic block model (SBM). In this model, the network is generated by the following process.</p> <ol> <li>Each node belongs to one of the $K$ communities.</li> <li>The probability of an edge between two nodes depends only on the community assignments of the two nodes.</li> </ol> <p>Let's try an example. Let's have a network with 20 nodes and 2 communities. Nodes 0-9 belong to community 0 and nodes 10-19 belong to community 1. The probability of an edge between two nodes is 0.7 if they belong to the same community and 0.05 if they belong to different communities.</p>"},{"location":"m07-communities/lab07-2/#bayes-theorem","title":"Bayes' theorem\u00b6","text":"<p>So how does this help us to detect communities? To understand this, let's look at the Bayes theorem:</p> <p>$$ P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} $$</p> <p>Here, $\\theta$ is the set of parameters of the model and $D$ is the observed data. If we ignore $P(D)$ and $P(\\theta)$ for a moment, we get the following.</p> <p>$$ P(\\theta | D) \\propto P(D | \\theta) $$</p> <p>What this means is that the probability of any set of parameter given what the data tells us ($P(\\theta | D)$) is proportional to the probability that the data is generated by the model with the parameters ($P(D | \\theta)$). This is the key idea of the Bayesian inference. If we have a generative model that can generate data from a set of parameters, then we can evaluate $P(D | \\theta)$, and this can in turn tell us how good the parameters are given the data $P(\\theta | D)$.</p> <p>Here $P(D | \\theta)$ is called the likelihood of the parameters given the data and $P(\\theta | D)$ is called the posterior probability of the parameters given the data.</p>"},{"location":"m07-communities/lab07-2/#maximum-likelihood-coin-flips","title":"Maximum likelihood Coin flips\u00b6","text":"<p>To understand this better, let's take a super quick detore to the coin flips. Here,</p> <ol> <li>Our data $D$ is the sequence of 10 coin flips: \"H\", \"T\", \"H\", \"H\", \"T\", \"T\", \"H\", \"T\", \"H\", \"H\".</li> <li>Our model is the Bernoulli distribution with the parameter $\\theta$ which is the probability of getting \"H\".</li> </ol> <p>Then, the likelihood ($P(D | \\theta)$) is the probability of getting the observed sequence of coin flips given the parameter $\\theta$. We can calculate this directly.</p>"},{"location":"m07-communities/lab07-2/#back-to-sbm","title":"Back to SBM\u00b6","text":"<p>Although the community detection problem with SBM is more complex, involving many more parameters, the basic idea is the same. Let's go back to the graph that we generated.</p>"},{"location":"m07-communities/lab07/","title":"Community detection","text":"In\u00a0[18]: Copied! <pre>import networkx as nx\n\noptions = {\n    'node_size': 100,\n    'width': 0.5,\n    'with_labels': False,\n}\n\nG = nx.barbell_graph(10, 0)\nnx.draw(G, node_color=\"black\", **options)\n</pre> import networkx as nx  options = {     'node_size': 100,     'width': 0.5,     'with_labels': False, }  G = nx.barbell_graph(10, 0) nx.draw(G, node_color=\"black\", **options) <p>An obvious way to identify the two groups is to \"cut\" the edge that connects the two complete graphs. This is the intuition behind the \"Girvan-Newman\" algorithm, which is one of the earliest and most intuitive community detection algorithms.</p> <p>This algorithm calculates the \"betweenness\" of each edge. Then, it removes the edge with the highest betweenness, and recalculates the betweenness of the edges. This process is repeated to break the network into smaller pieces. While doing so, we can measure the modularity of the network, which tells us when to stop the process.</p> <p>In this extreme case, there is a clear \"cut\" that separates the two groups. And that's exactly what the Girvan-Newman algorithm finds.</p> <p>Q: can you apply the Girvan-Newman algorithm to the barbell network?</p> In\u00a0[30]: Copied! <pre># YOUR SOLUTION HERE\n\ncommunities\n</pre> # YOUR SOLUTION HERE  communities Out[30]: <pre>([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19])</pre> In\u00a0[31]: Copied! <pre>node_color = [0] * len(G)\nfor i, comm in enumerate(communities):\n    for node in comm:\n        node_color[node] = i\n\nnx.draw(G, node_color=node_color, **options)\n</pre> node_color = [0] * len(G) for i, comm in enumerate(communities):     for node in comm:         node_color[node] = i  nx.draw(G, node_color=node_color, **options) <p>However, in reality, the network is not as clear-cut as the barbell network and the Girvan-Newman algorithm does not usually work well. It tends to destroy the network structure before finding the communities. Moreover, the algorithm is computationally expensive, as it requires calculating the betweenness of all edges (one of the slowest operations in network analysis).</p> <p>So we don't really use it anymore in practice. \ud83d\ude05</p> In\u00a0[32]: Copied! <pre># make a planted partition graph with 2 communities, each with 15 nodes. \n# use p_in = 0.7 and p_out = 0.05\n\n# YOUR SOLUTION HERE\n\nnx.draw(G_planted, node_color=\"black\", **options)\n</pre> # make a planted partition graph with 2 communities, each with 15 nodes.  # use p_in = 0.7 and p_out = 0.05  # YOUR SOLUTION HERE  nx.draw(G_planted, node_color=\"black\", **options) In\u00a0[34]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[36]: Copied! <pre>G_karate = nx.karate_club_graph()\n\n# YOUR SOLUTION HERE\n</pre> G_karate = nx.karate_club_graph()  # YOUR SOLUTION HERE  <p>Not exactly the community structure we expected! \ud83d\ude48</p> <p>Finding the exact two communities from this network does not mean that the algorithm is good; not finding the two communities also does not mean that the algorithm is bad! What is important is that we need to be very clear about what we want to find and how we evaluate the algorithm.</p>"},{"location":"m07-communities/lab07/#community-detection","title":"Community detection\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m07-communities/lab07/#intuition","title":"Intuition\u00b6","text":"<p>First, let's think about the extreme case. If a network consists of multiple groups that are barely connected, then we can expect that the network can be easily divided into these groups. Let's consider a \"barbell\" network, which consists of two complete graphs connected by a single edge.</p>"},{"location":"m07-communities/lab07/#divisive-vs-agglomerative","title":"Divisive vs. Agglomerative\u00b6","text":"<p>The Girvan-Newman algorithm is an example of a \"divisive\" community detection algorithm. It starts with the entire network and breaks it into smaller pieces until we find the communities. Another way to think about community detection is to start with individual nodes and \"merge\" them into communities. This is called \"agglomerative\" community detection. (statistical inference is a more principled approach and we will talk about it later.)</p> <p>In divisive algorithms, we focus on the boundary between communities. In agglomerative algorithms, we focus on the similarity or 'cohesion' within communities.</p>"},{"location":"m07-communities/lab07/#planted-partition-model","title":"Planted Partition Model\u00b6","text":"<p>Let's think about the \"barbell\" network again. Instead of saying that there should be only one edge that connects the two complete graphs, we can allow more edges to connect the two communities. We can also make each community not a complete graph, but just a dense subgraph. Both of these changes make the network less clear-cut.</p> <p>This is a special case of \"stoachastic block model\" (SBM), and commonly referred to as the \"planted partition model\" because we \"plant\" the \"partition\" (community structure) into the network. In this model, we set the probability of an edge between nodes in the same community ($p_{in}$) to be higher than the probability of an edge between nodes in different communities ($p_{out}$).</p> <p>Q: make a planted partition model with 2 communities with the following parameters.</p>"},{"location":"m07-communities/lab07/#louvain-algorithm","title":"Louvain algorithm\u00b6","text":"<p>Given a network, you can come up with many ways to cluster the nodes. For instance, you can define similarity between nodes by calculating neighbor overlap. Once you have the similarity matrix, you can use a clustering algorithm to find the communities. However, when similarity measure is not tied to what we want to find (communities), they tend not to work well in practice.</p> <p>The most successful (heuristic) algorithms tend to see this problem as an optimization problem (maximizing modularity or similar objective functions like map equation) and use heuristics to find the communities.</p> <p>As you learn from the course modules, the two best examples, also the most widely used community detection algorithms, are the Louvain algorithm and the Infomap algorithm. Here, let's play with the Louvain algorithm: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html#networkx.algorithms.community.louvain.louvain_communities</p> <p>Q: apply the Louvain algorithm to the planted partition model and visualize the communities.</p>"},{"location":"m07-communities/lab07/#zacharys-karate-club","title":"Zachary's Karate Club\u00b6","text":"<p>Now, let's look at a real-world network. Zachary's Karate Club is the most famous social network data in community detection literature. Since Girvan and Newman used this network to illustrate their algorithm, it has been widely used to showcase community detection algorithms. The network is very small and easy to grasp, although it is not necessarily representative of real-world networks with community structure, it has been used as a go-to example for community detection algorithms. Actually, it is so over-used that it became the subject of an in-joke in the network science community.</p> <p></p> <p>There is a \"club\" called \"The Zachary\u2019s Karate Club CLUB\" and an accompanying, traveling trophy: https://networkkarate.tumblr.com/ (you can also find me here...)</p> <p>The trophy holder should bring the trophy to the conferences and workshops, and then pass it to the person who shows the Zachary's Karate Club network in their presentation for the first time in the conference. \ud83d\ude02</p> <p>For the honor of the club, let's try the Louvain algorithm on the Zachary's Karate Club network.</p> <p>Q: apply the Louvain algorithm to the Zachary's Karate Club network and visualize the communities.</p>"},{"location":"m07-communities/lab07/#find-some-communities","title":"Find some communities!\u00b6","text":"<p>Now, pick a real-world network and find communities from the network! Pick a network where you expect to see fairly clear community structure (and small). Apply any community detection algorithm you like (Louvain, Infomap, etc.) and visualize the communities with whatever tool that you like (NetworkX, Gephi, etc.). Discuss the result. Did it find the structure that you expected? If not, why do you think it didn't work? Does the network community correspond to some groups in the real world? Why? Why not?</p> <p>You can either use this notebook or attach a separate document (PDF) with the visualization.</p>"},{"location":"m08-randomgraphs/lab08/","title":"Random graphs","text":"In\u00a0[1]: Copied! <pre>import networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndolphin_net = nx.read_gml(\"dolphins.gml\")\n</pre> import networkx as nx import matplotlib.pyplot as plt   dolphin_net = nx.read_gml(\"dolphins.gml\") <p>Q: Shall we measure the average clustering coefficent of this network?</p> In\u00a0[2]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <pre>Average clustering coefficient: 0.259\n</pre> <p>Let's then create many instances of ER random graphs with the exactly same number of nodes and edges as the dolphins network, and calculate the average clustering coefficient.</p> <p>Q: plot the distribution of the average clustering coefficient of many (say 1,000) ER random graphs.</p> In\u00a0[4]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>Q: where is the actual average clustering coefficient of the dolphins network in the distribution? Can you show it?</p> In\u00a0[5]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>It seems like the actual network has a much higher average clustering coefficient than the ER random graphs. This makes sense because the dolphins network is a social network, and social networks tend to have a high average clustering coefficient.</p> <p>But, let's think about the ER random graph model. Is the model a good null model for the dolphins network? Or, can it possibly be too random? Which of the important network properties does the ER random graph model not capture?</p> In\u00a0[6]: Copied! <pre># Say, there are 4 nodes with degree 1, 6 nodes with degree 2, etc. \ndegree_counts = {1: 4, 2: 6, 3: 6, 4: 5, 5: 4, 6: 3, 7: 2, 8: 1, 9: 1, 12: 1}\nplt.ylim(0, 7)\nplt.xticks(range(1, 13))\nfor d, c in degree_counts.items():\n    plt.plot([d, d], [0, c], 'k-')  # the pin/stem \n    plt.plot(d, c, 'ko')  # the \"head\"\n</pre> # Say, there are 4 nodes with degree 1, 6 nodes with degree 2, etc.  degree_counts = {1: 4, 2: 6, 3: 6, 4: 5, 5: 4, 6: 3, 7: 2, 8: 1, 9: 1, 12: 1} plt.ylim(0, 7) plt.xticks(range(1, 13)) for d, c in degree_counts.items():     plt.plot([d, d], [0, c], 'k-')  # the pin/stem      plt.plot(d, c, 'ko')  # the \"head\" <p>Q: can you plot the degree distribution of the dolphins network?</p> In\u00a0[7]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>Let's generate an instance of ER random graph with the same number of nodes and edges, and plot the degree distribution of the ER random graph.</p> <p>Q: can you plot the degree distribution of the ER random graph with the actual degree distribution of the dolphins network?</p> In\u00a0[9]: Copied! <pre># You can use offset to make the dots and lines distinguishable.\n\n# offset = 0.1\n# ...\n# \n# for d, c in degree_counts.items():\n#     plt.plot([d+offset, d+offset], [0, c], 'k-')\n#     plt.plot(d+offset, c, 'ko')\n#\n# for d, c in er_degree_counts.items():\n#     plt.plot([d-offset, d-offset], [0, c], 'r-')\n#     plt.plot(d-offset, c, 'ro')\n\n\n# YOUR SOLUTION HERE\n</pre> # You can use offset to make the dots and lines distinguishable.  # offset = 0.1 # ... #  # for d, c in degree_counts.items(): #     plt.plot([d+offset, d+offset], [0, c], 'k-') #     plt.plot(d+offset, c, 'ko') # # for d, c in er_degree_counts.items(): #     plt.plot([d-offset, d-offset], [0, c], 'r-') #     plt.plot(d-offset, c, 'ro')   # YOUR SOLUTION HERE <p>Although the difference is not dramatic, it seems to hint that the actual dolphin network may have slightly more nodes with high degrees than what we would expect from the ER random graph model. Let's check another network. This time, the famous karate club network.</p> <p>Q: create the same plots for the karate club network.</p> In\u00a0[10]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  <p>The discrepancy is even more dramatic in the karate club network.</p> <p>The thing is that the degree distribution is a very important property of the network. Degree distribution has an outsized impact on many network properties. For example, having hubs can dramatically decrease the average distance between nodes. Clustering coefficient is also affected by the degree distribution.</p> <p>In other words, the ER random graph model may be \"too random\" because it does not capture the degree distribution of the real networks. Is there any way to generate random graphs that exactly capture the degree distribution of the real networks, while keeping everything else random?</p> <p>That's where the \"configuration model\" comes in. The configuration model generates random graphs that have the same degree distribution as the real network. In other words, it keeps the degree sequence of the real network, in addition to the number of nodes and edges. You can picture the configuration model as follows:</p> <ol> <li>Break all the existing edges in the real network while keeping the \"ends\" of the edges (edge stubs) intact. If a node has degree $k$, then it keeps $k$ edge stubs.</li> <li>Randomly pick a pair of edge stubs and connect them.</li> <li>Repeat step 2 until all edge stubs are exhausted.</li> </ol> <p>Easy enough, right? Why don't you try a very small example by hand? You may encounter a few tricky cases. Did you find them?</p> Solution<p>Here's the tricky part. What if you are only left with two edge stubs connected to a single node? Should you connect them to create a self-loop? Or, should you just leave them unconnected? Or, what if two edge stubs that you pick are connected to the two nodes that are already connected? Should you connect them to create multiple edges between two nodes? Or should you find another pair?   <p>Another issue is that this process may generate a network with multiple components, even if the original network is connected. You want to be careful not to assume that the generated network is connected!</p> </p> <p>You can create an instance of the configuration model using <code>nx.configuration_model</code> as follows:</p> In\u00a0[11]: Copied! <pre>random_dolphin_net = nx.configuration_model([d for n, d in dolphin_net.degree()])\n\n# check multiple edges\nmultiedges = set()\nfor n1, n2 in random_dolphin_net.edges():\n    if random_dolphin_net.number_of_edges(n1, n2) &gt; 1:\n        multiedges.add((n1, n2))\nprint(\"Multiple edges:\", multiedges)\n\n# self loops\nselfloops = set()\nfor n in random_dolphin_net.nodes():\n    if random_dolphin_net.has_edge(n, n):\n        selfloops.add(n)\nprint(\"Self-loops:\", selfloops)\n\nnx.draw(random_dolphin_net, node_size=30, width=0.5, with_labels=True)\n</pre> random_dolphin_net = nx.configuration_model([d for n, d in dolphin_net.degree()])  # check multiple edges multiedges = set() for n1, n2 in random_dolphin_net.edges():     if random_dolphin_net.number_of_edges(n1, n2) &gt; 1:         multiedges.add((n1, n2)) print(\"Multiple edges:\", multiedges)  # self loops selfloops = set() for n in random_dolphin_net.nodes():     if random_dolphin_net.has_edge(n, n):         selfloops.add(n) print(\"Self-loops:\", selfloops)  nx.draw(random_dolphin_net, node_size=30, width=0.5, with_labels=True) <pre>Multiple edges: {(17, 29), (14, 20), (32, 33), (42, 47), (41, 45), (36, 51), (6, 51), (9, 33), (8, 15)}\nSelf-loops: {16, 33}\n</pre> <p>As you repeatedly run this code, you can probably see that the multiedges and self-loops are almost always present, and the network is sometimes disconnected.</p> <p>Always be aware of these issues! Don't just assume that the network generated from the configuration model is connected and has no multiedges or self-loops, all of which make it tricky to compute many network properties.</p> <p>You may be wondering, \"why don't we just rewire the multi-edges and self-loops until we don't have any?\" That's a good question! In fact, that's what people do to make the network more \"clean\". This can be done by 'edge swapping', a process that swaps a pair of edges so that all the degrees are preserved. We can keep doing this until we don't have any multi-edges or self-loops. The problem is that this process makes the network properties biased! So this is not perfect. To make the network truly random and unbiased, we need to allow multi-edges and self-loops.</p> In\u00a0[12]: Copied! <pre>g = nx.Graph(random_dolphin_net)\ng.remove_edges_from(nx.selfloop_edges(g))\n\n# check multiple edges\nmultiedges = set()\nfor n1, n2 in g.edges():\n    if g.number_of_edges(n1, n2) &gt; 1:\n        multiedges.add((n1, n2))\nprint(\"Multiple edges:\", multiedges)\n\n# self loops\nselfloops = set()\nfor n in g.nodes():\n    if g.has_edge(n, n):\n        selfloops.add(n)\nprint(\"Self-loops:\", selfloops)\n</pre> g = nx.Graph(random_dolphin_net) g.remove_edges_from(nx.selfloop_edges(g))  # check multiple edges multiedges = set() for n1, n2 in g.edges():     if g.number_of_edges(n1, n2) &gt; 1:         multiedges.add((n1, n2)) print(\"Multiple edges:\", multiedges)  # self loops selfloops = set() for n in g.nodes():     if g.has_edge(n, n):         selfloops.add(n) print(\"Self-loops:\", selfloops)  <pre>Multiple edges: set()\nSelf-loops: set()\n</pre> <p>Q: let's compare the average clustering coefficient of the real networks with the average clustering coefficient of the ER random graph and the configuration model.</p> In\u00a0[13]: Copied! <pre># Dolphins\n\n# YOUR SOLUTION HERE\n</pre> # Dolphins  # YOUR SOLUTION HERE   In\u00a0[29]: Copied! <pre># karate club\n\n# YOUR SOLUTION HERE\n</pre> # karate club  # YOUR SOLUTION HERE <p>Interesting! For the dolphin network, both random graph models produce very similar distributions of the average clustering coefficient, which is much smaller than the actual average clustering coefficient of the dolphin network. However, for the karate club network, the two distributions are quite different. According to the ER random graph model, the average clustering coefficient of the karate club network is much higher than what we would expect. On the other hand, if we consider the configuration model, the average clustering coefficient of the karate club network is not at all surprising, but it is what's expected from the degree distribution!</p> <p>This difference is likely due to the fact that the Karate Club network has several hubs with large degrees, which is not captured by the ER random graph model.</p> <p>Let's try another comparison, this time with a statistical test.</p> <p>We are going to do a rough test for our hypothesis by calculating the z-score for our dolphin network's degree assortativity coefficient. We will do this by generating a bunch of configuration models based on the dolphin degree sequence and calculating the assortativity coefficient for those null graphs. We can then compare our real network with the null model. If our z-score is high then it is unlikely that the dolphin network's assortativity can be accounted for by just the degree-sequence of a random graph, which means something more interesting is at work.</p> <p>Lets carry out this experiment:</p> In\u00a0[14]: Copied! <pre>import scipy.stats as stats\n\ndolphin_net = nx.read_gml(\"dolphins.gml\")\ndolphin_net = nx.Graph(dolphin_net)\n</pre> import scipy.stats as stats  dolphin_net = nx.read_gml(\"dolphins.gml\") dolphin_net = nx.Graph(dolphin_net)  <p>Now that the graph is loaded in lets calculate the degree assortativity:</p> In\u00a0[15]: Copied! <pre>real_assortativity = nx.degree_assortativity_coefficient(dolphin_net)\nprint(real_assortativity)\n</pre> real_assortativity = nx.degree_assortativity_coefficient(dolphin_net) print(real_assortativity) <pre>-0.04359402821531255\n</pre> <p>Interesting! So it appears that the network is disassortative, so it seems like our hypothesis about the dolphins is falling appart, but we still need to compare it to a null model in order to determine whether there is anything of interest to be pursued here.</p> <p>Q: can you calculate the degree assortativity of the configuration model and compare it to the degree assortativity of the dolphin network?</p> In\u00a0[16]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>Now we can calculate the z-scores using scipy's zscore function. It will print out the z-scores for all the elements of the distribution. If we assume that the dolphin network came from the null distribution then we can calculate the z-score for the dolphin network:</p> In\u00a0[17]: Copied! <pre># Prepend the real assortativity to the model list and have scipy calculate the zscores\nzscores = stats.zscore([real_assortativity] + model_assortativity)\n\n# Just print out the first score which corresponds to the real network\nprint(zscores[0])\n</pre> # Prepend the real assortativity to the model list and have scipy calculate the zscores zscores = stats.zscore([real_assortativity] + model_assortativity)  # Just print out the first score which corresponds to the real network print(zscores[0]) <pre>-0.006337357506093644\n</pre> <p>A z-score corresponds to how many standard deviations out from the mean the sample is. A score of +1 would be one standard deviation above the mean. Since the score is close to zero we can safely conclude that the disassortativity we observed in the dolphin network is explainable entirely from the degree sequence and no other special properties of the network.</p> <p>However, this doesn't mean there aren't low-level features hidden in the network. The degree assortativity score is an aggregative measure, and it can obfuscate local deviations from assortativity that are non-random. There is also a score for the local degree assortativity, but we will not pursue that further here.</p> <p>Now that we have a distribution of null assortativities we can also visualize the distribution and plot our dolphin network's assortativity along with it:</p> In\u00a0[18]: Copied! <pre>plt.hist(model_assortativity, bins=50)\nplt.axvline(real_assortativity, lw=1, color=\"red\") \nplt.show()\n</pre> plt.hist(model_assortativity, bins=50) plt.axvline(real_assortativity, lw=1, color=\"red\")  plt.show() <p>And there we have it! We have fair reason to conclude that not only is the hypothesis wrong but the surprising disassortativity we found really wasn't anything special after all. [note: reference for <code>axvline</code> in matplotlib]</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"m08-randomgraphs/lab08/#random-graphs","title":"Random graphs\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m08-randomgraphs/lab08/#what-are-the-random-graph-models","title":"What are the random graph models?\u00b6","text":"<p>\"Random graph models\" may refer a wide range of models where randomness is involved in the generation of the graph, particularly in the edge formation. But, the most basic random graph models \u2014 that are usually referred as random graph models \u2014 are the Erd\u0151s-R\u00e9nyi (ER) model and the configuration model. They both are commonly used as \"null models\" \u2014 the models that show us what we would expect if we don't know anything about the graph other than basic properties of the graphs \u2014 in network science.</p>"},{"location":"m08-randomgraphs/lab08/#why-random-graphs","title":"Why random graphs?\u00b6","text":"<p>Probably the most important reason to study random graphs is that they provide a baseline for comparison, or act as \"null models\". Let's say you collected a network data and found that the average clustering coefficient of the network is 0.5. Is this high or low?</p> <p></p> <p>If the network is large and very sparse, 0.5 may be almost impossibly high. On the other hand, if the network is extremely dense, then 0.5 may be extremely low. So, it is hard to say! What could be useful contexts?</p>"},{"location":"m08-randomgraphs/lab08/#er-random-graph","title":"ER random graph\u00b6","text":"<p>One useful context can be an ER random graph with the same number of nodes and edges. This model answers the following question: \"if we only know the number of nodes and edges, and nothing else, what would be the average clustering coefficient that we would expect?\" We can create many instances of ER random graphs and calculate the average clustering coefficient. Then, we can compare the average clustering coefficient of the real network with the distribution of the average clustering coefficient of the ER random graphs!</p> <p>Let's try with the dolphins network.</p>"},{"location":"m08-randomgraphs/lab08/#configuration-model","title":"Configuration model\u00b6","text":"<p>One very important property that the ER random graph model does not capture is, of course, the degree distribution. The degree distribution of the ER random graph is binomial (Poisson), which is very different from the degree distribution of the real networks. Let's check how different the degree distribution of the dolphins network is from the binomial distribution.</p> <p>Before doing this, here's a useful plot for showing the degree distribution of a small network.</p>"},{"location":"m08-randomgraphs/lab08/#lets-do-some-comparisons","title":"Let's do some comparisons!\u00b6","text":"<p>Let's try the clustering coefficient comparison that we did above with two networks (dolphins and karate club) and two null models (ER and configuration model). For multi-edges and self-loops, let's do a quick-and-dirty thing: just remove them.</p> <p>You can do the following to remove them.</p>"},{"location":"m08-randomgraphs/lab08/#testing-assortativity","title":"Testing assortativity\u00b6","text":"<p>Lets try another mock hypothesis using the dolphin social network. My hypothesis is that dolphins have a posh and very exclusive culture. Popular dolphins only hang with other popular dolphins, while the loners are stuck to mingle among themselves. A sad state of affairs, but social life is hard in the pods. How might we test this?</p> <p>There happens to be a similarity measure called assortativity where nodes of a certain type tend to be connected to nodes of the same type. In networkx there is a function called degree assortativity which tells us how strongly nodes of similar degree are attached to each other. If the network has high degree assortativity, then low degree nodes will be connected to other low degree nodes, and high degree nodes will be connected to other high degree nodes. Conversely, low degree assortativity (or even disassortativity) would imply no (or negative) relationship.</p> <p>Excellent, so we have a measure, but what do we compare our graph too? It wouldn't really be appropriate to compare it to an ER graph because the nodes all have different degrees which are about the same and normally distributed. Instead, we want to compare our dolphin network to a graph with the same degree distribution, and that is where the configuration model comes in.</p>"},{"location":"m08-randomgraphs/lab08/#now-try-it-yourself","title":"Now try it yourself!\u00b6","text":"<ol> <li><p>Pick a graph of your choosing. You can use the graphs you are using for your project or any other real-world graph. Here are a couple places you can find some: pajek datasets, Newman's datasets. [note: if you use directed graphs makes sure you use the corresponding function <code>directed_configuration_model</code>]</p> </li> <li><p>Construct two simple hypotheses around two different measures that you can calculate from the graph. Here is a list of algorithms that networkx has. You aren't limited by measures networkx can calculate. However, do not use a measure that depends entirely on the degree sequence. For instance, it would be silly to compare average degree to the random networks generated by the configuration model because it uses the same degree sequence and so will have the same average degree.</p> </li> <li><p>Carry out a simple hypothesis test for both hypotheses. It can be similar to what I showed here. You are welcome to carry it out in a more robust manner, but the focus of this assignment is getting a hang of using configuration models as null models. Here is a link to Statistics for Hackers. It has a nice little python tutorial for how you can carry out hypothesis testing without all the hard math. Even if you don't use it for this assignment I highly recommend checking it out.</p> </li> <li><p>Provide quantitative analysis and a graphical illustration of your results. It should be clear that your hypothesis was validated/invalidated/inconclusive.</p> </li> <li><p>Answer the following questions:</p> </li> </ol> <ul> <li>What graph are you using?</li> <li>What are you hypotheses?</li> <li>What measures will you be using to test your hypotheses? How do these measures accomplish this?</li> <li>Explain your results. Were they surprising? Did they confirm or reject your hypotheses?</li> <li>From these tests, what have you learned about the structure of the network you were investigating?</li> </ul> <ol> <li>Once you are complete submit your Jupyter notebook to Canvas.</li> </ol>"},{"location":"m09-netepi/lab09/","title":"Network Epidemiology","text":"<p>You may remember the March of 2020. \ud83d\udc40</p> <p>It was crazy how everything in our lives was suddenly governed by this new pathogen. It was a time when we all became armchair epidemiologists. We were all trying to understand the exponential growth of the virus and how it was spreading across the world. We were all trying to understand the basic reproduction number $R_0$ and how it was affecting the spread of the virus.</p> <p>Let's bring back some of that excitement(?) and learn how network perspectives can help us understand the spread of infectious diseases!</p> In\u00a0[1]: Copied! <pre>import networkx as nx\nimport random\n\nG = nx.barabasi_albert_graph(1000, 4)\n\n# sample a random node as the initial infected node (index case)\nindex_node = random.choice(list(G.nodes()))\nsecondary_infections = [\n    node for node in G.neighbors(index_node) if random.random() &lt; 0.5\n]\nprint(\"index: \", index_node)\nprint(\"secondary: \", secondary_infections)\nprint(\"degree of index: \", G.degree(index_node))\nprint(\"degree of secondary: \", [G.degree(node) for node in secondary_infections])\n</pre> import networkx as nx import random  G = nx.barabasi_albert_graph(1000, 4)  # sample a random node as the initial infected node (index case) index_node = random.choice(list(G.nodes())) secondary_infections = [     node for node in G.neighbors(index_node) if random.random() &lt; 0.5 ] print(\"index: \", index_node) print(\"secondary: \", secondary_infections) print(\"degree of index: \", G.degree(index_node)) print(\"degree of secondary: \", [G.degree(node) for node in secondary_infections]) <pre>index:  768\nsecondary:  [8, 71]\ndegree of index:  4\ndegree of secondary:  [56, 33]\n</pre> <p>Let's repeat many times to get the degree distribution (use CCDF).</p> In\u00a0[2]: Copied! <pre># repeat the simple simulation to obtain the degree distribution of the index and secondary nodes.\n# The degree distribution of the secondary nodes is expected to be different from the index node.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nN = 1000\ninfection_probability = 0.5\nindex_degrees = []\nsecondary_degrees = []\n\n# YOUR SOLUTION HERE\n</pre> # repeat the simple simulation to obtain the degree distribution of the index and secondary nodes. # The degree distribution of the secondary nodes is expected to be different from the index node. import matplotlib.pyplot as plt import numpy as np  N = 1000 infection_probability = 0.5 index_degrees = [] secondary_degrees = []  # YOUR SOLUTION HERE <p>As we expected, the secondary infection is more likely to happen to the nodes with higher degree. If you are well-connected, then you're more likely to get infected!</p> <p>Now, imagine a node with degree $k$ that has just infected by another node. If we think about the remaining degree that the infected node can further spread the disease, that's $k-1$ (because one of the edges is already used to infect the current node). Then the generating function for the remaining degree (that the node can propagate the disease) distribution is:</p> <p>$$ G_1(x) = \\frac{1}{\\langle k \\rangle} \\sum_k k p_k x^{k-1} = \\frac{G_0'(x)}{G_0'(1)}. $$</p> <p>This means that, if we randomly sample those who are infected, then we expect to see this distribution.</p> <p>Interestingly, when we do contact tracing, another factor comes into play. First, think about two directions of contact tracing. When we found an infected person, we can try to identify who infected this person (backward contact tracing) or whom this person infected (forward contact tracing).</p> <p>Forward tracing will produce the same distribution because we are simply following the edge to sample a new node, which is, according to the friendship paradox principle, the same as $G_1(x)$.</p> <p>By contrast, backward tracing will produce a different distribution. This is because the more \"offspring\" a node has created, the more likely it is to be sampled! This is another bias that comes into play.</p> <p>As you can see from this figure (d), the \"parent\" with more \"offsprings\" is more likely to be sampled and the factor is their degree in the transmission tree. If you work out the generating function for this distribution, it simply turns out to be:</p> <p>$$ G_2(x) = \\frac{G'_1(x)}{G'_1(1)} = \\frac{1}{\\sum_k k(k-1)p_k} \\sum_k k(k-1)p_k x^{k-2}. $$</p> <p>And this means that the degree distribution sampled from backward contact tracing process is even more biased towards the nodes with higher degree ($\\sim k^2 p_k$)!</p> <p>Here's an extra credit question (10pts): Can you show these three degree distributions by running simple simulations? It is essentially Fig. 2a in the paper: https://www.nature.com/articles/s41567-021-01187-2/figures/2</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>The first degree distribution is easy. It is just the degree distribution of the network. The second one is what we did above (randomly sampling infected nodes). You just need to plot the correct, expected degree distribution. The last one is a bit tricky. You'll need to simulate the transmission at least for a few steps and keep track of \"parents\" of the infected nodes so that we can trace back to them.</p> In\u00a0[3]: Copied! <pre>def initalize_graph(G):\n    for node in G.nodes():\n        G.nodes[node][\"state\"] = [\"S\"]\n    return G\n</pre> def initalize_graph(G):     for node in G.nodes():         G.nodes[node][\"state\"] = [\"S\"]     return G <p>Now we can randomly choose a certain fraction of nodes and mark them as initially infected using <code>choice()</code> function in numpy.</p> In\u00a0[10]: Copied! <pre>init_infected_fraction = 0.005\ninit_infected = set(\n    np.random.choice(\n        G.nodes(), size=int(len(G) * init_infected_fraction), replace=False\n    )\n)\ninit_infected\n</pre> init_infected_fraction = 0.005 init_infected = set(     np.random.choice(         G.nodes(), size=int(len(G) * init_infected_fraction), replace=False     ) ) init_infected Out[10]: <pre>{112, 453, 489, 704, 924}</pre> <p>We should set these nodes' states to be <code>I</code> and then we can begin the simulation.</p> In\u00a0[12]: Copied! <pre>G = initalize_graph(G)\nfor node in init_infected:\n    G.nodes[node][\"state\"][0] = \"I\"\nprint(G.nodes[0][\"state\"])\nprint(G.nodes[112][\"state\"])\n</pre> G = initalize_graph(G) for node in init_infected:     G.nodes[node][\"state\"][0] = \"I\" print(G.nodes[0][\"state\"]) print(G.nodes[112][\"state\"]) <pre>['S']\n['I']\n</pre> <p>But how should we update the states and run the simulation?</p> <p>There is an important issue here. If we immediately change the state of a particular node, that may affect the updating of other nodes! Imagine an extreme case where we are simulating a highly infectious disease on a chain-like network where node 0 is connected to node 1, node 1 is connected to node 0 and 2, and so on. In the simulation we are going through each node one by one from node 0 to node N and update them immediately. But it happens to be that node 0 was infected. Then, it can propagate to many nodes within a single time step because it is possible that node 1 gets updated to be infected by node 0, and then node 2 gets infected by node 1, and so on. This is clearly unrealistic.</p> <p>If we change a node's state, then the next node will be updating with respect to a network that is now in a different state!</p> <p>When modelling discrete-time dynamical systems there are generally two different update strategies: synchronous and asynchronous updating. In the asynchronous updating, a random node is picked and its state is updated according to the current network state. In the synchronous updating, there is a global time clock that all nodes are synced to, so nodes only update according to the state of the network at the current time-step and all nodes move to the next time step simultaneously.</p> <p>Choosing the updating scheme can have a huge impact on dynamics. We will be using the synchronous updating scheme, which means we need to store the next state of the system before updating everyone all at once. There are many ways to accomplish this, but for the sake of simplicity, we will just keep the whole history.</p> In\u00a0[13]: Copied! <pre>def run_SI(graph, tmax: int, beta: float, initial_inf: float):\n    \"\"\"Runs the SI model on the given graph with given parameters.\n\n    Parameters\n    ----------\n    graph : networkx graph object\n        The network (graph) on which the simulation will be run\n    tmax : int\n        The maximum time step that we will run the model\n    beta : float\n        The transmission probability\n    initial_inf : float\n        The initial fraction of infected nodes\n\n    Returns\n    -------\n    list[float]\n        the time-series of the fraction of infected nodes\n    \"\"\"\n    # First lets generate a set of initially infected nodes.\n    init_infected = set(\n        np.random.choice(\n            graph.nodes(), size=int(len(graph) * initial_inf), replace=False\n        )\n    )\n\n    # The code below uses a dictionary comprehension to generate a dictionary\n    # with keys=nodes and values=infection states. The \"I\" is for infected\n    # and \"S\" is for susceptible. We then give that dictionary to networkx's\n    # attribute function which then gives all the nodes the 'state' attribute.\n\n    # YOUR SOLUTION HERE\n\n    # Now we need to loop through for `tmax` time step. One time step equals to\n    # updating the whole network once.\n    for t in range(tmax):\n        for node in graph.nodes():\n            # Now we check if the node if susceptible to infection\n            # If it is, we need to determine the probability of it switching\n            # and then switch it for the next time-step\n            if graph.nodes[node][\"state\"][t] == \"S\":\n                # First determine how many infected neighbors the node has at time t:\n\n                # YOUR SOLUTION HERE\n\n                # Instead of drawing a bunch of random numbers for each neighbor\n                # we can just calculate the cumulative probability of getting\n                # infected since these events are independent and then just\n                # draw 1 random number to check against:\n                if np.random.random() &lt; (1 - (1 - beta) ** num_inf_neighbors):\n                    # If infection occurs we add a 1 to the state list of the node.\n                    # Note that by doing this we don't change how the other\n                    # nodes update, because they will be using time index t not t+1\n                    graph.nodes[node][\"state\"].append(\"I\")\n\n                else:\n                    # If no infection occurs, then just append the current state (0)\n                    graph.nodes[node][\"state\"].append(\"S\")\n\n            # Similarly, if the node is already infected it can't change back\n            # So we append the current state if it wasn't susceptible\n            else:\n                graph.nodes[node][\"state\"].append(\"I\")\n                \n    # the function returns a time series of the fraction of infected in each time step as a list.\n    # YOUR SOLUTION HERE\n</pre> def run_SI(graph, tmax: int, beta: float, initial_inf: float):     \"\"\"Runs the SI model on the given graph with given parameters.      Parameters     ----------     graph : networkx graph object         The network (graph) on which the simulation will be run     tmax : int         The maximum time step that we will run the model     beta : float         The transmission probability     initial_inf : float         The initial fraction of infected nodes      Returns     -------     list[float]         the time-series of the fraction of infected nodes     \"\"\"     # First lets generate a set of initially infected nodes.     init_infected = set(         np.random.choice(             graph.nodes(), size=int(len(graph) * initial_inf), replace=False         )     )      # The code below uses a dictionary comprehension to generate a dictionary     # with keys=nodes and values=infection states. The \"I\" is for infected     # and \"S\" is for susceptible. We then give that dictionary to networkx's     # attribute function which then gives all the nodes the 'state' attribute.      # YOUR SOLUTION HERE      # Now we need to loop through for `tmax` time step. One time step equals to     # updating the whole network once.     for t in range(tmax):         for node in graph.nodes():             # Now we check if the node if susceptible to infection             # If it is, we need to determine the probability of it switching             # and then switch it for the next time-step             if graph.nodes[node][\"state\"][t] == \"S\":                 # First determine how many infected neighbors the node has at time t:                  # YOUR SOLUTION HERE                  # Instead of drawing a bunch of random numbers for each neighbor                 # we can just calculate the cumulative probability of getting                 # infected since these events are independent and then just                 # draw 1 random number to check against:                 if np.random.random() &lt; (1 - (1 - beta) ** num_inf_neighbors):                     # If infection occurs we add a 1 to the state list of the node.                     # Note that by doing this we don't change how the other                     # nodes update, because they will be using time index t not t+1                     graph.nodes[node][\"state\"].append(\"I\")                  else:                     # If no infection occurs, then just append the current state (0)                     graph.nodes[node][\"state\"].append(\"S\")              # Similarly, if the node is already infected it can't change back             # So we append the current state if it wasn't susceptible             else:                 graph.nodes[node][\"state\"].append(\"I\")                      # the function returns a time series of the fraction of infected in each time step as a list.     # YOUR SOLUTION HERE <p>And there we have our SI model. The function is mostly comments, there are only a dozen lines of code involved in the whole process. Lets give it a run:</p> In\u00a0[14]: Copied! <pre>G = nx.barabasi_albert_graph(1000, 4)\nplt.plot(run_SI(G, tmax=20, beta=0.05, initial_inf=0.1))\nplt.xlabel(\"time\")\nplt.ylabel(\"fraction infected\")\nplt.show()\n</pre> G = nx.barabasi_albert_graph(1000, 4) plt.plot(run_SI(G, tmax=20, beta=0.05, initial_inf=0.1)) plt.xlabel(\"time\") plt.ylabel(\"fraction infected\") plt.show() <p>Let's try aggregating results from multiple runs. Let's do 50 runs and plot them all to see the variation.</p> In\u00a0[15]: Copied! <pre># Plot individual runs in light grey with small alpha (more transparent) and the average in blue\n\nG = nx.barabasi_albert_graph(1000, 4)\n\n# YOUR SOLUTION HERE\n</pre> # Plot individual runs in light grey with small alpha (more transparent) and the average in blue  G = nx.barabasi_albert_graph(1000, 4)  # YOUR SOLUTION HERE Out[15]: <pre>[&lt;matplotlib.lines.Line2D at 0x12c8eea50&gt;]</pre> <p>As you can see, it shows an S-shaped curve that is typical of the spread of infectious diseases. The number of infected nodes grows exponentially at first, then slows down to saturate as the number of susceptible nodes decreases.</p> <p>This type of average is commonly done, but a care must be taken when aggregating epidemic (forecast) curves. A very nice paper by Jonas Juul et al. showed that this type of aggregation (fixed-time descriptive statistics) can underestimate extreme cases.</p> <p>As you can see, the median curve (the black solid line) heavily underestimate how high the peak is expected to be. Our simulation does not show this because everything is aligned to the same time step, but this was a very important insight in the early days of the COVID-19 pandemic, when it is difficult to forecast when the epidemic curve would take off.</p> In\u00a0[115]: Copied! <pre>def run_SIR(graph: nx.Graph, tmax: int, beta: float, mu: float, initial_inf: float):\n    # YOUR SOLUTION HERE\n</pre> def run_SIR(graph: nx.Graph, tmax: int, beta: float, mu: float, initial_inf: float):     # YOUR SOLUTION HERE"},{"location":"m09-netepi/lab09/#network-epidemiology","title":"Network Epidemiology\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m09-netepi/lab09/#friendship-paradox-again","title":"Friendship paradox, again.\u00b6","text":"<p>The first and probably the most important insight that network science can provide to epidemiology is the biased sampling happening in an epidemic. This is the exactly same bias that we saw in the friendship paradox.</p> <p>Imagine a contact network through which a disease is spreading. Imagine that someone in the network just caught the virus and will spread to their neighbors. In this case, who will be the most likely to be the next victim of the virus? If every contact is equally likely to be infected, then the probability is proportional to the likelihood of them appearing in the contact list of the infected. And as we have learned multiple times, this is roughly proportional to the degree of the node due to the friendship paradox!</p> <p>Let's quickly simulate this.</p>"},{"location":"m09-netepi/lab09/#generating-functions-contact-tracing-and-friendship-paradox","title":"Generating functions, contact tracing, and friendship paradox\u00b6","text":"<p>One fascinating implication of this happens in the context of contact tracing, which I published during the pandemic: Kojaku et al., The effectiveness of backward contact tracing in networks, Nature Physics, 2021.</p> <p>Let's first write down the generating function for the degree distribution of the underlying network. With the degree distribution represented by $\\{ p_k \\}$, the probability generating function is written as:</p> <p>$$ G_0(x) = \\sum_k p_k x^k. $$</p>"},{"location":"m09-netepi/lab09/#si-model","title":"SI model\u00b6","text":"<p>Let's do a quick and dirty implementation of the simplest model, the SI model. In this model, every node is in one of the two states: susceptible (S) or infected (I). When a susceptible node is infected, it becomes infected and stays infected forever. The infected nodes can infect their neighbors with a certain probability.</p> <p>Given a network $G$, we can start with initializing the state of the nodes. Note that we are using a list of states to keep track of how the state changes over time for every node.</p>"},{"location":"m09-netepi/lab09/#sir-model","title":"SIR model\u00b6","text":"<p>SI model is useful to understand the early stage behavior of the epidemic. But the most commonly used model is the SIR model. In this model, the infected nodes can recover and become immune. For the sake of simplicity, you can assume that there is a recovery probability parameter that is the same for all nodes. In each time step, each infected node can recover and becomes immune with this probability.</p> <p>Q: implement the SIR model.</p> <ol> <li>Implement <code>run_SIR()</code>.</li> <li>Make a function that runs the SIR model and then produces a plot of \"S\", \"I\", \"R\" states (number of nodes) over time from the run. Show them in the same plot.</li> </ol>"},{"location":"m10-infodiff/lab10/","title":"Information diffusion on networks","text":"<p>Information diffusion or social contagion refers to all kinds of social phenomena where information, ideas, rumors, behaviors, beliefs, etc. spread through a network of people. This process is crucial in understanding our society. If you think about it, most social phenomena can be thought of as information diffusion processes and many challenges that our society is facing is often less about the lack of technology or resources but more about the challenge of effective information diffusion.</p> <p>Politics? It's about spreading a coherent set of ideas and beliefs about how our society should be run. Climate change? The biggest challenge is to convince people that it's real and urgent, so that the society can act on it. Fake news and influence campaigns? It's about spreading misinformation and manipulating people's beliefs and behaviors.</p> <p>So, how do we model and study information diffusion and what can network science help us understand about it?</p> In\u00a0[2]: Copied! <pre>import networkx as nx\nimport numpy as np\n\nG = nx.erdos_renyi_graph(10, 0.3, seed=42)\nnx.draw(G, with_labels=True)\n</pre> import networkx as nx import numpy as np  G = nx.erdos_renyi_graph(10, 0.3, seed=42) nx.draw(G, with_labels=True) <p>We will need to keep track of newly activated nodes, and then in each step, we need to 'try' all of their neighbors for the activation. So we need to be able to obtain the target nodes.</p> In\u00a0[3]: Copied! <pre>newly_activated = {0, 5}\n\ndef get_target_nodes(G, newly_activated):\n    target_nodes = set()\n    for node in newly_activated:\n        for neighbor in G.neighbors(node):\n            target_nodes.add(neighbor)\n    return target_nodes\n\nprint(list(G.neighbors(0)))\nprint(list(G.neighbors(5)))\nprint(get_target_nodes(G, newly_activated))\n</pre> newly_activated = {0, 5}  def get_target_nodes(G, newly_activated):     target_nodes = set()     for node in newly_activated:         for neighbor in G.neighbors(node):             target_nodes.add(neighbor)     return target_nodes  print(list(G.neighbors(0))) print(list(G.neighbors(5))) print(get_target_nodes(G, newly_activated)) <pre>[2, 3, 4, 8]\n[1, 2]\n{1, 2, 3, 4, 8}\n</pre> <p>Is this correct? \ud83e\udd28 Please take a look at the code closely and see if there is any problem.</p> <p>Q: Given the definition of independent cascade model, what is wrong with this code? Please fix it.</p> In\u00a0[4]: Copied! <pre>def get_target_nodes(G, newly_activated):\n    # YOUR SOLUTION HERE\n    return target_nodes\n\n# print(get_target_nodes(G, newly_activated))    # show the correct output (target nodes). \n</pre> def get_target_nodes(G, newly_activated):     # YOUR SOLUTION HERE     return target_nodes  # print(get_target_nodes(G, newly_activated))    # show the correct output (target nodes).  <p>Now we can implement the independent cascade model.</p> In\u00a0[36]: Copied! <pre>def indep_cascade(G: nx.Graph, seeds: set, p: float = 0.1):\n    \"\"\" Independent cascade model. \n    \n    returns a list that contains newly activated nodes at each time step.\n\n    \"\"\"\n    \n    activated = seeds.copy()\n    newly_activated = seeds.copy()\n    result = [newly_activated.copy()]\n\n    # YOUR SOLUTION HERE\n    \n    return result\n</pre> def indep_cascade(G: nx.Graph, seeds: set, p: float = 0.1):     \"\"\" Independent cascade model.           returns a list that contains newly activated nodes at each time step.      \"\"\"          activated = seeds.copy()     newly_activated = seeds.copy()     result = [newly_activated.copy()]      # YOUR SOLUTION HERE          return result  In\u00a0[37]: Copied! <pre>indep_cascade(G, {0, 5}, 0.4)\n</pre> indep_cascade(G, {0, 5}, 0.4) Out[37]: <pre>[{0, 5}, {2, 3, 4}, {1, 7, 9}, {6}, set()]</pre> <p>Let's visualize this. Because the network is small and we only have a few steps, we can visualize the network at each step.</p> In\u00a0[77]: Copied! <pre># visualize the result of indep_cascade. \nimport matplotlib.pyplot as plt\n\nG = nx.erdos_renyi_graph(10, 0.3, seed=42)\nlayout = nx.spring_layout(G)\n\nresult = indep_cascade(G, {0, 5}, 0.4)\n\n# create a subplot based on the number of time steps in result\nfig, axes = plt.subplots(1, len(result), figsize=(15, 4))\n\nactivated = set()\nfor i, newly_activated in enumerate(result):\n    activated |= newly_activated\n    ax = axes[i]\n    nx.draw(G, pos=layout, with_labels=True, ax=ax)\n    nx.draw_networkx_nodes(G, pos=layout, nodelist=activated, node_color='grey', ax=ax)\n    nx.draw_networkx_nodes(G, pos=layout, nodelist=newly_activated, node_color='r', ax=ax)\n    ax.set_title(f't={i}')\n\nplt.show()\n</pre> # visualize the result of indep_cascade.  import matplotlib.pyplot as plt  G = nx.erdos_renyi_graph(10, 0.3, seed=42) layout = nx.spring_layout(G)  result = indep_cascade(G, {0, 5}, 0.4)  # create a subplot based on the number of time steps in result fig, axes = plt.subplots(1, len(result), figsize=(15, 4))  activated = set() for i, newly_activated in enumerate(result):     activated |= newly_activated     ax = axes[i]     nx.draw(G, pos=layout, with_labels=True, ax=ax)     nx.draw_networkx_nodes(G, pos=layout, nodelist=activated, node_color='grey', ax=ax)     nx.draw_networkx_nodes(G, pos=layout, nodelist=newly_activated, node_color='r', ax=ax)     ax.set_title(f't={i}')  plt.show()  In\u00a0[78]: Copied! <pre>activated = {0, 5}\n\nlayout = nx.spring_layout(G)\nfig, ax = plt.subplots()\nnx.draw(G, pos=layout, with_labels=True, ax=ax)\nnx.draw_networkx_nodes(G, pos=layout, nodelist=activated, node_color='r', ax=ax)\n</pre> activated = {0, 5}  layout = nx.spring_layout(G) fig, ax = plt.subplots() nx.draw(G, pos=layout, with_labels=True, ax=ax) nx.draw_networkx_nodes(G, pos=layout, nodelist=activated, node_color='r', ax=ax) Out[78]: <pre>&lt;matplotlib.collections.PathCollection at 0x121182b10&gt;</pre> <p>The only possible nodes that can be activated are the ones that are connected to the activated nodes. So let's first find the set of nodes that we need to look at.</p> In\u00a0[79]: Copied! <pre>neighbors_of_activated = set()\nfor node in activated:\n    neighbors_of_activated |= set(G.neighbors(node))\n\ncandidates = neighbors_of_activated - activated\ncandidates\n</pre> neighbors_of_activated = set() for node in activated:     neighbors_of_activated |= set(G.neighbors(node))  candidates = neighbors_of_activated - activated candidates Out[79]: <pre>{1, 2, 3, 4, 8}</pre> <p>For each of these nodes, we need to check if the fraction of activated neighbors exceeds the threshold. To do so, let's first define a simple threshold function.</p> In\u00a0[80]: Copied! <pre>threshold = 0.4\n\ndef should_be_activated(G, node, activated, threshold):\n    # YOUR SOLUTION HERE\n\nfor node in candidates:\n    print(node, should_be_activated(G, node, activated, threshold))\n</pre> threshold = 0.4  def should_be_activated(G, node, activated, threshold):     # YOUR SOLUTION HERE  for node in candidates:     print(node, should_be_activated(G, node, activated, threshold)) <pre>1 False\n2 True\n3 False\n4 True\n8 False\n</pre> <p>Now we are ready to run the threshold model.</p> In\u00a0[81]: Copied! <pre>def threshold_model(G, seeds, threshold):\n    result = [seeds.copy()]\n    activated = seeds.copy()\n\n    # YOUR SOLUTION HERE\n\n    return result\n</pre> def threshold_model(G, seeds, threshold):     result = [seeds.copy()]     activated = seeds.copy()      # YOUR SOLUTION HERE      return result In\u00a0[82]: Copied! <pre>threshold_model(G, {0, 5}, 0.4)\n</pre> threshold_model(G, {0, 5}, 0.4) Out[82]: <pre>[{0, 5}, {2, 4}, {1}, {3, 9}, {6, 7, 8}]</pre> <p>Let's visualize this.</p> In\u00a0[122]: Copied! <pre>def visualize_contagion(G, result, g_kwargs={\"with_labels\": True}, node_kwargs={}):\n    # YOUR SOLUTION HERE\n\nvisualize_contagion(G, threshold_model(G, {0, 5}, 0.4))\n</pre> def visualize_contagion(G, result, g_kwargs={\"with_labels\": True}, node_kwargs={}):     # YOUR SOLUTION HERE  visualize_contagion(G, threshold_model(G, {0, 5}, 0.4)) In\u00a0[123]: Copied! <pre>visualize_contagion(G, threshold_model(G, {0, 5}, 0.5))\n</pre> visualize_contagion(G, threshold_model(G, {0, 5}, 0.5)) <p>Now, can you run these two models on the Watts-Strogatz networks?</p> <p>Q: play with model parameters and discuss the result. How does the contagion's behavior change as you vary the parameters? For instance, how well the threshold model spread as you change the randomization parameter of the Watts-Strogatz model?</p> In\u00a0[161]: Copied! <pre>G_ws = nx.watts_strogatz_graph(50, 10, 0.05, seed=42)   # Try different paramters. \nnx.draw(G_ws, with_labels=True)\n</pre> G_ws = nx.watts_strogatz_graph(50, 10, 0.05, seed=42)   # Try different paramters.  nx.draw(G_ws, with_labels=True) In\u00a0[162]: Copied! <pre>visualize_contagion(G_ws, threshold_model(G_ws, {0, 5}, 0.2), \n                    g_kwargs={\"with_labels\": False, \"node_size\": 10, \"width\": 0.1}, node_kwargs={\"node_size\": 10})\n</pre> visualize_contagion(G_ws, threshold_model(G_ws, {0, 5}, 0.2),                      g_kwargs={\"with_labels\": False, \"node_size\": 10, \"width\": 0.1}, node_kwargs={\"node_size\": 10}) In\u00a0[163]: Copied! <pre>visualize_contagion(G_ws, indep_cascade(G_ws, {0, 5}, 0.15), \n                    g_kwargs={\"with_labels\": False, \"node_size\": 10, \"width\": 0.1}, node_kwargs={\"node_size\": 10})\n</pre> visualize_contagion(G_ws, indep_cascade(G_ws, {0, 5}, 0.15),                      g_kwargs={\"with_labels\": False, \"node_size\": 10, \"width\": 0.1}, node_kwargs={\"node_size\": 10}) <pre>\n# YOUR SOLUTION HERE\n</pre>"},{"location":"m10-infodiff/lab10/#information-diffusion-on-networks","title":"Information diffusion on networks\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m10-infodiff/lab10/#models-of-information-diffusion","title":"Models of information diffusion\u00b6","text":"<p>Because of an obvious similarity and parallel between the spread of information and the spread of diseases, many (early) models of information diffusion are inspired by epidemic models. Many studies on information diffusion these days do use epidemic models. When we do that, we can bring all the knowledge, models, and techniques developed in the field of epidemiology.</p> <p>Another simple model that was inspired by these epidemic model is the Independent Cascade Model. In this model, we assume a set of seed nodes that are initially \"infected\" (or active), whereas all other nodes are \"susceptible\" or inactive. In each time step, a newly activated node can activate its neighbors with a certain probability. Each activated node has only one chance to activate its neighbors and if a node has multiple activated neighbors, each of these neighbors can \"try\" to activate the node independently. In other words, if a node has three newly activated neighbors, the probability that it will be activated is not simply $p$, but $1 - (1 - p)^3$.</p> <p>Thanks to its simplicity, this model is widely used in the study of information diffusion, particularly for the problem of influence maximization (choosing the best seeds to maximize the spread of information).</p> <p>But, as you have read, there are also many pieces of evidence that suggest that information diffusion is not like disease spreading because one's adoption of a behavior or belief is often strongly modulated by social signals. As more and more people around you adopt a behavior, you are more likely to adopt it. This is called social reinforcement and it can produce a very different pattern of information diffusion compared to the epidemic models or the independent cascade model.</p>"},{"location":"m10-infodiff/lab10/#independent-cascade-model","title":"Independent cascade model\u00b6","text":"<p>Let's try the independent cascade model first. Let's simplify the model by assumping a fixed probability $p$ that a node will activate its neighbor. Let's first create a small network to play with.</p>"},{"location":"m10-infodiff/lab10/#threshold-model","title":"Threshold model\u00b6","text":"<p>Now let's implement the threshold model, which captures the social reinforcement mechanism. Imagine a situation with peer pressure. For instance, if everyone in your circle of friends is adopting a new gadget, social media, new sports, or anything, each additional adoption by your friends makes you more likely to adopt it. This is exactly what exactly the threshold model captures.</p> <p>In the threshold model, each node has a threshold $T_i$. When the fraction of activated neighbors exceeds the threshold, the node will be activated. Also, once activated, the node will stay activated. For simplicity, we will assume a fixed threshold for all nodes.</p> <p>Let's start with the same network.</p>"},{"location":"m11-ml/lab11/","title":"Machine learning and graph embeddings","text":"In\u00a0[1]: Copied! <pre>import networkx as nx\n\nG = nx.barbell_graph(6, 0)\n\nfor i in range(6):\n    G.nodes[i]['color'] = 'red'\nfor i in range(6, 12):\n    G.nodes[i]['color'] = 'lightblue'\n\nnx.draw(G, with_labels=True, node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)\n</pre> import networkx as nx  G = nx.barbell_graph(6, 0)  for i in range(6):     G.nodes[i]['color'] = 'red' for i in range(6, 12):     G.nodes[i]['color'] = 'lightblue'  nx.draw(G, with_labels=True, node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8) In\u00a0[2]: Copied! <pre>from collections import Counter\n\nfor node in G.nodes:\n    # YOUR SOLUTION HERE\n    \n    print(\"Node id:\", node)\n    print(f\"predicted by neighbors: {most_common_color}\")\n    print(f\"actual color:           {G.nodes[node]['color']}\")\n    print()\n</pre> from collections import Counter  for node in G.nodes:     # YOUR SOLUTION HERE          print(\"Node id:\", node)     print(f\"predicted by neighbors: {most_common_color}\")     print(f\"actual color:           {G.nodes[node]['color']}\")     print() <pre>Node id: 0\npredicted by neighbors: red\nactual color:           red\n\nNode id: 1\npredicted by neighbors: red\nactual color:           red\n\nNode id: 2\npredicted by neighbors: red\nactual color:           red\n\nNode id: 3\npredicted by neighbors: red\nactual color:           red\n\nNode id: 4\npredicted by neighbors: red\nactual color:           red\n\nNode id: 5\npredicted by neighbors: red\nactual color:           red\n\nNode id: 6\npredicted by neighbors: lightblue\nactual color:           lightblue\n\nNode id: 7\npredicted by neighbors: lightblue\nactual color:           lightblue\n\nNode id: 8\npredicted by neighbors: lightblue\nactual color:           lightblue\n\nNode id: 9\npredicted by neighbors: lightblue\nactual color:           lightblue\n\nNode id: 10\npredicted by neighbors: lightblue\nactual color:           lightblue\n\nNode id: 11\npredicted by neighbors: lightblue\nactual color:           lightblue\n\n</pre> <p>A perfect prediction!</p> <p>As you can imagine, this strategy will become too simplistic as the network becomes more complex and noisy. But remember that what we just did is essentially the core idea of many, much fancier, machine learning algorithms including graph neural networks (GNNs). GNNs aggregate the information from the neighbors and combine with the node features to produce a new vector, which in turn can be used for more processing or prediction.</p> <p>Because of the usage of neural networks, GNNs are much more flexible than just counting the number of neighbors with a certain property. They can also utilize many features of the nodes and edges, not just the class labels. Still the idea is not that far!</p> In\u00a0[3]: Copied! <pre>nx.draw(G, with_labels=True, node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)\n</pre> nx.draw(G, with_labels=True, node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8) <p>Now, mark some edges to pretend that they are missing.</p> In\u00a0[4]: Copied! <pre># mark some edges to be missing. Set the edge attribute 'missing' to True\nimport random \n\nmissing_edges = random.sample(list(G.edges), 5)\nmissing_edges\n</pre> # mark some edges to be missing. Set the edge attribute 'missing' to True import random   missing_edges = random.sample(list(G.edges), 5) missing_edges Out[4]: <pre>[(0, 4), (2, 4), (9, 10), (8, 11), (10, 11)]</pre> <p>We can also sample \"negative\" edges, which are not present in the network.</p> In\u00a0[5]: Copied! <pre># sample negative edges\nnegative_edges = set()\nwhile len(negative_edges) &lt; 5:\n    edge = tuple(random.sample(list(G.nodes), 2))\n    if edge not in G.edges:\n        negative_edges.add(edge)\nnegative_edges\n</pre> # sample negative edges negative_edges = set() while len(negative_edges) &lt; 5:     edge = tuple(random.sample(list(G.nodes), 2))     if edge not in G.edges:         negative_edges.add(edge) negative_edges Out[5]: <pre>{(0, 7), (8, 2), (9, 1), (10, 5), (11, 2)}</pre> <p>Let's draw these two see where they are located.</p> In\u00a0[6]: Copied! <pre>edge_colors = []\nfor edge in G.edges:\n    if edge in missing_edges:\n        edge_colors.append('blue')\n    else:\n        edge_colors.append('black')\n\npos = nx.spring_layout(G)\n\nnx.draw(G, with_labels=True, pos=pos, \n        node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8, edge_color=edge_colors)\nnx.draw_networkx_edges(G, pos=pos, edgelist=negative_edges, edge_color='red')\n</pre> edge_colors = [] for edge in G.edges:     if edge in missing_edges:         edge_colors.append('blue')     else:         edge_colors.append('black')  pos = nx.spring_layout(G)  nx.draw(G, with_labels=True, pos=pos,          node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8, edge_color=edge_colors) nx.draw_networkx_edges(G, pos=pos, edgelist=negative_edges, edge_color='red') Out[6]: <pre>&lt;matplotlib.collections.LineCollection at 0x1152c0f90&gt;</pre> <p>What do you see?</p> <p>Yes! The missing edges are mostly between the nodes of the same color and the negative edges are mostly between the nodes of different colors. This is again due to the strong community structure that arises from the homophily. Because there are more edges within the same community, it's more likely that the missing edges are also within the same community; because there are fewer edges between the different communities, it's more likely that the negative edges are between the different communities.</p> <p>Then, what would be simple strategies to predict the missing edges correctly, or distinguish missing edges from the negative edges?</p> In\u00a0[7]: Copied! <pre>for edge in missing_edges:\n    if G.has_edge(*edge):\n        G.remove_edge(*edge)\n\nnx.draw(G, with_labels=True, pos=pos, \n        node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)\n</pre> for edge in missing_edges:     if G.has_edge(*edge):         G.remove_edge(*edge)  nx.draw(G, with_labels=True, pos=pos,          node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8) <p>Now, for every pair of disconnected nodes, we calculate the Adamic-Adar index.</p> In\u00a0[8]: Copied! <pre># calculate adamic-adar index for every pair of disconnected nodes\nimport numpy as np\nfrom itertools import combinations\n\naa_index = {}\nfor u, v in combinations(G.nodes, 2):\n    if G.has_edge(u, v):\n        continue\n    # YOUR SOLUTION HERE\n\n    # let's only keep the non-zero ones\n    if adamic_adar_index &gt; 0:\n        aa_index[(u, v)] = adamic_adar_index\n\nsorted(aa_index.items(), key=lambda x: x[1], reverse=True)\n</pre> # calculate adamic-adar index for every pair of disconnected nodes import numpy as np from itertools import combinations  aa_index = {} for u, v in combinations(G.nodes, 2):     if G.has_edge(u, v):         continue     # YOUR SOLUTION HERE      # let's only keep the non-zero ones     if adamic_adar_index &gt; 0:         aa_index[(u, v)] = adamic_adar_index  sorted(aa_index.items(), key=lambda x: x[1], reverse=True) Out[8]: <pre>[((8, 11), 1.6933439034806097),\n ((9, 10), 1.6933439034806097),\n ((0, 4), 1.6301195954722452),\n ((2, 4), 1.6301195954722452),\n ((10, 11), 1.072008968920998),\n ((0, 6), 0.5138983423697507),\n ((1, 6), 0.5138983423697507),\n ((2, 6), 0.5138983423697507),\n ((3, 6), 0.5138983423697507),\n ((4, 6), 0.5138983423697507),\n ((5, 7), 0.5138983423697507),\n ((5, 8), 0.5138983423697507),\n ((5, 9), 0.5138983423697507),\n ((5, 10), 0.5138983423697507),\n ((5, 11), 0.5138983423697507)]</pre> <p>Let's draw them on top of the network. Draw them with ligh grey color and use the thickness of the line to represent the value of the Adamic-Adar index.</p> In\u00a0[9]: Copied! <pre>nx.draw(G, with_labels=True, pos=pos, \n        node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)\n\nedge_width = [aa_index[edge] * 2 for edge in aa_index]\n\nnx.draw(G, with_labels=True, pos=pos,\n        node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)\n\n# draw potential edges based on adamic-adar index\nnx.draw_networkx_edges(G, pos=pos, edgelist=aa_index.keys(), width=edge_width, edge_color='lightgrey')\n</pre> nx.draw(G, with_labels=True, pos=pos,          node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)  edge_width = [aa_index[edge] * 2 for edge in aa_index]  nx.draw(G, with_labels=True, pos=pos,         node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)  # draw potential edges based on adamic-adar index nx.draw_networkx_edges(G, pos=pos, edgelist=aa_index.keys(), width=edge_width, edge_color='lightgrey')  Out[9]: <pre>&lt;matplotlib.collections.LineCollection at 0x11537e550&gt;</pre> <p>Cool! It seems like all the top predictions (large Adamic-Adar index) are located within the same community while those with small Adamic-Adar index are located between the different communities. We can compare this with the actual missing edges (positive) and negative edges.</p> In\u00a0[10]: Copied! <pre>import pandas as pd\n\n# YOUR SOLUTION HERE\n\naa_index_df\n</pre> import pandas as pd  # YOUR SOLUTION HERE  aa_index_df   Out[10]: edge adamic_adar_index missing negative 0 (8, 11) 1.693344 True False 1 (9, 10) 1.693344 True False 2 (0, 4) 1.630120 True False 3 (2, 4) 1.630120 True False 4 (10, 11) 1.072009 True False 5 (0, 6) 0.513898 False False 6 (1, 6) 0.513898 False False 7 (2, 6) 0.513898 False False 8 (3, 6) 0.513898 False False 9 (4, 6) 0.513898 False False 10 (5, 7) 0.513898 False False 11 (5, 8) 0.513898 False False 12 (5, 9) 0.513898 False False 13 (5, 10) 0.513898 False False 14 (5, 11) 0.513898 False False <p>This table shows that the top 5 edges with the largest Adamic-Adar index are all positive edges, while none of the node pairs with non-zero Adamic-Adar index are negative edges. This confirms that the Adamic-Adar index is pretty good at capturing the similarity between the nodes.</p> <p>Although Adamic-Adar index is a very good heuristic, there may be numerous other measures that may work similarly. This is how the machine learning problems were tackled before the deep learning era. Many new methods were about new hand-crafted features, new similarity measures, and new ways to combine them. However, the deep learning era has changed this landscape and ushered the era of end-to-end learning that focuses on good representations.</p> <p>Let's try this. There are many variations, but let's keep it simple and use the word2vec algorithm. We're reusing the graph with the missing edges.</p> In\u00a0[11]: Copied! <pre>nx.draw(G, with_labels=True, pos=pos, \n        node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8)\n</pre> nx.draw(G, with_labels=True, pos=pos,          node_color=[G.nodes[i]['color'] for i in G.nodes], node_size=200, font_size=8) <p>The first step is to generate the random walks.</p> In\u00a0[19]: Copied! <pre>def generate_random_walks(G, walk_length, num_walks):\n    \"\"\" Generate random walks on the graph G.\n    \"\"\"\n    for _ in range(num_walks):\n        # YOUR SOLUTION HERE\n        yield walk\n\nlist(generate_random_walks(G, 8, 3))\n</pre> def generate_random_walks(G, walk_length, num_walks):     \"\"\" Generate random walks on the graph G.     \"\"\"     for _ in range(num_walks):         # YOUR SOLUTION HERE         yield walk  list(generate_random_walks(G, 8, 3)) Out[19]: <pre>[[5, 1, 2, 0, 1, 0, 3, 0, 2],\n [1, 2, 5, 1, 0, 5, 1, 4, 3],\n [3, 4, 5, 0, 2, 0, 1, 2, 5]]</pre> <p>We can then feed these random walks to the word2vec model. For word2vec model, we can use the <code>gensim</code> library.</p> In\u00a0[17]: Copied! <pre># install gensim library \n!pip install gensim\n</pre> # install gensim library  !pip install gensim <pre>Requirement already satisfied: gensim in ./.venv/lib/python3.11/site-packages (4.3.2)\nRequirement already satisfied: numpy&gt;=1.18.5 in ./.venv/lib/python3.11/site-packages (from gensim) (1.26.3)\nRequirement already satisfied: scipy&gt;=1.7.0 in ./.venv/lib/python3.11/site-packages (from gensim) (1.11.4)\nRequirement already satisfied: smart-open&gt;=1.8.1 in ./.venv/lib/python3.11/site-packages (from gensim) (6.4.0)\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\n</pre> <p>Refer to the official documentation to train a model from the random walks.</p> In\u00a0[27]: Copied! <pre># gensim word2vec model using our random walk generator\nfrom gensim.models import Word2Vec\n\n\nrws = list(generate_random_walks(G, 10, 1000))\n\n# YOUR SOLUTION HERE\n\nmodel\n</pre> # gensim word2vec model using our random walk generator from gensim.models import Word2Vec   rws = list(generate_random_walks(G, 10, 1000))  # YOUR SOLUTION HERE  model  Out[27]: <pre>&lt;gensim.models.word2vec.Word2Vec at 0x173ebec10&gt;</pre> <p>Once we have a model, we can plot the embedding. Maybe we can use the PCA to reduce the dimensionality of the embeddings to 2D.</p> In\u00a0[35]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# YOUR SOLUTION HERE\n\nwords = list(model.wv.index_to_key)\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt from sklearn.decomposition import PCA  # YOUR SOLUTION HERE  words = list(model.wv.index_to_key) for i, word in enumerate(words):     plt.annotate(word, xy=(result[i, 0], result[i, 1]))  plt.xticks([]) plt.yticks([]) plt.show()  <p>As you can see, the first principal component separates the two communities quite well. In other words, by simplying observing many random walk trajectories on this network, the word2vec model was able to learn that there are two clusters of nodes in the data.</p> <p>Can we use this embedding for the node classification or link prediction?</p> In\u00a0[41]: Copied! <pre>for node in G.nodes:\n    print(\"Node id:\", node)\n    print(\"actual:\", G.nodes[node]['color'])\n    \n    # find the 3-nearest neighbors in the embedding space and \n    # use their majority class as the prediction\n    # YOUR SOLUTION HERE\n</pre> for node in G.nodes:     print(\"Node id:\", node)     print(\"actual:\", G.nodes[node]['color'])          # find the 3-nearest neighbors in the embedding space and      # use their majority class as the prediction     # YOUR SOLUTION HERE  <pre>Node id: 0\nactual: red\npredicted: red\n\nNode id: 1\nactual: red\npredicted: red\n\nNode id: 2\nactual: red\npredicted: red\n\nNode id: 3\nactual: red\npredicted: red\n\nNode id: 4\nactual: red\npredicted: red\n\nNode id: 5\nactual: red\npredicted: red\n\nNode id: 6\nactual: lightblue\npredicted: lightblue\n\nNode id: 7\nactual: lightblue\npredicted: lightblue\n\nNode id: 8\nactual: lightblue\npredicted: lightblue\n\nNode id: 9\nactual: lightblue\npredicted: lightblue\n\nNode id: 10\nactual: lightblue\npredicted: lightblue\n\nNode id: 11\nactual: lightblue\npredicted: lightblue\n\n</pre> <p>Yay! A perfect prediction!</p> In\u00a0[43]: Copied! <pre>similarities = {}\n\nfor u, v in combinations(G.nodes, 2):\n    if G.has_edge(u, v):\n        continue\n    # YOUR SOLUTION HERE\n\nsorted(similarities.items(), key=lambda x: x[1], reverse=True)\n</pre> similarities = {}  for u, v in combinations(G.nodes, 2):     if G.has_edge(u, v):         continue     # YOUR SOLUTION HERE  sorted(similarities.items(), key=lambda x: x[1], reverse=True) Out[43]: <pre>[((9, 10), 0.99832845),\n ((8, 11), 0.9982137),\n ((10, 11), 0.9981303),\n ((2, 4), 0.99602705),\n ((0, 4), 0.9918693),\n ((4, 6), 0.51385474),\n ((4, 8), 0.4770486),\n ((4, 11), 0.45507237),\n ((2, 6), 0.45084545),\n ((5, 8), 0.44465238),\n ((4, 10), 0.44050527),\n ((4, 9), 0.43067354),\n ((5, 11), 0.4222979),\n ((3, 6), 0.4199267),\n ((4, 7), 0.41550887),\n ((2, 8), 0.41253433),\n ((0, 6), 0.41068062),\n ((5, 10), 0.40792263),\n ((1, 6), 0.40312147),\n ((5, 9), 0.39760047),\n ((2, 11), 0.38934413),\n ((5, 7), 0.38227364),\n ((3, 8), 0.38113827),\n ((2, 10), 0.3747149),\n ((0, 8), 0.37161374),\n ((2, 9), 0.36459893),\n ((1, 8), 0.36389732),\n ((3, 11), 0.3577143),\n ((2, 7), 0.34931344),\n ((0, 11), 0.34853363),\n ((3, 10), 0.34225166),\n ((1, 11), 0.340205),\n ((0, 10), 0.33327013),\n ((3, 9), 0.33221066),\n ((1, 10), 0.32469416),\n ((0, 9), 0.32284117),\n ((3, 7), 0.316696),\n ((1, 9), 0.3148909),\n ((0, 7), 0.3072211),\n ((1, 7), 0.29900423)]</pre> <p>We can again make a table.</p> In\u00a0[44]: Copied! <pre># YOUR SOLUTION HERE\n\nsimilarity_df\n</pre> # YOUR SOLUTION HERE  similarity_df  Out[44]: edge similarity missing negative 0 (9, 10) 0.998328 True False 1 (8, 11) 0.998214 True False 2 (10, 11) 0.998130 True False 3 (2, 4) 0.996027 True False 4 (0, 4) 0.991869 True False 5 (4, 6) 0.513855 False False 6 (4, 8) 0.477049 False False 7 (4, 11) 0.455072 False False 8 (2, 6) 0.450845 False False 9 (5, 8) 0.444652 False False 10 (4, 10) 0.440505 False False 11 (4, 9) 0.430674 False False 12 (5, 11) 0.422298 False False 13 (3, 6) 0.419927 False False 14 (4, 7) 0.415509 False False 15 (2, 8) 0.412534 False False 16 (0, 6) 0.410681 False False 17 (5, 10) 0.407923 False False 18 (1, 6) 0.403121 False False 19 (5, 9) 0.397600 False False 20 (2, 11) 0.389344 False False 21 (5, 7) 0.382274 False False 22 (3, 8) 0.381138 False False 23 (2, 10) 0.374715 False False 24 (0, 8) 0.371614 False False 25 (2, 9) 0.364599 False False 26 (1, 8) 0.363897 False False 27 (3, 11) 0.357714 False False 28 (2, 7) 0.349313 False False 29 (0, 11) 0.348534 False False 30 (3, 10) 0.342252 False False 31 (1, 11) 0.340205 False False 32 (0, 10) 0.333270 False False 33 (3, 9) 0.332211 False False 34 (1, 10) 0.324694 False False 35 (0, 9) 0.322841 False False 36 (3, 7) 0.316696 False False 37 (1, 9) 0.314891 False False 38 (0, 7) 0.307221 False True 39 (1, 7) 0.299004 False False <p>As you can see, all the missing edges (positive set) have similarity value larger than 0.99 while all other pairs have similarity value at most about 0.5. This is pretty good!</p> <p>Q: as the final exercise, pick a small-ish real-world network, learn the node embeddings, and make both traditional network visualization (node-link diagram) and the embedding visualization. You can try PCA or UMAP.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"m11-ml/lab11/#machine-learning-and-graph-embeddings","title":"Machine learning and graph embeddings\u00b6","text":"Open this notebook in Google Colab               Download this notebook (File -&gt; Save As)"},{"location":"m11-ml/lab11/#a-simple-illustration","title":"A simple illustration\u00b6","text":"<p>Let's first conceptually think about the two main graph machine learning tasks: node classification and link prediction.</p> <p>First, let me create a very simple, stylized network. The network has two distinct community structure, which is completely correlated with the colors (the classes) of the nodes (red or lightblue). Most edges are within the same community.</p>"},{"location":"m11-ml/lab11/#node-classification","title":"Node classification\u00b6","text":"<p>Now, let's think about the node classification problem. Given this network, let's assume that we don't know the node class (color) of node 3 (pretend that you don't know the color of node 3!).</p> <p>What would be your strategy to predict the color of node 3? (Again, pretend that you don't know the color...)</p> <p>One obvious strategy that you can think of is just to look at the immediate neighbors of node 3. If most of the neighbors are red, then maybe it's safe to guess that node 3 is also red. If most of the neighbors are lightblue, then you can guess that node 3 is lightblue. This is more or less same as the K-nearest neighbors algorithm, one of the simplest yet powerful machine learning algorithms.</p> <p>Why is this strategy viable? Which property of the real-world networks does this strategy rely on?</p> <p>Yes! This works thanks to the homophily, which is the tendency of nodes to connect to similar nodes. Whenever homophily exists, we also expect that the network has a community structure and a node's properties can be inferred from its neighbors! Let's try this.</p>"},{"location":"m11-ml/lab11/#link-prediction","title":"Link prediction\u00b6","text":"<p>How about link prediction? Let's start with the same network.</p>"},{"location":"m11-ml/lab11/#link-prediction-is-often-about-defining-the-similarity-between-the-nodes","title":"Link prediction is often about defining the similarity between the nodes\u00b6","text":"<p>One general idea is to think about the similarity between the nodes. If two nodes are similar, or \"close\", then they are more likely to be connected. Then how should we think about the similarity?</p>"},{"location":"m11-ml/lab11/#link-prediction-with-community-membership-and-sbm","title":"Link prediction with community membership, and SBM\u00b6","text":"<p>A super simple idea is simply use the community membership as the most informative feature of the nodes. If two nodes are in the same community, then they are \"similar\". If they are in different communities, then they are \"dissimilar\". You may think that this is too simple because we are not considering any other properties once the communities are found. But, do you remember the stochastic block models (SBM)? In the most basic SBM, the probability of connection between two nodes is determined solely by the community membership of the nodes. So, this simple idea may be not that bad!</p>"},{"location":"m11-ml/lab11/#adamic-adar-index","title":"Adamic-Adar index\u00b6","text":"<p>Another simple idea is to think about similarity between nodes based on the number of common neighbors they share. If two people have many shared friends, then they are very likely to be friends. This is the idea behind the Adamic-Adar index and similar. Rather than counting the shared neighbors, Adamic-Adar index gives more weight to the shared neighbors with fewer neighbors. In doing so, the index discounts the shared neighbors that are too popular and thus less informative.</p> <p>$$ \\text{Adamic-Adar}(u, v) = \\sum_{w \\in N(u) \\cap N(v)} \\frac{1}{\\log |N(w)|} $$</p> <p>where $N(u)$ is the set of neighbors of node $u$. Shall we try this? Let's first remove the missing edges from the network first.</p>"},{"location":"m11-ml/lab11/#graph-embeddings","title":"Graph embeddings\u00b6","text":"<p>Some of the most important lessons from deep learning are:</p> <ol> <li>Good representations are crucial for good performance.</li> <li>Usually it is hard to design good representations by hand.</li> <li>By letting the neural network learn the representations, we can achieve better performance.</li> </ol> <p>In a more classical approach, one may construct hand-crafted features and use them as input to the machine learning algorithms. For instance, in the link prediction problem, one may use the Adamic-Adar index as a feature of each pair of nodes. You can also add more features such as the number of common neighbors, the Jaccard index, the distance, etc. Each of these number captures some aspect of the similarity between the nodes and then the machine learning algorithm (e.g., logistic regression) learns how to use these features to predict the link based on examples (training data).</p> <p>However, our intuition is never perfect! We may miss some important features, or we may include some irrelevant features. By contrast, neural network methods tend to be end-to-end, meaning that they learn the representations and the prediction model simultaneously. This leads to much better representations (features) that are optimized for the task at hand.</p> <p>But, another interesting lesson has been that we can \"pre-train\" the neural networks using lots of data that captures the structure of the data. Once we have a good pre-trained model (and good representations), we can reuse the model for other tasks quite successfully. Word embeddings, LLMs, and graph embeddings are all examples of this idea.</p>"},{"location":"m11-ml/lab11/#word2vec","title":"Word2vec\u00b6","text":"<p>Word2vec is a famous example of learning embeddings. The idea is to learn the embeddings of the words simply by trying to predict words that appear in the context of the target word. This originates from the idea of \"language model\".</p> <p>A language model is just a statistical model (probability distribution) that capture the structure of the language. In the simplest sense, a language model is a model that can estimate the joint probability of a sequence of words.</p> <p>$$ \\tilde{P}_{LM}(w_1, w_2, \\ldots, w_T) \\approx P(w_1, w_2, \\ldots, w_T) $$</p> <p>This conditional probability can be factorized into the product of the conditional probabilities of each word given the previous words.</p> <p>$$ P(w_1, w_2, \\ldots, w_T) = P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \\ldots P(w_T | w_1, w_2, \\ldots, w_{T-1}) $$</p> <p>In other words, if you can simply predict the next word (token) given the previous words, then you have a language model. Sounds familiar? This is exactly what the ChatGPT and other LLMs are doing!</p> <p>The core idea behind these complex LLMs originated from the simple idea of word2vec -- learn the vector representations (embeddings) of the words that allow us to predict the next (or previous) word.</p> <p>From the conditional probability formulation of the language model, word2vec has another simplification. Instead of thinking about many previous token, it says \"let's just use a single token to predict another\". In other words,</p> <p>$$ P(w_t | w_{t-1}, w_{t-2}, \\ldots, w_{t-n}) \\approx P(w_t | w_{t-\\tau}) $$</p> <p>where $\\tau$ is a small number (window size). This is called the \"skip-gram\" model because we are skipping all the words in between. Given a large corpus (a collection of sentences), word2vec keeps trying to predict the words around a focal word and use the result to update the embeddings.</p>"},{"location":"m11-ml/lab11/#word2vec-to-node2vec","title":"word2vec to node2vec\u00b6","text":"<p>The input to the word2vec model is just sentences, many of them. From this, we get the embeddings of the words. A really cool thing about this model is that the input \"sentences\" don't have to be real sentences. Any sequence of tokens can be used as the input and word2vec will happily produce the embeddings. The \"tokens\" that consist the input sentences can be anything -- words, characters, or even nodes in a graph!</p> <p>That's the idea behind many graph embedding methods. Instead of learning the embeddings of the words, why don't we learn the embeddings of the nodes in a graph? We just need to feed many sequences of nodes to the word2vec model.</p> <p>But, what should be the reasonable sequences of nodes? These sentences should be able to capture the structure of the graph. When a node follows another, they should be somehow related. How can we do that?</p> <p>One super simple, yet extremely powerful idea is to use the random walks. A random walk is a sequence of nodes that starts from a node and moves to a randomly selected neighbor. By repeating this process many times, we can generate many sequences of nodes that capture the structure of the graph. Because random walks follow edges, it organically captures the structure of the graph. The more two nodes are connected (via different paths), the more likely they will appear in the same random walk. Thus they are more likely to be close in the embedding space!</p>"},{"location":"m11-ml/lab11/#node-classification-with-node-embeddings","title":"Node classification with node embeddings\u00b6","text":"<p>Let's try the node classification first. Instead of looking at the network neighbors, we are going to use the node embeddings. Let's look at the three nearest neighbors of each node in the embedding space.</p>"},{"location":"m11-ml/lab11/#link-prediction-with-node-embeddings","title":"Link prediction with node embeddings\u00b6","text":"<p>Let's try the link prediction next. The similarity between two nodes is simply the distance in the embedding space.</p>"},{"location":"resources/","title":"Project","text":"<p>Information about the project can be found here: https://github.com/yy/netsci-course/wiki/Projects</p>"}]}