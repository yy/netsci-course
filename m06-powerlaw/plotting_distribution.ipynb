{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `terrorism.txt` from http://tuvalu.santafe.edu/~aaronc/powerlaws/data.htm (#7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'terrorism.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open in a text editor or simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head 'terrorism.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tail 'terrorism.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a sorted list of \"degree\" or size values. In the case of terrorism, each line represents a terrorist attack and the number represents the number of deaths in the attack. Note that it doesn't have to come from a network. In network case, each line corresponds to a node and the values would represent the degree of the node. \n",
    "\n",
    "We can load the numbers like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nums = [int(x) for x in open(fname)]\n",
    "print(nums[:5], '...', nums[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot the distribution. First, we want to import `matplotlib` for plotting. `%matplotlib inline` allows us to see the plot directly in this Jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful container is `Counter`. It is a dictionary that is specialized for counting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "a = Counter([1,1,1,1,2,2])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can just pass the `nums` to get the degree distribution. First, let's count the number of data points that we have and construct the counter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(nums)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nk = Counter(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we'll create two lists: `x` stores the unique degree or the values and `y` stores the number of data points with the value. For instance, according to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nk[1], nk[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4802 data points with one death and 1600 data points with two deaths. If we just have them in our dataset, `x` and `y` will be:\n",
    "\n",
    "    x = [1, 2]\n",
    "    y = [4802, 1600]\n",
    "    \n",
    "Let's construct them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for k in sorted(nk):\n",
    "    x.append(k)\n",
    "    y.append(nk[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly plotting the degree distribution\n",
    "\n",
    "If we just plot them, it's the raw (degree) distribution. Let's plot in the normal scale first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Log scale\n",
    "\n",
    "This is how a heavy-tailed distribution look like in normal scale. You can't see much. Let's try log-log scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.ylim((0.7, 10000)) # more clearly show the points at the bottom. \n",
    "plt.xlabel('Number of deaths, x')\n",
    "plt.ylabel(\"n(X)\")\n",
    "plt.loglog(x,y, 'o', markersize=3, markerfacecolor='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-binning\n",
    "\n",
    "Log-binning indicates a way to create bins for histogram in a way that they are equally distributed in log-scale. So, if the first bin is [1.0, 2.0], the next bin will be [2.0, 4.0], and [4.0, 8.0], and so on. `numpy` has a very conveninent function for this. As we know that the data ranges from 1.0 to several thousands, we can set the range to be [0.0, 4.0] (`0.0 = log 1.0; 4.0 = log 10000`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bins = np.logspace(0.0, 4.0, num=40)\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In log-binnning, you *multiply* a constant to create the subsequent bin, rather than *adding* a constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can literally draw a \"histogram\" based on these bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.set_xlabel('Number of deaths, x')\n",
    "ax.set_ylabel('p(x)')\n",
    "ax.hist(nums, bins=bins, normed=True)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can obtain the histogram using [`numpy`'s `histogram` function](http://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html) and then plot the points in the same style as we did with the raw degree distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y, X = np.histogram(nums, bins=bins, normed=True)\n",
    "\n",
    "X = [x*np.sqrt(bins[1]) for x in X][:-1]  # find the center point for each bin. can you explain this?\n",
    "\n",
    "plt.ylim((0.00001, 1))\n",
    "plt.xlabel('Number of deaths, x')\n",
    "plt.ylabel(\"n(X)\")\n",
    "plt.loglog(X,Y, 'o', markersize=3, markerfacecolor='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Can you plot the complementary cumulative distribution function?\n",
    "\n",
    "First of all, you may be confused between CDF (cumulative distribution function) and CCDF (complementary cumulative distribution function). They are all \"cumulative\", but the difference is that the former starts from left and the latter starts from the right. \n",
    "\n",
    "So, as [Power laws, Pareto distributions and Zipf's law](https://arxiv.org/pdf/cond-mat/0412004v3.pdf) (Fig. 3 (d)) paper explains, plotting the CCDF (complementary cumulative distribution function) is probably the best method to show a heavy-tailed distribution. \n",
    "\n",
    "To build the CCDF you start with the unbinned data `x` and `y`. Recall that `y` are the sorted counts associated with each value in `x`. To get the CCDF's value at a particular `x` you want to accumulate all the counts including and following that value. So the first value would just be the sum of all the counts, while the second value would be the sum of all the counts excluding the first one, and so on. In order to represent these values as probabilities, we need to normalize them by dividing all their elements by the largest value (which will always be the first value). This makes sure that the CCDF starts at 1.0 on the log-log plot.\n",
    "\n",
    "Below, calculate the CCDF and plot it in a similar style (log-log, with symbols). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
